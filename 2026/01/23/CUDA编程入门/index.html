<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 8.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"0x822a5b87.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.26.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"stackoverflow-light","dark":"stackoverflow-light"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":true,"style":"flat"},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":false,"nav":null,"activeClass":"utterances"},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="环境配置   效果图  CUDA 开发环境的配置流程虽不算复杂，但主流的实现方案均存在明显的体验短板：  macOS 远程对接 Linux 开发：这种在 macOS 系统下通过远程连接 Linux 环境开发的方式，核心痛点包括：  多数 IDE 对跨平台编译、调试（DEBUG）的支持度不足，功能完整性和稳定性欠佳； 开发过程中易遇到不影响核心功能但影响体验的小问题，整体使用感受较差； 核心问题在于">
<meta property="og:type" content="article">
<meta property="og:title" content="CUDA编程入门">
<meta property="og:url" content="https://0x822a5b87.github.io/2026/01/23/CUDA%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/index.html">
<meta property="og:site_name" content="0x822a5b87的博客">
<meta property="og:description" content="环境配置   效果图  CUDA 开发环境的配置流程虽不算复杂，但主流的实现方案均存在明显的体验短板：  macOS 远程对接 Linux 开发：这种在 macOS 系统下通过远程连接 Linux 环境开发的方式，核心痛点包括：  多数 IDE 对跨平台编译、调试（DEBUG）的支持度不足，功能完整性和稳定性欠佳； 开发过程中易遇到不影响核心功能但影响体验的小问题，整体使用感受较差； 核心问题在于">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://0x822a5b87.github.io/images/cuda/%E6%95%88%E6%9E%9C%E5%9B%BE.png">
<meta property="og:image" content="https://0x822a5b87.github.io/images/cuda/nsys-profile.png">
<meta property="og:image" content="https://0x822a5b87.github.io/images/cuda/grid-and-block.png">
<meta property="og:image" content="https://0x822a5b87.github.io/images/cuda/cuda-stream-parallel.png">
<meta property="og:image" content="https://0x822a5b87.github.io/images/cuda/%E7%BA%BF%E7%A8%8B%E7%B2%97%E5%8C%96.png">
<meta property="article:published_time" content="2026-01-23T01:09:55.000Z">
<meta property="article:modified_time" content="2026-01-29T10:39:39.706Z">
<meta property="article:author" content="2183814023">
<meta property="article:tag" content="CUDA">
<meta property="article:tag" content="GPU">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://0x822a5b87.github.io/images/cuda/%E6%95%88%E6%9E%9C%E5%9B%BE.png">


<link rel="canonical" href="https://0x822a5b87.github.io/2026/01/23/CUDA%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://0x822a5b87.github.io/2026/01/23/CUDA%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/","path":"2026/01/23/CUDA编程入门/","title":"CUDA编程入门"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>CUDA编程入门 | 0x822a5b87的博客</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.10.1/mermaid.min.js","integrity":"sha256-BmQmdWDS8X2OTbrwELWK366LV6escyWhHHe0XCTU/Hk="}}</script>
  <script src="/js/third-party/tags/mermaid.js" defer></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">0x822a5b87的博客</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">到码头整点薯条吃</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="nav-number">1.</span> <span class="nav-text">环境配置</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%80%E6%9C%89%E4%BE%9D%E8%B5%96"><span class="nav-number">1.1.</span> <span class="nav-text">所有依赖</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cuda%E5%92%8Cwindows"><span class="nav-number">1.1.1.</span> <span class="nav-text">CUDA和windows</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#wsl"><span class="nav-number">1.1.2.</span> <span class="nav-text">WSL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vscode"><span class="nav-number">1.1.3.</span> <span class="nav-text">VSCode</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83"><span class="nav-number">1.2.</span> <span class="nav-text">测试环境</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#cuda%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%BC%96%E8%AF%91"><span class="nav-number">2.</span> <span class="nav-text">CUDA程序的编译</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#nvcc-%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BC%96%E8%AF%91%E9%80%BB%E8%BE%91"><span class="nav-number">2.1.</span> <span class="nav-text">nvcc 的核心编译逻辑</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E4%B8%AAcuda%E7%A8%8B%E5%BA%8F"><span class="nav-number">3.</span> <span class="nav-text">第一个CUDA程序</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#cpu%E7%89%88%E6%9C%AC"><span class="nav-number">3.1.</span> <span class="nav-text">CPU版本</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gpu%E7%89%88%E6%9C%AC"><span class="nav-number">3.2.</span> <span class="nav-text">GPU版本</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90"><span class="nav-number">3.3.</span> <span class="nav-text">性能分析</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#nsys"><span class="nav-number">4.</span> <span class="nav-text">nsys</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9D%E6%AD%A5%E7%9A%84%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90"><span class="nav-number">4.1.</span> <span class="nav-text">初步的性能分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%88%91%E4%BB%AC%E7%9A%84%E7%A8%8B%E5%BA%8F%E5%9C%A8%E5%B9%B2%E4%BB%80%E4%B9%88"><span class="nav-number">4.2.</span> <span class="nav-text">我们的程序在干什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E7%9A%84%E4%BC%98%E5%8C%96%E7%82%B9"><span class="nav-number">4.3.</span> <span class="nav-text">最大的优化点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%A6%E4%B8%80%E4%B8%AA%E8%A7%92%E5%BA%A6%E6%9F%A5%E7%9C%8B%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90"><span class="nav-number">4.4.</span> <span class="nav-text">另一个角度查看性能分析</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ncu"><span class="nav-number">5.</span> <span class="nav-text">ncu</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%89%8D%E7%9A%84%E5%87%86%E5%A4%87"><span class="nav-number">6.</span> <span class="nav-text">优化前的准备</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%98%BE%E5%8D%A1%E7%9A%84%E5%AE%8F%E8%A7%82%E7%BB%93%E6%9E%84"><span class="nav-number">6.1.</span> <span class="nav-text">显卡的宏观结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%98%BE%E5%8D%A1%E7%9A%84%E5%BE%AE%E8%A7%82%E7%BB%93%E6%9E%84"><span class="nav-number">6.2.</span> <span class="nav-text">显卡的微观结构</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AC%A1%E4%BC%98%E5%8C%96threads"><span class="nav-number">7.</span> <span class="nav-text">第一次优化（Threads）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#coalesced-access"><span class="nav-number">7.1.</span> <span class="nav-text">Coalesced Access</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#coalesced-access-%E7%9A%84%E7%A4%BA%E6%84%8F%E5%9B%BE"><span class="nav-number">7.2.</span> <span class="nav-text">Coalesced Access 的示意图</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%90%8E%E7%9A%84%E4%BB%A3%E7%A0%81%E9%80%BB%E8%BE%91"><span class="nav-number">7.3.</span> <span class="nav-text">优化后的代码逻辑</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E7%A5%9E%E5%A5%87%E7%9A%84%E7%8E%B0%E8%B1%A1"><span class="nav-number">7.4.</span> <span class="nav-text">一个神奇的现象</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AC%A1%E4%BC%98%E5%8C%96%E5%90%8E%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"><span class="nav-number">7.5.</span> <span class="nav-text">第一次优化后的性能对比</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E6%AC%A1%E4%BC%98%E5%8C%96blocks"><span class="nav-number">8.</span> <span class="nav-text">第二次优化（Blocks）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF"><span class="nav-number">8.1.</span> <span class="nav-text">优化思路</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%90%8E%E7%9A%84%E4%BB%A3%E7%A0%81"><span class="nav-number">8.2.</span> <span class="nav-text">优化后的代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E6%AC%A1%E4%BC%98%E5%8C%96pinned-memory"><span class="nav-number">9.</span> <span class="nav-text">第三次优化（Pinned Memory）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%90%8E%E7%9A%84%E4%BB%A3%E7%A0%81-1"><span class="nav-number">9.1.</span> <span class="nav-text">优化后的代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87"><span class="nav-number">9.2.</span> <span class="nav-text">性能指标</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E6%AC%A1%E4%BC%98%E5%8C%96overlap"><span class="nav-number">10.</span> <span class="nav-text">第四次优化（Overlap）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#overlap%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E9%99%90%E5%88%B6"><span class="nav-number">10.1.</span> <span class="nav-text">Overlap的原理和限制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%90%8E%E7%9A%84%E4%BB%A3%E7%A0%81-2"><span class="nav-number">10.2.</span> <span class="nav-text">优化后的代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87-1"><span class="nav-number">10.3.</span> <span class="nav-text">性能指标</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#cuda%E7%9A%84%E4%BA%8C%E7%BB%B4%E8%AE%A1%E7%AE%97"><span class="nav-number">11.</span> <span class="nav-text">CUDA的二维计算</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%B4%E5%BA%A6%E5%B1%82%E7%BA%A7%E4%BB%8E%E7%82%B9%E5%88%B0%E4%BD%93"><span class="nav-number">11.1.</span> <span class="nav-text">维度层级：从点到体</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E6%98%A0%E5%B0%84"><span class="nav-number">11.2.</span> <span class="nav-text">为什么需要映射</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#matrix-transpose"><span class="nav-number">11.3.</span> <span class="nav-text">Matrix Transpose</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%89%B2%E4%B8%80%E4%B8%AA%E7%9F%A9%E9%98%B5%E7%9A%84%E6%80%9D%E8%B7%AF"><span class="nav-number">11.3.1.</span> <span class="nav-text">分割一个矩阵的思路</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%A9%E7%90%86%E6%9C%AC%E8%B4%A8%E4%B8%80%E7%BB%B4%E7%9A%84%E9%95%BF%E9%98%B5"><span class="nav-number">11.3.1.1.</span> <span class="nav-text">物理本质：一维的长阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E9%87%8D%E5%A1%91"><span class="nav-number">11.3.1.2.</span> <span class="nav-text">逻辑重塑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8C%E7%BA%A7%E7%9F%A9%E9%98%B5block%E6%9C%80%E5%B0%8F%E5%88%86%E9%85%8D%E5%8D%95%E5%85%83"><span class="nav-number">11.3.1.3.</span> <span class="nav-text">二级矩阵（Block）：最小分配单元</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E7%BA%A7%E7%9F%A9%E9%98%B5grid%E5%85%A8%E5%B1%80%E4%BB%BB%E5%8A%A1%E5%88%92%E5%88%86"><span class="nav-number">11.3.1.4.</span> <span class="nav-text">一级矩阵（Grid）：全局任务划分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9D%90%E6%A0%87%E5%90%88%E6%88%90%E4%B8%8E%E7%B4%A2%E5%BC%95%E8%BF%98%E5%8E%9F"><span class="nav-number">11.3.1.5.</span> <span class="nav-text">坐标合成与索引还原</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%88%86%E5%89%B2%E7%9A%84%E5%AE%9E%E4%BE%8B"><span class="nav-number">11.3.2.</span> <span class="nav-text">矩阵分割的实例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E5%90%8E%E4%BB%A3%E7%A0%81"><span class="nav-number">11.4.</span> <span class="nav-text">最后代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8sharedmemory%E4%BC%98%E5%8C%96%E7%9F%A9%E9%98%B5%E8%BD%AC%E7%BD%AE"><span class="nav-number">12.</span> <span class="nav-text">使用SharedMemory优化矩阵转置</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%87%BA%E7%8E%B0%E4%B8%8D%E8%83%BD%E5%90%88%E5%B9%B6%E8%AE%BF%E9%97%AE%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">12.1.</span> <span class="nav-text">为什么会出现不能合并访问的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E5%90%88%E5%B9%B6%E8%AE%BF%E9%97%AE%E5%BC%82%E5%B8%B8"><span class="nav-number">12.2.</span> <span class="nav-text">如何处理合并访问异常</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%99%85%E4%BB%A3%E7%A0%81"><span class="nav-number">12.3.</span> <span class="nav-text">实际代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sharedmemory%E4%B8%8B%E7%9A%84%E7%9F%A9%E9%98%B5%E8%BD%AC%E6%8D%A2%E5%AE%9E%E4%BE%8B"><span class="nav-number">12.4.</span> <span class="nav-text">SharedMemory下的矩阵转换实例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"><span class="nav-number">12.5.</span> <span class="nav-text">优化性能对比</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#sgemm"><span class="nav-number">13.</span> <span class="nav-text">SGEMM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#sgemm%E7%9A%84%E5%B7%A5%E5%85%B7%E5%87%BD%E6%95%B0"><span class="nav-number">13.1.</span> <span class="nav-text">SGEMM的工具函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sgemm-%E7%9A%84%E7%BB%93%E6%9E%9C%E9%AA%8C%E8%AF%81"><span class="nav-number">13.2.</span> <span class="nav-text">SGEMM 的结果验证</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#naive-sgemm"><span class="nav-number">13.3.</span> <span class="nav-text">Naive SGEMM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shared-memory-sgemm"><span class="nav-number">13.4.</span> <span class="nav-text">Shared Memory SGEMM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#index_a"><span class="nav-number">13.4.1.</span> <span class="nav-text">index_a</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#index_b"><span class="nav-number">13.4.2.</span> <span class="nav-text">index_b</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AF%B9%E6%AF%94"><span class="nav-number">13.5.</span> <span class="nav-text">第一次性能优化对比</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#gpu%E8%BF%90%E8%A1%8C%E6%95%88%E7%8E%87%E5%AF%B9%E6%AF%94"><span class="nav-number">13.5.1.</span> <span class="nav-text">GPU运行效率对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E5%AF%B9%E6%AF%94"><span class="nav-number">13.5.2.</span> <span class="nav-text">任务调度对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A1%AC%E4%BB%B6%E5%8D%A0%E7%94%A8%E7%8E%87"><span class="nav-number">13.5.3.</span> <span class="nav-text">硬件占用率</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E7%B2%97%E5%8C%962d-thread-tiling"><span class="nav-number">13.6.</span> <span class="nav-text">线程粗化（2D Thread Tiling）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E7%B2%97%E5%8C%96%E7%9A%84%E5%AE%9E%E7%8E%B0%E9%80%BB%E8%BE%91"><span class="nav-number">13.6.1.</span> <span class="nav-text">线程粗化的实现逻辑</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81"><span class="nav-number">13.6.2.</span> <span class="nav-text">实现代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E6%AC%A1%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AF%B9%E6%AF%94"><span class="nav-number">13.7.</span> <span class="nav-text">第二次性能优化对比</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90-1"><span class="nav-number">13.7.1.</span> <span class="nav-text">性能分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#sm%E4%BD%BF%E7%94%A8%E7%8E%87"><span class="nav-number">13.7.1.1.</span> <span class="nav-text">SM使用率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#syncthreads"><span class="nav-number">13.7.1.2.</span> <span class="nav-text">__syncthreads</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#theoretical-occupancy"><span class="nav-number">13.7.1.3.</span> <span class="nav-text">theoretical occupancy</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF-1"><span class="nav-number">13.7.2.</span> <span class="nav-text">优化思路</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E5%AF%B9%E6%AF%94"><span class="nav-number">13.7.3.</span> <span class="nav-text">指标对比</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#gpu%E8%BF%90%E8%A1%8C%E6%95%88%E7%8E%87%E5%AF%B9%E6%AF%94-1"><span class="nav-number">13.7.3.1.</span> <span class="nav-text">GPU运行效率对比</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E5%AF%B9%E6%AF%94-1"><span class="nav-number">13.7.3.2.</span> <span class="nav-text">任务调度对比</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A1%AC%E4%BB%B6%E5%8D%A0%E7%94%A8%E7%8E%87-1"><span class="nav-number">13.7.3.3.</span> <span class="nav-text">硬件占用率</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#qa"><span class="nav-number">14.</span> <span class="nav-text">QA</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%AF%E8%AF%AD"><span class="nav-number">14.1.</span> <span class="nav-text">术语</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ncu%E7%9A%84%E6%8C%87%E6%A0%87"><span class="nav-number">14.2.</span> <span class="nav-text">ncu的指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#gpu-speed-of-light-sol-throughput"><span class="nav-number">14.2.1.</span> <span class="nav-text">GPU Speed Of Light (SOL)
Throughput</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#launch-statistics"><span class="nav-number">14.2.2.</span> <span class="nav-text">Launch Statistics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#occupation"><span class="nav-number">14.2.3.</span> <span class="nav-text">Occupation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#warp"><span class="nav-number">14.3.</span> <span class="nav-text">warp</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF-warp"><span class="nav-number">14.3.1.</span> <span class="nav-text">什么是 Warp</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88warp%E6%98%AF32%E4%B8%AA%E7%BA%BF%E7%A8%8B"><span class="nav-number">14.3.2.</span> <span class="nav-text">为什么warp是32个线程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#warp%E5%92%8Cblock"><span class="nav-number">14.3.3.</span> <span class="nav-text">warp和block</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#warp%E5%92%8C%E5%90%88%E5%B9%B6%E8%AE%BF%E9%97%AE"><span class="nav-number">14.3.4.</span> <span class="nav-text">warp和合并访问</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#warp-%E7%9A%84%E6%A0%B8%E5%BF%83%E5%8E%9F%E5%88%99"><span class="nav-number">14.3.5.</span> <span class="nav-text">warp 的核心原则</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gpu%E7%9A%84%E6%95%B0%E6%8D%AE%E6%80%BB%E7%BA%BF"><span class="nav-number">14.4.</span> <span class="nav-text">GPU的数据总线</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#memory-burst"><span class="nav-number">14.4.1.</span> <span class="nav-text">Memory Burst</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gpu%E7%9A%84%E6%BC%94%E8%BF%9B"><span class="nav-number">14.5.</span> <span class="nav-text">GPU的演进</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#gpu-%E7%9A%84%E7%A1%AC%E4%BB%B6%E6%94%AF%E6%92%91%E7%BB%84%E4%BB%B6"><span class="nav-number">14.5.1.</span> <span class="nav-text">GPU 的硬件支撑组件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cpu-%E4%B8%8E-gpu-%E7%9A%84%E9%A2%86%E5%9C%B0%E5%B7%AE%E5%BC%82"><span class="nav-number">14.5.2.</span> <span class="nav-text">CPU 与 GPU 的领地差异</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F%E7%9A%84%E6%BC%94%E8%BF%9B"><span class="nav-number">14.5.3.</span> <span class="nav-text">内存访问模式的演进</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9F%E6%96%B9%E5%BC%8F%E6%89%8B%E5%8A%A8%E6%90%AC%E8%BF%90-explicit-copy"><span class="nav-number">14.5.3.1.</span> <span class="nav-text">传统方式：手动搬运 (Explicit
Copy)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%AA%E7%9B%B4%E6%8E%A5%E8%AE%BF%E9%97%AE%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98-unified-memory"><span class="nav-number">14.5.3.2.</span> <span class="nav-text">“伪”直接访问：统一内存
(Unified Memory)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%94%81%E9%A1%B5%E5%86%85%E5%AD%98zero-copy-memory-pinned-memory"><span class="nav-number">14.5.3.3.</span> <span class="nav-text">锁页内存：Zero-Copy
Memory (Pinned Memory)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%88%E6%9E%81%E5%8A%A0%E9%80%9Fgpudirect-%E6%97%8F%E6%8A%80%E6%9C%AF"><span class="nav-number">14.5.3.4.</span> <span class="nav-text">终极加速：GPUDirect 族技术</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%94%81%E9%A1%B5%E5%86%85%E5%AD%98dma"><span class="nav-number">14.6.</span> <span class="nav-text">锁页内存&#x2F;DMA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dma"><span class="nav-number">14.6.1.</span> <span class="nav-text">DMA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BF%85%E9%A1%BB%E7%94%B3%E8%AF%B7%E7%89%B9%E6%AE%8A%E5%86%85%E5%AD%98"><span class="nav-number">14.6.1.1.</span> <span class="nav-text">为什么必须申请“特殊内存”</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dma%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84"><span class="nav-number">14.6.1.2.</span> <span class="nav-text">DMA是如何工作的</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dma-%E7%9A%84%E9%99%90%E5%88%B6"><span class="nav-number">14.6.1.3.</span> <span class="nav-text">DMA 的限制</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%94%81%E9%A1%B5%E5%86%85%E5%AD%98"><span class="nav-number">14.6.2.</span> <span class="nav-text">锁页内存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E5%90%8E%E7%9A%84%E7%BB%93%E6%9E%9C"><span class="nav-number">14.6.3.</span> <span class="nav-text">最后的结果</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">2183814023</p>
  <div class="site-description" itemprop="description">到码头整点薯条吃</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">43</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">39</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://0x822a5b87.github.io/2026/01/23/CUDA%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="2183814023">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="0x822a5b87的博客">
      <meta itemprop="description" content="到码头整点薯条吃">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="CUDA编程入门 | 0x822a5b87的博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CUDA编程入门
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2026-01-23 09:09:55" itemprop="dateCreated datePublished" datetime="2026-01-23T09:09:55+08:00">2026-01-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2026-01-29 18:39:39" itemprop="dateModified" datetime="2026-01-29T18:39:39+08:00">2026-01-29</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="环境配置">环境配置</h1>
<figure>
<img src="\images\cuda\效果图.png" alt="效果图" />
<figcaption aria-hidden="true">效果图</figcaption>
</figure>
<p>CUDA
开发环境的配置流程虽不算复杂，但主流的实现方案均存在明显的体验短板：</p>
<ul>
<li><strong>macOS 远程对接 Linux 开发</strong>：这种在 macOS
系统下通过远程连接 Linux 环境开发的方式，核心痛点包括：
<ul>
<li>多数 IDE
对跨平台编译、调试（DEBUG）的支持度不足，功能完整性和稳定性欠佳；</li>
<li>开发过程中易遇到不影响核心功能但影响体验的小问题，整体使用感受较差；</li>
<li>核心问题在于算力资源（算例）多按小时租赁且成本较高，为控制开销频繁启停实例时，需反复配置开发环境，严重降低开发效率。</li>
</ul></li>
<li><strong>Windows
原生开发</strong>：该方案能满足基础开发流程需求，但核心问题集中在适配性和工具层面：
<ul>
<li>行业内多数 CUDA 相关依赖框架、技术文档均基于 Linux
环境开发和撰写，迁移到 Windows
时，需额外花费精力寻找适配方案，甚至手动调整兼容逻辑；</li>
<li>核心开发工具 Visual Studio
体积臃肿、配置项繁杂，对新手不友好，且易因配置不当引发环境问题。</li>
</ul></li>
</ul>
<p>经过我一段时间的摸索和测试，目前找到了一个比较好的解决方案：<code>Windows</code>
+ <code>WSL(Ubuntu)</code> + <code>VSCode(WSL Plugin)</code>
来作为开发环境，即有极高的兼容性，同时对于习惯于 linux
开发的我来说更顺手。最重要的是，这一套环境的配置远低于其他的环境配置，这里我们介绍一下怎么搭建这个开发环境。</p>
<h2 id="所有依赖">所有依赖</h2>
<ul>
<li>CUDA
<ul>
<li><code>显卡驱动</code>：我们可以在
https://www.nvidia.cn/geforce/drivers/ 下载安装；</li>
<li><code>nvcc</code>：这个就是编译CUDA程序的核心组件，类似于
GCC，主要的作用就是编译 CUDA 程序中的核函数（Kernel function）；</li>
</ul></li>
<li>windows
<ul>
<li><code>Visual Studio</code>：Windows 环境下的 CUDA 编译工具
<code>nvcc</code> 并非完全独立，它需要借助
MSVC（CL）完成主机端（CPU）代码的编译，仅自行处理设备端（GPU）的 CUDA
核函数编译</li>
<li><code>WSL</code>：作为我们CUDA程序执行的子系统；</li>
<li><code>VSCode</code> 在 https://code.visualstudio.com/
下载最新版即可；</li>
<li><code>GCC</code></li>
</ul></li>
<li><code>VSCode</code>
<ul>
<li><code>WSL</code></li>
<li><code>C/C++</code></li>
<li><code>C/C++ Runner</code></li>
<li><code>CMake Tools</code></li>
<li><code>CMake</code></li>
<li><code>CMake Language Support</code></li>
</ul></li>
</ul>
<h3 id="cuda和windows">CUDA和windows</h3>
<p>在 Windows 进行安装时需要选自定义模式，采用精简模式安装后无法运行
nvcc 命令。</p>
<p>安装成功后我们需要在环境变量中，把 <code>Visual Stduio</code> 的
<code>CL</code> 添加到环境变量中：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$&#123;YOUR VS INSTALLING PATH&#125;</span>&#125;\VC\Tools\MSVC\14.50.35717\bin\Hostx64\x64</span><br></pre></td></tr></table></figure>
<p>安装 Toolkit 之后还需要配置下环境变量。默认系统会已经有
<code>CUDA_PATH</code> 和 <code>CUDA_PATH_V11.0</code>（11.0
应该是版本号），需要自己在额外添加如下环境变量：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CUDA_BIN_PATH: %CUDA_PATH%\bin</span><br><span class="line">CUDA_LIB_PATH: %CUDA_PATH%\lib\x64</span><br><span class="line">CUDA_SDK_BIN_PATH: %CUDA_SDK_PATH%\bin\win64</span><br><span class="line">CUDA_SDK_LIB_PATH: %CUDA_SDK_PATH%\common\lib\x64</span><br></pre></td></tr></table></figure>
<p>此外，还需要在系统变量 PATH 中添加如下变量：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%CUDA_BIN_PATH%</span><br><span class="line">%CUDA_LIB_PATH%</span><br><span class="line">%CUDA_SDK_BIN_PATH%</span><br><span class="line">%CUDA_SDK_LIB_PATH%</span><br></pre></td></tr></table></figure>
<h3 id="wsl">WSL</h3>
<p>我们打开 <code>PowerShell</code>，先更新并重启 <code>WSL</code>；</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wsl --update</span><br><span class="line">wsl --shutdown</span><br></pre></td></tr></table></figure>
<p>WSL 依赖底层的虚拟化，我们需要在 “启用或关闭Windows功能”
中，确保以下三项已经勾选：</p>
<ul>
<li>适用于linux的windows子系统；</li>
<li>虚拟机平台；</li>
<li>Hyper-V（这个是用于实现虚拟化的，如果没有可以不管）；</li>
</ul>
<p>随后，我们在<code>PowerShell</code> 下安装 ubuntu</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl --install -d Ubuntu-24.04</span><br></pre></td></tr></table></figure>
<p>等安装好，我们就可以进入到我们的 WSL 了，在 WSL 下我们需要先验证 GPU
是否穿透成功：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure>
<p>如果成功，那就说明我们的 WSL 已经穿透成功了。</p>
<h3 id="vscode">VSCode</h3>
<p>这里需要注意的是，我们在宿主机 windows 上只需要安装 <code>WSL</code>
插件，随后通过 <code>WSL</code> 插件直接选择
<code>Connect WSL</code>，所以依赖的插件：<code>CMake</code>，<code>C/C++ Runner</code>
都需要在连接到 <code>WSL</code> 后安装。</p>
<h2 id="测试环境">测试环境</h2>
<p>我们在刚配置好的 <code>VSCode</code> 下打开文件
<code>hello.cu</code>，输入我们的测试代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">cuda_say_hello</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Hello world, CUDA! %d\n&quot;</span>, threadIdx.x);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Hello world, CPU\n&quot;</span>);</span><br><span class="line">    cuda_say_hello&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line"></span><br><span class="line">    cudaError_t cudaerr = <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">if</span> (cudaerr != cudaSuccess)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;kernel launch failed with error \&quot;%s\&quot;.\n&quot;</span>,</span><br><span class="line">               <span class="built_in">cudaGetErrorString</span>(cudaerr));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行应该能看到我们的CUDA已经正常初始化了，这里重要的是看到我们的 CUDA
初始化逻辑完成：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hangyudu@0x822a5b873:~/code/test-cuda/build$ /home/hangyudu/code/test-cuda/build/hello</span><br><span class="line">Hello world, CPU</span><br><span class="line marked">Hello world, CUDA! 0</span><br></pre></td></tr></table></figure>
<h1 id="cuda程序的编译">CUDA程序的编译</h1>
<p>我们的CUDA测试程序虽然可以直接在 <code>VSCode</code>
下执行，但是我们还是想再以命令行的形式介绍我们的程序编译执行：如果我们直接使用
<code>g++</code> 对这段代码进行编译：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g++ hello.cu</span><br></pre></td></tr></table></figure>
<p>我们会得到如下异常：</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/usr/</span>bin/ld:hello.cu: file format not recognized; treating as linker script</span><br><span class="line"><span class="regexp">/usr/</span>bin/ld:hello.cu:<span class="number">3</span>: syntax error</span><br><span class="line">collect2: error: ld returned <span class="number">1</span> <span class="keyword">exit</span> status</span><br></pre></td></tr></table></figure>
<ul>
<li><code>syntax error</code> 是因为 g++ 只能处理标准 C/C++
代码，完全不认识 CUDA 特有的语法（如
<code>__global__</code>、<code>__device__</code> 关键字）和 GPU
编译逻辑；</li>
<li><code>file format not recognized; treating as linker script</code>
错误，核心原因是<strong>编译器 / 链接器无法识别目标文件的格式 </strong>
——
<code>ld</code>（链接器）在尝试兜底：既然编译器认不出这是代码，它就猜这可能是一个用户自定义的链接脚本
，因此会把 <code>.cu</code> 文件误判为普通文本 / 链接脚本；</li>
</ul>
<p>我们知道，编译器在编译源码的逻辑是这样的（以编译 hello.c 为例）：</p>
<pre><code class="highlight mermaid">flowchart LR
    c(&quot;hello.c（C源码文件）&quot;):::purple
    c_compiler(&quot;gcc/clang（C编译器）&quot;):::blue
    c_obj(&quot;hello.o（C目标文件）&quot;):::animate
    libc(&quot;系统标准库（libc.so）&quot;):::gray
    linker(&quot;ld（链接器）&quot;):::pink
    c_exe(&quot;hello（可执行文件，Linux下为ELF格式）&quot;):::green

    c --&gt;|1.编译（预处理/编译/汇编）| c_compiler --&gt; c_obj
    c_obj &amp; libc --&gt;|2.链接（遵循ABI调用规范）| linker --&gt; c_exe

    classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
    classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
    classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
    classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
    classDef green fill:#695,color:#fff,font-weight:bold;
    classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
    classDef gray fill:#ccc,stroke:#333,font-weight:bold;
    classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
    classDef coral fill:#f8f,stroke:#333,stroke-width:4px;</code></pre>
<p>而我们在使用 <code>g++</code> 编译 CUDA
程序时，<code>__global__</code>
是它不认识的特有语法，也就是说，在生成目标文件时就失败了。我们只能通过
<code>nvcc</code> 来编译这个程序，而 nvcc
是怎么实现这个编译逻辑的呢？</p>
<pre><code class="highlight mermaid">graph TD
    A(CUDA 源码 .cu):::purple --&gt;|nvcc 拆分| B(主机端代码（CPU）：普通C/C++):::blue
    A --&gt;|nvcc 拆分| C(设备端代码（GPU）：\_\_global\_\_/\_\_device\_\_ 函数):::blue
    B --&gt;|编译| D(CL.exe（MSVC）：生成 CPU 目标文件 .obj):::animate
    C --&gt;|编译| E(nvcc：生成 GPU 目标文件 .cubin/.obj):::animate
    D &amp; E --&gt;|链接| F(link.exe（MSVC链接器）：生成最终 .exe/.dll):::green
    
    classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
    classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
    classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
    classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
    classDef green fill:#695,color:#fff,font-weight:bold;
    classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
    classDef gray fill:#ccc,stroke:#333,font-weight:bold;
    classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
    classDef coral fill:#f8f,stroke:#333,stroke-width:4px;</code></pre>
<p>从这个角度来讲，CUDA 很像 <code>g++</code>
中的预处理器和编译器的结合：</p>
<ul>
<li>先通过 <code>nvcc 预处理器</code> 将代码分为两个部分；</li>
<li>再编译属于自己的那部分；</li>
<li>除此之外，nvcc 还需要通过对应平台的编译器（在windows上时 CL.exe）
去编译生成对应平台的目标文件；</li>
<li>最后，再使用链接器生成最后的可执行文件。</li>
</ul>
<h2 id="nvcc-的核心编译逻辑">nvcc 的核心编译逻辑</h2>
<p><code>nvcc</code> 的核心任务是处理 <strong>异构代码（Heterogeneous
Code）</strong>。它会将 <code>.cu</code> 文件拆分成运行在 CPU 上的
<strong>Host 代码</strong> 和运行在 GPU 上的 <strong>Device
代码</strong>。其详细步骤如下：</p>
<ol type="1">
<li><strong>代码拆分 (Splitting):</strong> <code>nvcc</code> 识别出
<code>__global__</code> 等关键字，将 GPU 代码剥离出来。</li>
<li><strong>Device 编译:</strong>
<ul>
<li>将 GPU 代码编译为虚拟指令集
<strong>PTX</strong>（类似字节码）。</li>
<li>再通过 <code>ptxas</code> 将 PTX 编译为特定 GPU 架构的二进制码
<strong>SASS</strong>（cubin）。</li>
</ul></li>
<li><strong>Host 编译 (借用外壳):</strong>
<ul>
<li><code>nvcc</code> 将 Host 代码（以及用于启动内核的 CUDA Runtime
API）交给宿主编译器（Linux 下是 <code>g++</code>，Windows 下是
<code>cl.exe</code>）。</li>
</ul></li>
<li><strong>合并与链接:</strong>
<ul>
<li><code>nvcc</code> 将生成的二进制 GPU 代码嵌入到 Host
的目标文件中。</li>
<li>最后调用链接器，将 CUDA 运行时库 (<code>libcudart</code>)
链接进去，生成最终的可执行文件。</li>
</ul></li>
</ol>
<h1 id="第一个cuda程序">第一个CUDA程序</h1>
<blockquote>
<p>我们给定两个 1D 的张量 x 和 y，要求输出他们的和</p>
</blockquote>
<h2 id="cpu版本">CPU版本</h2>
<p>我们先在CPU的下实现这个逻辑，非常简单，我们直接对程序进程暴力的计算即可，这里没有什么可以说的，就是分配内存后进行计算。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">add</span><span class="params">(<span class="type">float</span> *x, <span class="type">float</span> *y, <span class="type">float</span> *r, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        *(r + i) = *(x + i) + *(y + i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">call_add</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> N = <span class="number">1000000</span>;</span><br><span class="line">    <span class="type">size_t</span> mem_size = <span class="built_in">sizeof</span>(<span class="type">float</span>) * N;</span><br><span class="line">    <span class="type">float</span>* x, *y, *r;</span><br><span class="line"></span><br><span class="line">    x = <span class="built_in">static_cast</span>&lt;<span class="type">float</span>*&gt;(<span class="built_in">malloc</span>(mem_size));</span><br><span class="line">    y = <span class="built_in">static_cast</span>&lt;<span class="type">float</span>*&gt;(<span class="built_in">malloc</span>(mem_size));</span><br><span class="line">    r = <span class="built_in">static_cast</span>&lt;<span class="type">float</span>*&gt;(<span class="built_in">malloc</span>(mem_size));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++) &#123;</span><br><span class="line">        *(x + i) = <span class="number">1.0</span>;</span><br><span class="line">        *(y + i) = <span class="number">2.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">add</span>(x, y, r, N);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;r[%d] = %.3f\n&quot;</span>, i, *(r + i));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">free</span>(x);</span><br><span class="line">    <span class="built_in">free</span>(y);</span><br><span class="line">    <span class="built_in">free</span>(r);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> <span class="type">const</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> start = std::chrono::high_resolution_clock::<span class="built_in">now</span>();</span><br><span class="line">    <span class="built_in">call_add</span>();</span><br><span class="line">    <span class="keyword">auto</span> end = std::chrono::high_resolution_clock::<span class="built_in">now</span>();</span><br><span class="line">    std::chrono::duration&lt;<span class="type">double</span>, std::milli&gt; elapsed = end - start;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Execution time: %f ms\n&quot;</span>, elapsed.<span class="built_in">count</span>());</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="gpu版本">GPU版本</h2>
<blockquote>
<p>在开始之前，我们可以阅读一下 <a href="#gpu的演进">GPU的演进</a>
来初步了解CPU和GPU的差别。</p>
</blockquote>
<p>目前我们碰到的最大问题是，<code>malloc</code>
分配的是我们的普通物理内存，然而我们的GPU不能直接访问物理内存，所以我们需要有一个方法将我们的数据从物理内存搬运到显存，从物理硬件上来讲，数据流转是这样的：</p>
<pre><code class="highlight mermaid">---
title: 数据流转
---
graph LR

data(&quot;data&quot;):::gray --&gt;|PCIe| CPU(&quot;CPU内存&quot;):::yellow --&gt;|PCIe| GPU(&quot;GPU显存&quot;):::green
    
    classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
    classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
    classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
    classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
    classDef green fill:#695,color:#fff,font-weight:bold;
    classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
    classDef gray fill:#ccc,stroke:#333,font-weight:bold;
    classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
    classDef coral fill:#f8f,stroke:#333,stroke-width:4px;</code></pre>
<p>而在实际执行时，它的流转是这样的，这里我特意把我们的内存单独画出来，<strong>因为我们优化的核心就是要避免过多的从内存拷贝数据到显存</strong>：</p>
<pre><code class="highlight mermaid">---
title: 数据流转
---

graph LR
    data(&quot;外部数据&quot;):::gray --&gt;|PCIe| Host(&quot;CPU内存&quot;):::error
    Host --&gt;|cudaMemcpy| Device(&quot;GPU显存 (VRAM)&quot;):::green
    
    subgraph GPU_SM [Streaming Multiprocessor]
        Device --&gt;|Load| SRAM(&quot;Shared Memory / Cache&quot;):::green
        SRAM --&gt;|Register| Core(&quot;CUDA/Tensor Core&quot;):::green
        Core --&gt;|Result| SRAM
        SRAM --&gt;|Store| Device
    end

    Device --&gt;|cudaMemcpy| Host2(&quot;CPU内存&quot;):::error

classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">add</span><span class="params">(<span class="type">float</span>* x, <span class="type">float</span>* y , <span class="type">float</span>* r, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">        *(r + i) = *(x + i) + *(y + i); </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">call_add</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n = <span class="number">1000000</span>;</span><br><span class="line">    <span class="type">int</span> mem_size = <span class="built_in">sizeof</span>(<span class="type">float</span>) * n;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *x, *y, *r;</span><br><span class="line">    x = <span class="built_in">static_cast</span>&lt;<span class="type">float</span>*&gt;(<span class="built_in">malloc</span>(mem_size));</span><br><span class="line">    y = <span class="built_in">static_cast</span>&lt;<span class="type">float</span>*&gt;(<span class="built_in">malloc</span>(mem_size));</span><br><span class="line">    r = <span class="built_in">static_cast</span>&lt;<span class="type">float</span>*&gt;(<span class="built_in">malloc</span>(mem_size));</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">        *(x + i) = <span class="number">1</span>;</span><br><span class="line">        *(y + i) = <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *cuda_x, *cuda_y, *cuda_r;</span><br><span class="line">    <span class="keyword">auto</span> e = <span class="built_in">cudaMalloc</span>(&amp;cuda_x, mem_size);</span><br><span class="line">    <span class="keyword">if</span> (e != cudaSuccess) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Error code: %d\n&quot;</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line">    e = <span class="built_in">cudaMemcpy</span>(cuda_x, x, mem_size, cudaMemcpyKind::cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="keyword">if</span> (e != cudaSuccess) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Error code: %d\n&quot;</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;cuda_y, mem_size);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(cuda_y, y, mem_size, cudaMemcpyKind::cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;cuda_r, mem_size);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(cuda_r, r, mem_size, cudaMemcpyKind::cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    add&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;(cuda_x, cuda_y, cuda_r, n);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(r, cuda_r, mem_size, cudaMemcpyKind::cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; ++i) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;r[%d] = %.3f\n&quot;</span>, i, *(r + i));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = n - <span class="number">10</span>; i &lt; n; ++i) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;r[%d] = %.3f\n&quot;</span>, i, *(r + i));</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaFree</span>(cuda_r);</span><br><span class="line">    <span class="built_in">cudaFree</span>(cuda_y);</span><br><span class="line">    <span class="built_in">cudaFree</span>(cuda_x);</span><br><span class="line">    <span class="built_in">free</span>(r);</span><br><span class="line">    <span class="built_in">free</span>(y);</span><br><span class="line">    <span class="built_in">free</span>(x);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> <span class="type">const</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> start = std::chrono::high_resolution_clock::<span class="built_in">now</span>();</span><br><span class="line">    <span class="built_in">call_add</span>();</span><br><span class="line">    <span class="keyword">auto</span> end = std::chrono::high_resolution_clock::<span class="built_in">now</span>();</span><br><span class="line">    std::chrono::duration&lt;<span class="type">double</span>, std::milli&gt; elapsed = end - start;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Execution time: %f ms\n&quot;</span>, elapsed.<span class="built_in">count</span>());</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="性能分析">性能分析</h2>
<p>对比一下，我们惊人的发现：<strong>CUDA版本执行是
3800ms，而CPU版本的只需要
8ms</strong>。我们这里其实主要的性能问题主要在以下几个方面：</p>
<ol type="1">
<li>我们的代码中，存在大量的数据传输：内存复制到显存，显存计算完后又复制到内存；</li>
<li>我们的 <code>add</code> 函数，根本没有真正的并行 --
我们相当于在GPU的同一个核上进行了全量的计算；</li>
<li>其他的开销：例如额外的内存申请和销毁等；</li>
</ol>
<p>我们后续的优化重点就瞄准于这些逻辑。</p>
<h1 id="nsys">nsys</h1>
<h2 id="初步的性能分析">初步的性能分析</h2>
<p>前面提到，我们的 GPU
版本非常的慢，如果我们想知道程序的性能瓶颈在哪里怎么办呢：</p>
<ul>
<li>对于 <code>compute capability 8.0</code> 以下的版本，我们使用
<code>nvprof</code>；</li>
<li>对于更高的版本，我们使用 <code>nsys</code>；</li>
</ul>
<p>这里我们以 <code>nsys</code> 作为例子：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nvcc gpu/add_gpu.cu -o ~/tmp/a.out</span><br><span class="line"></span><br><span class="line">nsys profile ~/tmp/a.out</span><br></pre></td></tr></table></figure>
<p>这里，我们会生成两个文件：</p>
<ol type="1">
<li><code>.nsys-rep</code> (Nsight System Report) 它是为了在 Windows 或
Linux 的图形界面里查看而设计的。</li>
<li>`<code>.sqlite</code> (SQLite Database)
它是为了<strong>自动化分析</strong>和<strong>二次开发</strong>设计的，我们可以用
Python 的 <code>sqlite3</code> 库或者任何 SQL
工具打开它，并根据需求去定制我们的对比逻辑。</li>
</ol>
<p>我们在 <code>nsys UI</code>
下打开我们得到的文件，我们可以看到如下形式的输出：</p>
<figure>
<img src="\images\cuda\nsys-profile.png" alt="效果图" />
<figcaption aria-hidden="true">效果图</figcaption>
</figure>
<ol type="1">
<li>视图层级与架构分析
<ul>
<li><strong>CPU (16 Cores):</strong>
准确反映了宿主机的逻辑处理器资源。该视图通过时间轴展示了每个核心的负载分布，是判断
<strong>CPU Bound（CPU 受限）</strong> 或 <strong>进程调度延迟</strong>
的核心依据。</li>
<li><strong>CUDA HW (NVIDIA GeForce RTX 4060 Ti):</strong>
硬件物理执行层。直接展示 GPU
内部计算单元（Kernels）与数据传输单元（Memory）的真实物理占用。</li>
<li><strong>Threads (9):</strong>
软件逻辑与上下文层。展示了当前进程（<code>a.out</code>）及其背后支撑环境的所有活动线程。</li>
</ul></li>
<li>CPU 行为洞察
<ul>
<li><strong>核心调度：</strong> 虽然系统识别到 16
个核心，但实际采样中仅有 1 个核心出现明显活动。这反映了 Linux 内核与
WSL2 的调度策略：为了优化缓存命中率和能耗，负载被集中分配。</li>
<li><strong>热点分析：</strong> <code>CPU 4</code>
出现的长黑实线代表了<strong>主线程的在疯狂执行，只不过这个执行并不意味着我们已经开始了数据从内存到显存的搬运</strong>，我们可以对比这段时间的的
<code>CPU</code> 和 <code>CUDA HW-&gt;Memory</code>，我们会发现这段时间
<code>CUDA HW-&gt;Memory</code>
并没有任何的热力图，也就是说根本没有进行任何的数据搬运操作。那这段时间我们的CPU在干什么呢：
<ul>
<li><strong>Driver &amp; Runtime Handshake (驱动与运行时握手)：</strong>
在 WSL2 中，Linux 侧的 CUDA 库需要通过一个名为 <code>dxgkrnl</code>
(DirectX Kernel) 的“桥梁”与 Windows
原生驱动建立连接。这个过程涉及复杂的跨系统内存空间映射，在第一次调用
CUDA API 时会产生巨大的固定开销。</li>
<li><strong>GPU Context Creation (上下文创建)：</strong>
驱动需要为我们的程序在 GPU
上初始化一个“沙盒”。这包括分配常量内存空间、初始化指令队列、设置硬件监控等。</li>
<li><strong>Kernel Just-In-Time (JIT) Linking：</strong>
如果我们的代码没有针对特定的显卡架构（如 <code>sm_89</code> 对于我们的
4060 Ti）进行预编译，CUDA
驱动会在运行时扫描我们的二进制代码并进行最后的链接。</li>
</ul></li>
</ul></li>
<li>CUDA 硬件层性能分析（瓶颈诊断）
<ul>
<li><strong>负载构成：</strong> <code>Kernels (93.1%)</code> vs
<code>Memory (6.9%)</code>。</li>
<li><strong>诊断：</strong> 极高的 Kernel
占比在当前场景下并非代表高效，而是<strong>计算极低效</strong>的信号。由于代码未能开启并行（单线程执行），GPU
强大的算力被严重浪费在了一个串行任务上，导致计算时间被拉长。</li>
<li><strong>理想模型：</strong>
对于向量加法这类算子，应通过增大并行规模将计算时间压缩至微秒级，使性能瓶颈转移到
<strong>Memory Bandwidth（显存带宽）</strong> 上。</li>
<li><strong>内存传输异常：</strong> 观测到 <code>DtoH (66.9%)</code>
耗时远超 <code>HtoD (33.1%)</code>。</li>
<li><strong>原理推析：</strong> 尽管 DtoH 数据量更小（$1 $ 对比 <span
class="math inline">\(3 \times\)</span>），但由于 DtoH
触发了隐式同步，且可能涉及 Host 侧非锁页内存（Pageable
Memory）的页面映射开销，导致其有效带宽显著低于 HtoD。</li>
</ul></li>
<li>线程模型与驱动行为
<ul>
<li><strong>多线程体系：</strong></li>
<li><strong><code>a.out</code> (Main Thread):</strong> 负责业务逻辑与
CUDA API 的下发。</li>
<li><strong>CUDA Driver Workers:</strong>
负责管理显存映射、命令队列调度以及处理 GPU 硬件的中断反馈。</li>
<li><strong>Nsight 辅助线程:</strong> 如 <code>CPUTI</code> 和
<code>CommsProcessor</code>，负责在不严重干扰主程序运行的情况下，采集硬件计数器并实时流传输分析数据。</li>
<li><strong>因果链：</strong> 主线程的占用率波动与 CPU
核心负载高度同步，证实了该线程是驱动整个计算流程的“指挥官”。</li>
</ul></li>
</ol>
<h2 id="我们的程序在干什么">我们的程序在干什么？</h2>
<p>现在，我们可以总结以下我们的程序到底做了什么：</p>
<ol type="1">
<li><strong>0 - 600ms (初始化期)：</strong> CPU 4 满载忙等，GPU
待机。这是驱动在打通 WSL2 到 Windows 的隧道。</li>
<li><strong>600ms 左右 (瞬间搬运期)：</strong> Memory 轴出现极短的
<code>HtoD</code>。这证实了 PCIe 传输其实很快，带宽不是瓶颈。</li>
<li><strong>600ms 之后 (低效计算期)：</strong> <code>add</code> 函数在
CUDA HW
轴上占据了统治地位（93.1%）。<strong>这是真正的性能“重灾区”</strong>，因为此时
GPU 正在用几千个核心中的 <strong>1 个</strong> 核心在慢慢跑循环。</li>
<li><strong>最后 (同步与拷回期)：</strong> CPU
继续忙等，直到单线程计算结束，触发 <code>DtoH</code>。</li>
</ol>
<h2 id="最大的优化点">最大的优化点</h2>
<figure>
<img src="\images\cuda\grid-and-block.png" alt="grid and block" />
<figcaption aria-hidden="true">grid and block</figcaption>
</figure>
<p>如果我们聚焦于 <code>add</code>
函数，我们可以看到我们最大的优化点：此时 <code>grid</code> 和
<code>block</code> 都是
<code>&lt;&lt;&lt;1, 1, 1&gt;&gt;&gt;</code>，这意味着我们GPU上数千个
CUDA Core 并没有被真正的利用起来。我们在后面的章节中会解释这个逻辑。</p>
<h2 id="另一个角度查看性能分析">另一个角度查看性能分析</h2>
<p>此外，我们还可以直接查看结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nsys profile --stats=<span class="literal">true</span> ~/tmp/a.out</span><br></pre></td></tr></table></figure>
<ol type="1">
<li><code>osrt_sum</code>：<code>osrt</code> 是 <strong>Operating System
Runtime</strong> 的缩写。<code>osrt_sum</code>
表统计的是我们的程序调用的 <strong>Linux 操作系统原生
API</strong>（系统调用）的耗时：
<ul>
<li><strong>内存管理：</strong> <code>mmap</code>,
<code>mprotect</code>, <code>brk</code>（对应 C++ 的
<code>new</code>/<code>malloc</code> 或 CUDA
驱动申请内存的底层动作）。</li>
<li><strong>文件/IO 操作：</strong> <code>read</code>,
<code>write</code>, <code>open</code>（比如我们用 <code>std::cout</code>
打印日志）。</li>
<li><strong>同步机制：</strong> <code>ioctl</code>。在 WSL2 中，所有的
CUDA 指令最终都要通过 <code>ioctl</code> 这个系统调用穿透到内核，发送给
GPU 驱动。</li>
</ul></li>
<li><code>cuda_api_sum</code> 表明了我们对 cuda api
调用的统计分析，可以看到我们花费最多的是
<code>cudaMalloc</code>，<code>cudaMemcpy</code>，<code>cudaFree</code>；
在实际的深度学习框架（如 PyTorch）中，为了规避这个开销，会设计
<strong>Memory Pool (显存池/Caching
Allocator)</strong>。程序启动时一次性申请一大块显存，后续使用时只是从池子里“借”，从而消除
<code>cuda_api_sum</code> 里的这些大头。</li>
<li><code>cuda_gpu_kern_sum</code> 表明了 Kernel 函数的调用，这里只有
<code>add</code>；</li>
<li><code>cuda_gpu_mem_time_sum</code>
统计了CUDA内的内存搬运情况，这里只有 <code>DtoH</code> 和
<code>HtoD</code>，这里我们应该关注的是 <strong>Throughput
(吞吐量)</strong>，如果这个值远低于我们显卡的 PCIe 理论带宽（例如 PCIe
4.0 x16 是
31.5GB/s），那就说明小数据量的传输无法填满带宽，<strong>传输开销（Latency）占了主导或者计算出现了性能瓶颈</strong>；</li>
<li><code>cuda_gpu_mem_size_sum</code>
统计了显存的使用情况，这里总共是使用了 12MB，正好对应于我们的300万个
float；</li>
</ol>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">4</span>/<span class="number">8</span>] Executing &#x27;osrt_sum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Name</span><br><span class="line"> --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------</span><br><span class="line">     <span class="number">46</span>.<span class="number">7</span>        <span class="number">125586682</span>          <span class="number">5</span>  <span class="number">25117336.4</span>  <span class="number">14348158.0</span>      <span class="number">1623</span>  <span class="number">80863774</span>   <span class="number">31945408.5</span>  poll</span><br><span class="line">     <span class="number">42</span>.<span class="number">3</span>        <span class="number">113634498</span>        <span class="number">467</span>    <span class="number">243328.7</span>     <span class="number">21812.0</span>      <span class="number">1003</span>   <span class="number">8033042</span>     <span class="number">865567.7</span>  ioctl</span><br><span class="line">      <span class="number">9</span>.<span class="number">5</span>         <span class="number">25480839</span>          <span class="number">7</span>   <span class="number">3640119.9</span>     <span class="number">94518.0</span>      <span class="number">1275</span>  <span class="number">13140210</span>    <span class="number">6074492.7</span>  fread</span><br><span class="line">      <span class="number">0</span>.<span class="number">4</span>          <span class="number">1005425</span>         <span class="number">26</span>     <span class="number">38670.2</span>      <span class="number">2694</span>.<span class="number">5</span>      <span class="number">1056</span>    <span class="number">458091</span>     <span class="number">116303.8</span>  fopen</span><br><span class="line">      <span class="number">0</span>.<span class="number">4</span>           <span class="number">975468</span>          <span class="number">7</span>    <span class="number">139352.6</span>      <span class="number">2636</span>.<span class="number">0</span>      <span class="number">1508</span>    <span class="number">366719</span>     <span class="number">172996.9</span>  read</span><br><span class="line">      <span class="number">0</span>.<span class="number">2</span>           <span class="number">607168</span>          <span class="number">3</span>    <span class="number">202389.3</span>    <span class="number">105488.0</span>     <span class="number">57359</span>    <span class="number">444321</span>     <span class="number">210896.4</span>  sem_timedwait</span><br><span class="line">      <span class="number">0</span>.<span class="number">1</span>           <span class="number">309841</span>          <span class="number">7</span>     <span class="number">44263.0</span>      <span class="number">8282</span>.<span class="number">0</span>      <span class="number">1238</span>    <span class="number">143976</span>      <span class="number">61918.4</span>  open</span><br><span class="line">      <span class="number">0</span>.<span class="number">1</span>           <span class="number">285645</span>          <span class="number">3</span>     <span class="number">95215.0</span>     <span class="number">92495.0</span>     <span class="number">73967</span>    <span class="number">119183</span>      <span class="number">22730.4</span>  pthread_create</span><br><span class="line">      <span class="number">0</span>.<span class="number">1</span>           <span class="number">255878</span>         <span class="number">22</span>     <span class="number">11630.8</span>     <span class="number">11779.0</span>      <span class="number">2899</span>     <span class="number">24350</span>       <span class="number">6170</span>.<span class="number">4</span>  mmap</span><br><span class="line">      <span class="number">0</span>.<span class="number">1</span>           <span class="number">186824</span>          <span class="number">8</span>     <span class="number">23353.0</span>      <span class="number">2378</span>.<span class="number">5</span>      <span class="number">1044</span>    <span class="number">134238</span>      <span class="number">45896.9</span>  fclose</span><br><span class="line">      <span class="number">0</span>.<span class="number">1</span>           <span class="number">159524</span>          <span class="number">1</span>    <span class="number">159524.0</span>    <span class="number">159524.0</span>    <span class="number">159524</span>    <span class="number">159524</span>          <span class="number">0</span>.<span class="number">0</span>  pthread_join</span><br><span class="line">      <span class="number">0</span>.<span class="number">0</span>            <span class="number">66661</span>          <span class="number">1</span>     <span class="number">66661.0</span>     <span class="number">66661.0</span>     <span class="number">66661</span>     <span class="number">66661</span>          <span class="number">0</span>.<span class="number">0</span>  pthread_cond_wait</span><br><span class="line">      <span class="number">0</span>.<span class="number">0</span>            <span class="number">45821</span>          <span class="number">3</span>     <span class="number">15273.7</span>     <span class="number">15690.0</span>     <span class="number">14163</span>     <span class="number">15968</span>        <span class="number">971</span>.<span class="number">9</span>  write</span><br><span class="line">      <span class="number">0</span>.<span class="number">0</span>            <span class="number">26925</span>          <span class="number">1</span>     <span class="number">26925.0</span>     <span class="number">26925.0</span>     <span class="number">26925</span>     <span class="number">26925</span>          <span class="number">0</span>.<span class="number">0</span>  fgets</span><br><span class="line">      <span class="number">0</span>.<span class="number">0</span>            <span class="number">20878</span>          <span class="number">3</span>      <span class="number">6959</span>.<span class="number">3</span>      <span class="number">6837</span>.<span class="number">0</span>      <span class="number">2532</span>     <span class="number">11509</span>       <span class="number">4489</span>.<span class="number">8</span>  pipe2</span><br><span class="line">      <span class="number">0</span>.<span class="number">0</span>            <span class="number">11803</span>          <span class="number">1</span>     <span class="number">11803.0</span>     <span class="number">11803.0</span>     <span class="number">11803</span>     <span class="number">11803</span>          <span class="number">0</span>.<span class="number">0</span>  pthread_cond_broadcast</span><br><span class="line">      <span class="number">0</span>.<span class="number">0</span>             <span class="number">8656</span>          <span class="number">5</span>      <span class="number">1731</span>.<span class="number">2</span>      <span class="number">1253</span>.<span class="number">0</span>      <span class="number">1141</span>      <span class="number">3658</span>       <span class="number">1080</span>.<span class="number">1</span>  fcntl</span><br><span class="line">      <span class="number">0</span>.<span class="number">0</span>             <span class="number">7971</span>          <span class="number">4</span>      <span class="number">1992</span>.<span class="number">8</span>      <span class="number">2025</span>.<span class="number">5</span>      <span class="number">1015</span>      <span class="number">2905</span>        <span class="number">920</span>.<span class="number">3</span>  close</span><br><span class="line">      <span class="number">0</span>.<span class="number">0</span>             <span class="number">4842</span>          <span class="number">2</span>      <span class="number">2421</span>.<span class="number">0</span>      <span class="number">2421</span>.<span class="number">0</span>      <span class="number">1372</span>      <span class="number">3470</span>       <span class="number">1483</span>.<span class="number">5</span>  fwrite</span><br><span class="line"></span><br><span class="line">[<span class="number">5</span>/<span class="number">8</span>] Executing &#x27;cuda_api_sum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Time (%)  Total Time (ns)  Num Calls   Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name</span><br><span class="line"> --------  ---------------  ---------  ----------  ---------  --------  ---------  -----------  ----------------------</span><br><span class="line">     <span class="number">71</span>.<span class="number">6</span>        <span class="number">140866925</span>          <span class="number">3</span>  <span class="number">46955641.7</span>   <span class="number">307899.0</span>    <span class="number">257351</span>  <span class="number">140301675</span>   <span class="number">80840040.2</span>  cudaMalloc</span><br><span class="line">     <span class="number">27</span>.<span class="number">1</span>         <span class="number">53235196</span>          <span class="number">4</span>  <span class="number">13308799.0</span>   <span class="number">637978.5</span>    <span class="number">445941</span>   <span class="number">51513298</span>   <span class="number">25469846.2</span>  cudaMemcpy</span><br><span class="line">      <span class="number">1</span>.<span class="number">0</span>          <span class="number">1919198</span>          <span class="number">1</span>   <span class="number">1919198.0</span>  <span class="number">1919198.0</span>   <span class="number">1919198</span>    <span class="number">1919198</span>          <span class="number">0</span>.<span class="number">0</span>  cudaLaunchKernel</span><br><span class="line">      <span class="number">0</span>.<span class="number">4</span>           <span class="number">725322</span>          <span class="number">3</span>    <span class="number">241774.0</span>   <span class="number">201415.0</span>    <span class="number">166258</span>     <span class="number">357649</span>     <span class="number">101878.7</span>  cudaFree</span><br><span class="line">      <span class="number">0</span>.<span class="number">0</span>              <span class="number">680</span>          <span class="number">1</span>       <span class="number">680</span>.<span class="number">0</span>      <span class="number">680</span>.<span class="number">0</span>       <span class="number">680</span>        <span class="number">680</span>          <span class="number">0</span>.<span class="number">0</span>  cuModuleGetLoadingMode</span><br><span class="line"></span><br><span class="line">[<span class="number">6</span>/<span class="number">8</span>] Executing &#x27;cuda_gpu_kern_sum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                 Name</span><br><span class="line"> --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  -----------------------------------</span><br><span class="line">    <span class="number">100</span>.<span class="number">0</span>         <span class="number">47248532</span>          <span class="number">1</span>  <span class="number">47248532.0</span>  <span class="number">47248532.0</span>  <span class="number">47248532</span>  <span class="number">47248532</span>          <span class="number">0</span>.<span class="number">0</span>  add(float *, float *, float *, int)</span><br><span class="line"></span><br><span class="line">[<span class="number">7</span>/<span class="number">8</span>] Executing &#x27;cuda_gpu_mem_time_sum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Operation</span><br><span class="line"> --------  ---------------  -----  ---------  ---------  --------  --------  -----------  ----------------------------</span><br><span class="line">     <span class="number">66</span>.<span class="number">9</span>          <span class="number">2346695</span>      <span class="number">1</span>  <span class="number">2346695.0</span>  <span class="number">2346695.0</span>   <span class="number">2346695</span>   <span class="number">2346695</span>          <span class="number">0</span>.<span class="number">0</span>  [CUDA memcpy Device-to-Host]</span><br><span class="line">     <span class="number">33</span>.<span class="number">1</span>          <span class="number">1162276</span>      <span class="number">3</span>   <span class="number">387425.3</span>   <span class="number">336623.0</span>    <span class="number">334351</span>    <span class="number">491302</span>      <span class="number">89967.0</span>  [CUDA memcpy Host-to-Device]</span><br><span class="line"></span><br><span class="line">[<span class="number">8</span>/<span class="number">8</span>] Executing &#x27;cuda_gpu_mem_size_sum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation</span><br><span class="line"> ----------  -----  --------  --------  --------  --------  -----------  ----------------------------</span><br><span class="line">     <span class="number">12</span>.<span class="number">000</span>      <span class="number">3</span>     <span class="number">4</span>.<span class="number">000</span>     <span class="number">4</span>.<span class="number">000</span>     <span class="number">4</span>.<span class="number">000</span>     <span class="number">4</span>.<span class="number">000</span>        <span class="number">0</span>.<span class="number">000</span>  [CUDA memcpy Host-to-Device]</span><br></pre></td></tr></table></figure>
<h1 id="ncu">ncu</h1>
<p>参考 <a href="#ncu的指标">ncu的指标</a> 和 <a href="#sgemm">sgemm</a>
中对 ncu 的使用。</p>
<h1 id="优化前的准备">优化前的准备</h1>
<blockquote>
<p>在我们开始之前，我们需要搞清楚GPU的组成以及它是如何调度我们的线程的，我们可以查看
<a href="#术语">术语</a> 这一节查看每个术语的含义。</p>
</blockquote>
<h2 id="显卡的宏观结构">显卡的宏观结构</h2>
<p>从最宏观的角度来将，一个显卡的核心组成包括以下几个部分：</p>
<ol type="1">
<li><code>PCIe</code> 显卡需要通过 PCIe
和外界（CPU/内存）通信，而我们往往最大的性能开销之一就是在PCIe上
<code>内存 -&gt; VRAM -&gt; 内存</code>
的数据流转，尽可能的减少这个数据传输是优化性能的核心之一；</li>
<li><code>VRAM</code>
显存，通常来说由于硬件限制，显卡通常不能直接访问内存（通过 Pinned Memory
可以实现 GPU 直接访问物理内存，但它本质上还是走 PCIe
总线），所以我们需要通过 CPU 和 PCIe 将数据从内存搬运到VRAM；</li>
<li><code>GPU</code>
负责接受数据和分发计算指令（由TPC负责）到SM来进行实际的数据计算；</li>
</ol>
<h2 id="显卡的微观结构">显卡的微观结构</h2>
<blockquote>
<p>这里，我们省略了电源，风扇和PCB等不核心的组件。</p>
</blockquote>
<p>我的显卡是 <code>NVIDIA 4060TI</code>，使用的是
<code>Ada Lovelace</code> 架构，大概的结构如下：</p>
<ol type="1">
<li>数据层面：
<ol type="1">
<li><code>Graphics Card</code> 通过 PCIe 总线和 RAM 进行数据传输；</li>
<li>数据在进入到 RAM 之后，当计算单元发出访问请求时，如果 L2
未命中（Miss），<code>Message Controller</code> 会从 VRAM
读取数据并填充到 L2，这个是比 VRAM
更快的缓存。这个位置其实和CPU的逻辑是完全一样的 -- 80%
的访问是对热点数据的访问，从 L2 Cache 的效率会远高于每次都从 VRAM
取（我们可以通过 <code>Tiling</code> 来让更多的数据停留在L2
Cache）；</li>
</ol></li>
<li>计算层面：
<ol type="1">
<li>当 SM 开始执行指令需要数据时，它会先看 L1，没找到再去 L2，L2
没找到再去 VRAM。我们可以看到，一个GPU中可能包含了多个GPC，一个 GPC
又可能包含了多个 TPC，一个 TPC 下又包含了多个 SM，这里的 SM
才是真正和我们的 CPU 的 Core 对应的最小处理单元（ALU）；</li>
<li>GPU的计算任务是以 <code>grid</code> 的形式提交，也就是我们在
<code>Kernel</code> 中的
<code>&lt;&lt;&lt;1, 1&gt;&gt;&gt;</code>（形式是
<code>&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;</code>）。首先任务会通过
<code>GigaThread Engine</code> 将任务拆分为 block，并将
<code>block</code> 分配到一个合理的 <code>GPC</code>（一个特定的 Block
只能在一个 GPC 下的某个 SM 运行，它不能“横跨”两个
GPC）；<strong>我们之前代码的问题就是，我们只用到了一个单独的GPC。</strong></li>
<li>在 <code>block</code> 被分配到 <code>GPC</code>
后，<code>UnitScheduler</code> 会为 <code>block</code> 选择一个合理的
<code>TPC</code> -- 通常是空闲或者负载较低的 <code>TPC</code>；</li>
<li>在绑定 <code>TPC</code>
之后，<code>Polymorph Engine &amp; Scheduler</code> 会为我们的
blocks/tasks 绑定一个
SM，<strong>在绑定SM之后，任务不会再被调度到其他的SM，因为将 SM 中
Shared Memory 全量的搬运到另一个 SM 代价实在太昂贵</strong>。</li>
<li>随后，我们的线程就开始在 SM 上并行的执行，直到任务结束；</li>
</ol></li>
<li>一些思考：
<ol type="1">
<li>我们前面提到过：<strong>同一个 block 可以数据共享，这里并不是指在
TPC 中有一个 <code>TPC Cache</code>
的硬件来为不同的SM进行数据共享，而是说：当一个 block
绑定到SM之后，就不会再变更，那么 block 下所有的 threads 都可以访问这个
SM 下的 Shared Memory。</strong></li>
<li>CPU 和 GPU 的设计哲学：
<ul>
<li>CPU 的设计哲学是
”极致的快“，一个CPU内部的核数不多，但是每个核都有自己独立的L1/L2缓存，同时还有CPU共享的缓存，而内存极其缓慢。所以CPU使用<strong>分支预测
(Branch Prediction)</strong> 和 <strong>乱序执行 (Out-of-Order
Execution)</strong> 试图提前从内存加载数据到 L1/L2
缓存来缓解IO瓶颈。并且CPU的核数极少，所以完全不能接受因为读取内存带来的IO等待。在这种情况下CPU愿意在内存IO阻塞时，花费昂贵的代价进行上下文切换去处理那些在L1/L2缓存中已经缓存的数据。</li>
<li>GPU 的设计哲学是 ”吞吐量“，一个 GPU
内部有远超CPU的核数，每个SM内部驻留着海量的线程，所以当内存IO阻塞时，GPU不需要进行上下文切换，它只需要通过
<code>Warp Scheduler</code>
去找到那个可以执行的任务即可。这也是为什么每个 SM
内部都有自己的独立寄存器 -- 当 SM1 里的 Warp A（32个线程）因为内存 IO
阻塞时，SM1 内部的 <code>Warp Scheduler</code> 会立刻切换到驻留在同一个
SM1 里的 Warp B。</li>
</ul></li>
<li>GPU的寄存器堆（Register File）：GPU
的寄存器和CPU的寄存器不一样，CPU通常只有少量的通用寄存器和控制寄存器。而GPU内部的每个SM都有自己独立的寄存器堆，而这个寄存器堆中的寄存器数量通常远超CPU中的寄存器数量。这意味着，假设线程需要使用10个寄存器，那么我在线程绑定到SM时便可以为分配这10个寄存器。在线程切换的时候，只需要去读自己绑定的寄存器即可。</li>
</ol></li>
</ol>
<pre><code class="highlight mermaid">flowchart LR

    subgraph Host[&quot;Host Side (CPU/RAM)&quot;]
        Memory[(&quot;System Memory&lt;br/&gt;(RAM)&quot;)]:::animate
    end

    Host &lt;== PCIe_Bus ==&gt; GC

    subgraph GC[&quot;Graphics Card&quot;]
        direction TB
        PCIe(&quot;PCIe Interface&quot;):::blue
        VRAM[(&quot;VRAM&quot;)]:::gray
        
        subgraph GPU[&quot;GPU Chip&quot;]
            direction TB
            MC(&quot;Memory Controller&quot;):::blue
            L2(&quot;L2 Cache&quot;):::blue

            GigaThreadEngine(&quot;GigaThread Engine&quot;):::yellow 
            subgraph GPCs[&quot;GPC 1 ~ N (Multiple Clusters)&quot;]
                direction TB
                UnitScheduler(&quot;UnitScheduler&quot;):::pink
                subgraph TPCs[&quot;TPC 1 ~ M (Multiple per GPC)&quot;]
                    direction TB
                    PE[&quot;Polymorph Engine &amp; Scheduler&quot;]:::orange
                    subgraph SMs[&quot;SM Group (2 per TPC)&quot;]
                        direction LR
                        subgraph SM1[&quot;SM 0&quot;]
                            direction TB
                            Reg1[(&quot;Registers&quot;)]:::pink
                            Core1(&quot;CUDA Cores&quot;):::pink
                            SMem1(&quot;Shared Memory&quot;):::pink
                        end
                        subgraph SM2[&quot;SM 1&quot;]
                            direction TB
                            Reg2[(&quot;Registers&quot;)]:::pink
                            Core2(&quot;CUDA Cores&quot;):::pink
                            SMem2(&quot;Shared Memory&quot;):::pink
                        end
                    end
                    PE --&gt;|Dispatch Blocks/Tasks| SM1
                    PE --&gt;|Dispatch Blocks/Tasks| SM2
                end
            end
        end
    end

    %% Data Path Logic
    PCIe &lt;--&gt; VRAM
    VRAM &lt;--&gt; MC
    MC &lt;--&gt; L2
    L2 &lt;--&gt; GigaThreadEngine

    GigaThreadEngine --&gt;|Grid拆分为Block| GPCs
    GigaThreadEngine --&gt;|Grid拆分为Block| GPCs

    UnitScheduler --&gt;|找到合适的TPC| TPCs
    UnitScheduler --&gt;|找到合适的TPC| TPCs

classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<h1 id="第一次优化threads">第一次优化（Threads）</h1>
<blockquote>
<p>现在，我们知道了GPU的结构以及CUDA怎么调度任务，那我们现在可以开启第一次优化，目标是将我们的
<code>&lt;&lt;&lt;1, 1&gt;&gt;&gt;</code> 改成多线程版本
<code>&lt;&lt;&lt;1, 256&gt;&gt;&gt;</code>；</p>
</blockquote>
<h2 id="coalesced-access">Coalesced Access</h2>
<p>按照我们之前的描述，我们这个
<code>&lt;&lt;&lt;1, 256&gt;&gt;&gt;</code>
的多线程版本，会被绑定到一个单独的 <code>SM</code>
进行执行，此时我们会面临一个问题，我们实现的方式有两种，假设我们线程的索引是
<code>t</code>，那么我们计算的逻辑可以有两种：</p>
<ul>
<li>线程 <code>t</code> 负责计算我们数组中
<code>[t + 0 * 256, t + 1 * 256, t + 2 * 256, ...]</code></li>
<li>线程 <code>t</code> 负责
<code>[t * 256, min(t * 256 + 3907， 1000000))</code></li>
</ul>
<p>他们分别在内存中是一片不连续的区域和一片连续的区域。</p>
<blockquote>
<p>不连续</p>
</blockquote>
<pre><code class="highlight mermaid">block-beta

columns 6

thread index0 index1 index2 index3[&quot;...&quot;] index4
t0(&quot;thread 0&quot;) ti0(&quot;0 * 256 + 0&quot;) ti1(&quot;1 * 256 + 0&quot;) ti2(&quot;2 * 256 + 0&quot;) ti4(&quot;...&quot;) ti3(&quot;3907 * 256 + 0&quot;)
t1(&quot;thread 1&quot;) t1i0(&quot;0 * 256 + 1&quot;) t1i1(&quot;1 * 256 + 1&quot;) t1i2(&quot;2 * 256 + 1&quot;) t1i4(&quot;...&quot;) t1i3(&quot;3907 * 256 + 0 + 1&quot;)
t2(&quot;thread 1&quot;) t2i0(&quot;0 * 256 + 2&quot;) t2i1(&quot;1 * 256 + 2&quot;) t2i2(&quot;2 * 256 + 2&quot;) t2i4(&quot;...&quot;) t2i3(&quot;3907 * 256 + 0 + 2&quot;)
t3(&quot;...&quot;) space:5
t4(&quot;thread 255&quot;) t4i0(&quot;0 * 256 + 255&quot;) t4i1(&quot;1 * 256 + 255&quot;) t4i2(&quot;...&quot;) t4i3(&quot;1000000&quot;)

class thread,index0,index1,index2,index3,index4 green
class t0,t2,t1,t3,t4 purple

class ti0,ti1,ti2,ti3,ti4 gray
class t1i0,t2i0,t1i4,t4i0 gray
class t1i2,t2i1,t3i1,t4i1 gray
class t1i3,t2i2,t3i2,t4i2 gray
class t1i1,t2i3,t3i3 gray
class t2i4,t3i4 gray

class t4i3 error

classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<blockquote>
<p>内存连续</p>
</blockquote>
<pre><code class="highlight mermaid">block-beta

columns 6

thread index0 index1 index2 index3[&quot;...&quot;] index4
t0(&quot;thread 0&quot;) ti0(&quot;0 * 3907&quot;) ti1(&quot;0 * 3907 + 1&quot;) ti2(&quot;0 * 3907 + 2&quot;) ti4(&quot;...&quot;) ti3(&quot;0 * 3907 + 3906&quot;)
t1(&quot;thread 1&quot;) t1i0(&quot;1 * 3907&quot;) t1i1(&quot;1 * 3907 + 1&quot;) t1i2(&quot;1 * 3907 + 2&quot;) t2i4(&quot;...&quot;) t2i3(&quot;1 * 3907 + 3906&quot;)
t2(&quot;thread 2&quot;) t2i0(&quot;2 * 3907&quot;) t2i1(&quot;2 * 3907 + 1&quot;) t2i2(&quot;2 * 3907 + 2&quot;) t3i4(&quot;...&quot;) t3i3(&quot;2 * 3907 + 3906&quot;)
t3(&quot;...&quot;) space:5
t4(&quot;thread 255&quot;) t4i0(&quot;255 * 3907&quot;) t4i1(&quot;255 * 3907 + 1&quot;) t4i2(&quot;...&quot;) t4i3(&quot;1000000&quot;)

class thread,index0,index1,index2,index3,index4 green
class t0,t2,t1,t3,t4 purple

class ti0,ti1,ti2,ti3,ti4 gray
class t1i0,t2i0,t3i0,t4i0 gray
class t1i2,t2i1,t3i1,t4i1 gray
class t1i3,t2i2,t3i2,t4i2 gray
class t1i1,t2i3,t3i3 gray
class t2i4,t3i4 gray

class t4i3 error

classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<p>从CPU的角度来讲，我们在读取内存时是分页的，所以我们在<strong>第二个方案中，每个线程计算时读取连续内存性能一定会比读取不连续内存更高</strong>。这个结论在显存的计算中仍然成立吗？<strong>答案是非常反直觉的，第一个方案这种看似不连续的性能会更高，这是GPU
性能优化中排在第一位的准则：访存合并 (Memory Coalescing)。</strong></p>
<p>这里涉及到CPU和GPU的内存访问模式：</p>
<ul>
<li>单核 CPU 的 “并行”
本质是<strong>时间片轮转的并发</strong>，通过快速上下文切换模拟多任务同时执行，同一时刻物理上仅能执行一条指令；</li>
<li>GPU
的并行是<strong>空间上的硬件级并行</strong>，SM（流多处理器）内的大量
CUDA Core（或流处理器）可在同一时钟周期内，对不同数据执行相同指令（即
SIMT 架构），属于真正的并行计算。</li>
</ul>
<p>对于CPU来讲，任务执行时的内存模型是这样的：</p>
<pre><code class="highlight mermaid">block-beta

columns 11

x0(&quot;0&quot;) x1(&quot;1&quot;) x2(&quot;2&quot;) x3(&quot;...&quot;) x4(&quot;3906&quot;) 
x5(&quot;0 + 3907&quot;) x6(&quot;1 + 3907&quot;) x7(&quot;2 + 3907&quot;) x8(&quot;...&quot;) x9(&quot;3906 + 3907&quot;)
x10(&quot;...&quot;)

t0:5
t1:5

class x0,x1,x2,x3,x4 green
class x5,x6,x7,x8,x9 yellow
class x10 blue
class t0,t1 pink

classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<p>在 <code>t0</code> 时间，CPU 集中的处理一段连续的内存；在
<code>t1</code> 时间，CPU 集中的处理另外一段连续的内存。</p>
<p>对于GPU来讲，任务执行时的内存模型是这样的：</p>
<pre><code class="highlight mermaid">block-beta

columns 25

x0(&quot;0&quot;) x1(&quot;1&quot;) x2(&quot;...&quot;) x3(&quot;31&quot;) x4(&quot;...&quot;) x5(&quot;255&quot;)
y0(&quot;0&quot;) y1(&quot;1&quot;) y2(&quot;...&quot;) y3(&quot;31&quot;) y4(&quot;...&quot;) y5(&quot;255&quot;)
z0(&quot;0&quot;) z1(&quot;1&quot;) z2(&quot;...&quot;) z3(&quot;31&quot;) z4(&quot;...&quot;) z5(&quot;255&quot;)
a(&quot;...&quot;)
b0(&quot;0&quot;) b1(&quot;1&quot;) b2(&quot;...&quot;) b3(&quot;31&quot;) b4(&quot;...&quot;) b5(&quot;255&quot;)

class x0,x1,x2,x3 green
class y0,y1,y2,y3 yellow
class b0,b1,b2,b3 blue
class z0,z1,z2,z3 orange

warp1(&quot;warp&quot;):4
space:2
warp2(&quot;warp&quot;):4
space:2
warp3(&quot;warp&quot;):4
space:3
warp4(&quot;warp&quot;):4
space:2

class warp1,warp2,warp3,warp4 purple


classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<p>GPU 的执行单位是 Warp（32个线程）。当指令执行到访存操作时，这 32
个线程会<strong>同时</strong>发出内存请求：当warp中的32个线程执行时，请求的是一片<strong>连续的、对齐的</strong>内存块。
<strong>硬件行为：</strong> GPU 的 <strong>Memory Controller
(MC)</strong> 发现我们要的数据挨在一起，它会发起一次 <strong>合并访问
(Coalesced Access)</strong>。原本要发 32 次指令，现在只需 1
次总线事务就能把这一块数据全部拉回
SM。也就是说，在这个场景下，<code>[0, 31]</code>
对GPU来说才是一段连续的内存。</p>
<p><strong>CPU 的连续是“时间轴上的连续”</strong>：</p>
<ul>
<li>因为 CPU 只有一个（或少数几个）核心在跑。对于这个核心来说，它希望在
<span class="math inline">\(T_0, T_1, T_2\)</span>
这一系列时间点上，访问的地址是 <span class="math inline">\(N, N+1,
N+2\)</span>。</li>
<li><strong>目的</strong>：为了让数据留在 <strong>L1 Cache</strong>
里。</li>
</ul>
<p><strong>GPU 的连续是“空间轴上的连续”</strong>：</p>
<ul>
<li>因为 GPU 是 Warp (32个核心)
在同一时刻跑。对于这组核心来说，它们希望在同一个时间点 <span
class="math inline">\(T_0\)</span>，访问的地址分别是 <span
class="math inline">\(N, N+1, \dots, N+31\)</span>。</li>
<li><strong>目的</strong>：为了触发 <strong>Memory Coalescing
(访存合并)</strong>。</li>
</ul>
<p>从 <strong>Memory Controller (MC)</strong> 的视角来看：</p>
<ol type="1">
<li><strong>硬件的“集装箱”机制</strong>：显存（VRAM）和 SM
之间的数据传输不是按“字节”传的，而是按“块（Segment）”传的（通常是
<strong>32 字节、64 字节或 128 字节</strong>）。</li>
<li><strong>方案一</strong>：Warp 里的 32 个线程正好要地址 [0-31]。MC
发现这 32 个需求正好能塞进一个 <strong>128-byte</strong>
的集装箱里。于是，一次总线搬运，32 个线程全吃饱。</li>
<li><strong>方案二</strong>：Thread 0 要地址 0，Thread 1 要地址 3907。MC
必须搬运包含地址 0 的整个集装箱，结果只给 Thread 0 用了 4 字节，剩下的
124 字节全扔了；然后还得再去搬运包含 3907 的集装箱。</li>
</ol>
<h2 id="coalesced-access-的示意图">Coalesced Access 的示意图</h2>
<p>GPU 和 CPU 的最大区别在于，GPU以 WARP 的形式并行的执行线程，所以在 T0
时刻：<code>Warp</code> 中的 32 个线程在同一时刻对 <span
class="math inline">\(x\)</span> 和 <span
class="math inline">\(y\)</span> 发起访问：</p>
<ul>
<li><strong>第一次读取（x 数组）</strong>：<span
class="math inline">\(t_0\)</span> 到 <span
class="math inline">\(t_{31}\)</span> 同时说：“我要 <span
class="math inline">\(x[0]\)</span> 到 <span
class="math inline">\(x[31]\)</span>”。Memory Controller (MC) 看到这 32
个连续的需求，启动一次总线事务（Transaction），读取 <span
class="math inline">\(x[0..31]\)</span>。此时，总线读取的内存是一片连续的内存，只需要一次总线事务；</li>
<li><strong>第二次读取（y 数组）</strong>：同理，一次性把 <span
class="math inline">\(y[0..31]\)</span> 搬运过来。</li>
</ul>
<pre><code class="highlight mermaid">block-beta

Warp(&quot;Warp&quot;)
block:warp_block:10
t0(&quot;thread 0&quot;)
t1(&quot;thread 1&quot;)
t2(&quot;thread 2&quot;)
t3(&quot;thread 3&quot;)
t4(&quot;...&quot;)
end
space:11
L2Cache(&quot;L2 Cache&quot;)
block:xcache:5
    xc0(&quot;x[0]&quot;)
    xc1(&quot;x[1]&quot;)
    xc2(&quot;x[2]&quot;)
    xc3(&quot;x[3]&quot;)
    xc4(&quot;...&quot;)
end
block:ycache:5
    yc0(&quot;y[0]&quot;)
    yc1(&quot;y[1]&quot;)
    yc2(&quot;y[2]&quot;)
    yc3(&quot;y[3]&quot;)
    yc4(&quot;...&quot;)
end
columns 11
space:3
blockArrowId6&lt;[&quot; &quot;]&gt;(up)
space:4
blockArrowId5&lt;[&quot; &quot;]&gt;(up)
space:2
vram(&quot;vram&quot;)
block:x:5
    xv0(&quot;x[0]&quot;):1
    xv1(&quot;x[1]&quot;)
    xv2(&quot;x[2]&quot;)
    xv3(&quot;x[3]&quot;)
    xv4(&quot;...&quot;)
end
block:y:5
    xy0(&quot;y[0]&quot;):1
    xy1(&quot;y[1]&quot;)
    xy2(&quot;y[2]&quot;)
    xy3(&quot;y[3]&quot;)
    xy4(&quot;...&quot;)
end
space:3
blockArrowId4&lt;[&quot; &quot;]&gt;(up)
space:4
blockArrowId3&lt;[&quot; &quot;]&gt;(up)
space:2
PCIe(&quot;PCIe&quot;):11
space:3
blockArrowId2&lt;[&quot; &quot;]&gt;(up)
space:4
blockArrowId1&lt;[&quot; &quot;]&gt;(up)
space:2
ram(&quot;RAM&quot;)
x1(&quot;x in RAM&quot;):5
y1(&quot;y in RAM&quot;):5

xc0 --&quot;t0&quot;--&gt; t0
yc0 --&quot;t0&quot;--&gt; t0
xc1 --&quot;t0&quot;--&gt; t1
yc1 --&quot;t0&quot;--&gt; t1
xc2 --&quot;t0&quot;--&gt; t2
yc2 --&quot;t0&quot;--&gt; t2
xc3 --&quot;t0&quot;--&gt; t3
xc3 --&quot;t0&quot;--&gt; t3
xc4 --&quot;t0&quot;--&gt; t4
yc4 --&quot;t0&quot;--&gt; t4

class ram,vram,L2Cache,Warp purple
class x1,y1 yellow
class PCIe gray
class xv0,xv1,xv2,xv3,xv4,xy0,xy1,xy2,xy3,xy4 blue
class xc0,xc1,xc2,xc3,xc4,yc0,yc1,yc2,yc3,yc4 green
class t0,t1,t2,t3,t4 pink

classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<h2 id="优化后的代码逻辑">优化后的代码逻辑</h2>
<p>CUDA 提供了一些内建的变量来访问线程相关的信息，比如：</p>
<table>
<thead>
<tr class="header">
<th><strong>变量名</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong><code>threadIdx</code></strong></td>
<td><strong>线程在 Block 里的索引</strong></td>
</tr>
<tr class="even">
<td><strong><code>blockIdx</code></strong></td>
<td><strong>Block 在 Grid 里的索引</strong></td>
</tr>
<tr class="odd">
<td><strong><code>blockDim</code></strong></td>
<td><strong>Block 的维度（大小）</strong></td>
</tr>
<tr class="even">
<td><strong><code>gridDim</code></strong></td>
<td><strong>Grid 的维度（大小）</strong></td>
</tr>
<tr class="odd">
<td><code>warpSize</code></td>
<td><strong>warp的数量</strong></td>
</tr>
</tbody>
</table>
<p>我们现在需要用到的是：</p>
<ul>
<li><code>threadIdx.x</code>:
指此线程在<code>thread block</code>中的下标位置</li>
<li><code>blockDim.x</code>:
指一个<code>thread block</code>中的线程数</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">add_stride</span><span class="params">(<span class="type">float</span>* x, <span class="type">float</span>* y , <span class="type">float</span>* r, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> index = threadIdx.x;</span><br><span class="line">    <span class="type">int</span> stride = blockDim.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = index; i &lt; n; i += stride) &#123;</span><br><span class="line">        *(r + i) += *(x + i) + *(y + i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>随后，我们来执行我们的函数，<strong>这里因为我们使用了WSL，为了排除WSL执行时的干扰，我们将数组的数量增加到了一亿。</strong>，具体的分析对比我们可以查看
<a href="#第一次优化后的性能对比">第一次优化后的性能对比</a>。</p>
<h2 id="一个神奇的现象">一个神奇的现象</h2>
<p>如果我们将代码改成如下形式：<strong>我们没有对for循环进行并行优化，也就是
<code>+=</code> 被重复的执行了256次，然而最后结果却仍然是可能是
<code>{3,6,9,12,15,...}</code> 等输出</strong> ，这是因为，每一行
<code>*(r + i) += ...</code>
实际上包含了三个隐藏的步骤，这在计算机底层被称为
<strong>“读取-修改-写入”（Read-Modify-Write）</strong> 序列：</p>
<ol type="1">
<li><strong>Read</strong>：从显存（VRAM）中读取 <code>r[i]</code>
的当前值到 SM 的<strong>寄存器</strong>中。</li>
<li><strong>Modify</strong>：在计算单元（ALU）中完成加法。</li>
<li><strong>Write</strong>：将计算后的新值从寄存器写回到显存
<code>r[i]</code>。</li>
</ol>
<p><strong>问题出在“同时性”上：</strong> 当我们执行
<code>&lt;&lt;&lt;1, 256&gt;&gt;&gt;</code> 时，一个
Warp（32个线程）是在<strong>同一个时钟周期</strong>发出内存请求的。</p>
<ul>
<li><strong>T0时刻</strong>：线程 0 到 线程 31 同时读取
<code>r[0]</code>。此时它们读到的值都是初始值 <strong>0</strong>。</li>
<li><strong>T1时刻</strong>：每个线程在自己的寄存器里算出了
<code>0 + x[0] + y[0] = 3</code>。</li>
<li><strong>T2时刻</strong>：这 32 个线程同时尝试把 <strong>3</strong>
写回显存地址 <code>r[0]</code>。</li>
</ul>
<p>而我们的结果最终却不是预料的
<code>768</code>，这是因为这里产生的<strong>竞态条件</strong>，最终的结果是完全不确定的。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">add</span><span class="params">(<span class="type">float</span>* x, <span class="type">float</span>* y , <span class="type">float</span>* r, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line marked">        *(r + i) += *(x + i) + *(y + i); </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">call_add</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line marked">    add&lt;&lt;&lt;<span class="number">1</span>, <span class="number">256</span>&gt;&gt;&gt;(cuda_x, cuda_y, cuda_r, n);</span><br><span class="line">	<span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>验证的方法也很简单：我们使用原子加法即可得到结果
<code>768.00</code></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">atomic_add</span><span class="params">(<span class="type">float</span>* x, <span class="type">float</span>* y , <span class="type">float</span>* r, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">        <span class="type">float</span> val = *(x + i) + *(y + i);</span><br><span class="line marked">        <span class="built_in">atomicAdd</span>(r + i, val);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="第一次优化后的性能对比">第一次优化后的性能对比</h2>
<p>这里，我们会对核心指标进行对比： | 指标 |
<code>add&lt;&lt;&lt;1, 1&gt;&gt;&gt;</code> |
<code>add_stride&lt;&lt;&lt;1, 256&gt;&gt;&gt;</code> | |
---------------- | --------------- | ------------------------ | |
Kernels &amp; Memory | <code>89.0%</code>/<code>11.0%</code> |
<code>14.5%</code>/<code>85.5%</code> | | Kernal执行时间 | 4984ms |
104ms | | CUDA memcpy DtoH | 409ms | 456ms | | CUDA memcpy HtoD | 134ms
| 137ms |</p>
<p>可以看到：我们在 <code>memcpy</code>
没有变化不大的情况下，<code>Kernels</code>
的执行时间大幅下降，因为我们并行执行能更好的利用GPU的并行计算能力，也就是说，我们的性能瓶颈由SM计算变成了我们的VRAM访问。</p>
<h1 id="第二次优化blocks">第二次优化（Blocks）</h1>
<blockquote>
<p>我们之前使用了多线程对并行计算进行优化，现在我们开始使用
<code>grid</code>来继续优化。</p>
</blockquote>
<h2 id="优化思路">优化思路</h2>
<p>根据我们之前的描述，我们目前是所有的数据被绑定在一个SM上：</p>
<pre><code class="highlight mermaid">block-beta

columns 11

t(&quot;thread&quot;)

block:warp:10
t0(&quot;thread 0&quot;)
t1(&quot;thread 1&quot;)
t2(&quot;thread 2&quot;)
t3(&quot;thread 3&quot;)
t4(&quot;thread 4&quot;)
t5(&quot;thread 5&quot;)
t6(&quot;thread 6&quot;)
t7(&quot;thread 7&quot;)
t8(&quot;thread 8&quot;)
t9(&quot;...&quot;)
end

space:11

blk(&quot;TPCs&quot;)
block:tpc1:5
    sm1(&quot;SM1&quot;)
    sm2(&quot;SM2&quot;)
end
block:tpc2:5
    sm3(&quot;...&quot;)
    sm4(&quot;...&quot;)
end

warp --&gt; sm1

class t,blk purple
class t0,t1,t2,t3,t4,t5,t6,t7,t8,t9 green
class sm1,sm2,sm3,sm4 orange

classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<p>我们的目标是，将我们的 TPCs 都充分的利用起来：<strong>GPU调度线程以
warp 为单位，所以我们最好的选择是，每个 block 分配足量的线程，让我们的
256 个线程拆分到不同的 block，这样我们整个的计算既可以最大化的利用
warp，又可以充分的利用我们的TPCs：</strong></p>
<ul>
<li><code>warp</code> 32 是warp执行的最小原子，所以我们的 warp
数量一定要是 32 的整数倍；</li>
<li>一个 SM 不止可以跑一个
warp，这和CPU编程完全不一样。在CPU编程中，由于CPU的核寄存器数量极少所以在进行上下文切换时开销很大，最好的策略不要分配过多的线程导致过多的上下文切换。而在GPU编程中，GPU的寄存器数量远超CPU，所以线程的切换不仅开销很低，更可以充分的利用GPU的寄存器组来避免VRAM读取时的线程挂起，从而达到更大的吞吐量。</li>
</ul>
<p>所以我们的最佳策略是：</p>
<ol type="1">
<li>分配足量的 <code>blocks</code>，确保每个 SM 都能承载计算；</li>
<li>每个 <code>block</code> 中还需要分配足量的 <code>threads</code>
，确保我们分配到每个 SM 的 warp 可以尽可能的利用GPU的寄存器组。</li>
</ol>
<p>可以我们以我的显卡 <code>4060 TI</code>（有34个SM）
为例子，将这个策略转换为我们的公式：</p>
<p><span class="math display">\[TotalThreads =
\underbrace{SM\_Count}_{34} \times \underbrace{Blocks\_per\_SM}_{2 \sim
4} \times \underbrace{Threads\_per\_Block}_{128 \sim 256}\]</span></p>
<p>注意，这里我们会发现有一个额外的参数 <span
class="math inline">\(\underbrace{Blocks\_per\_SM}_{2 \sim 4}\)</span>
，这个参数并不是显式的声明的，而是通过 <code>GridSize</code>
的设置“推导”出来的。当我们的代码中指定
<code>add&lt;&lt;&lt;GridSize, BlockSize&gt;&gt;&gt;</code>
时，硬件的分配逻辑如下：</p>
<p><span class="math display">\[\text{Blocks\_per\_SM} =
\frac{\text{GridSize}}{\text{SM\_Count}}\]</span></p>
<p>那么，我们现在优化后的思路就需要有如下的分配逻辑了：</p>
<pre><code class="highlight mermaid">block-beta

columns 11

t(&quot;threads&quot;)

block:threads:10
    t0(&quot;thread 0&quot;)
    t1(&quot;thread 1&quot;)
    t2(&quot;thread 2&quot;)
    t3(&quot;...&quot;)
    t4(&quot;thread 255&quot;)
    t5(&quot;...&quot;)
end

space:11

b(&quot;blocks&quot;)
block:blocks:10
    b0(&quot;block 0&quot;)
    b1(&quot;block 1&quot;)
    b2(&quot;block 2&quot;)
    b3(&quot;block 3&quot;)
    b4(&quot;block 4&quot;)
    b5(&quot;block 5&quot;)
    b6(&quot;block 6&quot;)
    b7(&quot;block 7&quot;)
    b8(&quot;...&quot;)
    b9(&quot;block 134&quot;)
    b10(&quot;block 135&quot;)
end

space:11

blk(&quot;TPCs&quot;)
block:tpc1:5
    sm1(&quot;SM1&quot;)
    sm2(&quot;SM2&quot;)
end
block:tpc2:5
    sm3(&quot;...&quot;)
    sm4(&quot;...&quot;)
end

class b,blk,t purple
class t0,t1,t2,t3,t4,t5,t6,t7,t8,t9 green
class sm1,sm2,sm3,sm4 orange
class blocks,tpc1,tpc2,threads animate
class b0,b1,b2,b3,b4,b5,b6,b7,b8,b9,b10 blue

b0 --&gt; sm1
b1 --&gt; sm1
b2 --&gt; sm1
b3 --&gt; sm1
b4 --&gt; sm2
b5 --&gt; sm2
b6 --&gt; sm2
b7 --&gt; sm2
b9 --&gt; sm4
b10 --&gt; sm4

t0 --&gt; b0
t1 --&gt; b0
t2 --&gt; b0
t3 --&gt; b0
t4 --&gt; b0

classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<blockquote>
<p><strong>此外，在我们增加了 <code>blocks</code>
之后，我们又回到之前的那个问题：怎么样合理的安排循环才可以尽可能的利用我们的
<code>Memory Controller</code> 的并行读取能力？</strong></p>
<p>还是那句话，GPU以 <code>warp</code>
为单位对线程进行调度，并且在同一个 <code>warp</code>
中，线程的执行是并行的 -- 也就是说，<strong>我们希望每次
<code>warp</code>
执行时，所有的线程需要的数据在内存的同一片区域。</strong>那么，我们每个线程应该负责的索引如下：</p>
<ol type="1">
<li><strong><code>tid</code> (Global Thread ID)</strong>: <span
class="math inline">\(blockIdx.x \times blockDim.x +
threadIdx.x\)</span></li>
<li><strong><code>stride</code> (Total Grid Size)</strong>: <span
class="math inline">\(gridDim.x \times blockDim.x\)</span></li>
</ol>
</blockquote>
<h2 id="优化后的代码">优化后的代码</h2>
<table style="width:100%;">
<colgroup>
<col style="width: 19%" />
<col style="width: 18%" />
<col style="width: 29%" />
<col style="width: 32%" />
</colgroup>
<thead>
<tr class="header">
<th>指标</th>
<th><code>add&lt;&lt;&lt;1, 1&gt;&gt;&gt;</code></th>
<th><code>add_stride&lt;&lt;&lt;1, 256&gt;&gt;&gt;</code></th>
<th>add_parallel&lt;&lt;&lt;136, 256&gt;&gt;&gt;</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Kernels &amp; Memory</td>
<td><code>89.0%</code>/<code>11.0%</code></td>
<td><code>14.5%</code>/<code>85.5%</code></td>
<td><code>1.0%</code>/<code>99.0%</code></td>
</tr>
<tr class="even">
<td>Kernal执行时间</td>
<td>4984ms</td>
<td>104ms</td>
<td>6ms</td>
</tr>
<tr class="odd">
<td>CUDA memcpy DtoH</td>
<td>409ms</td>
<td>456ms</td>
<td>386ms</td>
</tr>
<tr class="even">
<td>CUDA memcpy HtoD</td>
<td>134ms</td>
<td>137ms</td>
<td>131ms</td>
</tr>
</tbody>
</table>
<h1 id="第三次优化pinned-memory">第三次优化（Pinned Memory）</h1>
<p>现在，我们的程序核心总耗时为：</p>
<p><span class="math display">\[131\text{ms (HtoD)} + 6\text{ms
(Kernel)} + 386\text{ms (DtoH)} \approx 523\text{ms}\]</span></p>
<p>我们整个程序的性能瓶颈已经完全在 <code>PCIe</code>
的数据传输上，<strong>我们的思路应该从“优化计算”到“优化搬运”</strong>：</p>
<ol type="1">
<li><strong>引入“锁页内存”（Pinned
Memory）</strong>：我们现在使用的是标准的
<code>malloc</code>。如果我们改用 <code>cudaMallocHost</code>，可以跳过
CPU 的中转缓冲区，让 PCIe 搬运速度直接翻倍。</li>
<li><strong>引入“异步流”（CUDA
Streams）</strong>：现在的程序执行是：<code>DtoH(x) -&gt; DtoH(y) -&gt; DtoH(r) -&gt; add -&gt; HtoD(r)</code>，我们可以参考CPU的流水线改成
<code>DtoH</code>/<code>add</code>/<code>HtoD</code>
并行的模式，也就是读到数据就开始算，算完结果就开始写。</li>
</ol>
<p>这里，我们先尝试使用锁页内存。</p>
<h2 id="优化后的代码-1">优化后的代码</h2>
<p>我们通过 <code>cudaMallocHost</code> 和 <code>cudaFreeHost</code>
来管理我们的锁页内存：注意，<strong><code>cudaMallocHost</code> 只是把
CPU 端的内存“锁”在了物理内存中，防止它被操作系统交换到硬盘（Page
Out），我们仍然需要调用 <code>cudaMemcpy</code>
来将数据移动到显存</strong>。而之所以使用锁页内存比普通内存更快的原因是：</p>
<p>当我们使用普通内存（Pageable Memory）时，CUDA
驱动程序其实偷偷做了两步：：</p>
<ol type="1">
<li><strong>内部拷贝</strong>：驱动程序先将数据从“普通内存”拷贝到一块<strong>隐藏的锁页内存缓存区</strong>中。</li>
<li><strong>DMA 传输</strong>：硬件（DMA
控制器）再将这块隐藏区域的数据通过 PCIe 总线传给 GPU。</li>
</ol>
<p>当我们使用锁页内存（Pinned
Memory）时，由于内存已经“锁死”在物理地址上，不会被系统移动：</p>
<ol type="1">
<li><strong>直接传输</strong>：GPU 的 DMA
控制器可以直接访问这块内存地址，<strong>跳过 CPU 中转</strong>。</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">add_parallel</span><span class="params">(<span class="type">float</span>* x, <span class="type">float</span>* y , <span class="type">float</span>* r, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> index = blockDim.x * blockIdx.x + threadIdx.x;</span><br><span class="line">    <span class="type">int</span> stride = blockDim.x * gridDim.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = index; i &lt; n; i += stride) &#123;</span><br><span class="line">        *(r + i) += *(x + i) + *(y + i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">call_add</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n = <span class="number">100000000</span>;</span><br><span class="line">    <span class="type">int</span> mem_size = <span class="built_in">sizeof</span>(<span class="type">float</span>) * n;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *x, *y, *r;</span><br><span class="line">    <span class="built_in">cudaMallocHost</span>((<span class="type">void</span>**)&amp;x, mem_size);</span><br><span class="line">    <span class="built_in">cudaMallocHost</span>((<span class="type">void</span>**)&amp;y, mem_size);</span><br><span class="line">    <span class="built_in">cudaMallocHost</span>((<span class="type">void</span>**)&amp;r, mem_size);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">        *(x + i) = <span class="number">1</span>;</span><br><span class="line">        *(y + i) = <span class="number">2</span>;</span><br><span class="line">        *(r + i) = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *d_x, *d_y, *d_r;</span><br><span class="line marked">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span>**)&amp;d_x, mem_size);</span><br><span class="line marked">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span>**)&amp;d_y, mem_size);</span><br><span class="line marked">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span>**)&amp;d_r, mem_size);</span><br><span class="line marked">    <span class="built_in">cudaMemcpy</span>(d_x, x, mem_size, cudaMemcpyHostToDevice);</span><br><span class="line marked">    <span class="built_in">cudaMemcpy</span>(d_y, y, mem_size, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    add_parallel&lt;&lt;&lt;<span class="number">136</span>, <span class="number">256</span>&gt;&gt;&gt;(d_x, d_y, d_r, n);</span><br><span class="line marked">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line marked">    <span class="built_in">cudaMemcpy</span>(r, d_r, mem_size, cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">        <span class="keyword">if</span> (*(r + i) != <span class="number">3</span>) &#123;</span><br><span class="line">            count++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (count != <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Total errors: %d\n&quot;</span>, count);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaFreeHost</span>(r);</span><br><span class="line">    <span class="built_in">cudaFreeHost</span>(y);</span><br><span class="line">    <span class="built_in">cudaFreeHost</span>(x);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_r);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_y);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_x);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> <span class="type">const</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> start = std::chrono::high_resolution_clock::<span class="built_in">now</span>();</span><br><span class="line">    <span class="built_in">call_add</span>();</span><br><span class="line">    <span class="keyword">auto</span> end = std::chrono::high_resolution_clock::<span class="built_in">now</span>();</span><br><span class="line">    std::chrono::duration&lt;<span class="type">double</span>, std::milli&gt; elapsed = end - start;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Execution time: %f ms\n&quot;</span>, elapsed.<span class="built_in">count</span>());</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="性能指标">性能指标</h2>
<table>
<colgroup>
<col style="width: 16%" />
<col style="width: 15%" />
<col style="width: 25%" />
<col style="width: 27%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="header">
<th>指标</th>
<th><code>add&lt;&lt;&lt;1, 1&gt;&gt;&gt;</code></th>
<th><code>add_stride&lt;&lt;&lt;1, 256&gt;&gt;&gt;</code></th>
<th>add_parallel&lt;&lt;&lt;136, 256&gt;&gt;&gt;</th>
<th>锁页内存</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Kernels &amp; Memory</td>
<td><code>89.0%</code>/<code>11.0%</code></td>
<td><code>14.5%</code>/<code>85.5%</code></td>
<td><code>1.0%</code>/<code>99.0%</code></td>
<td><code>6.2%</code>/<code>93.8%</code></td>
</tr>
<tr class="even">
<td>Kernal执行时间</td>
<td>4984ms</td>
<td>104ms</td>
<td>6ms</td>
<td>6ms</td>
</tr>
<tr class="odd">
<td>CUDA memcpy DtoH</td>
<td>409ms</td>
<td>456ms</td>
<td>386ms</td>
<td>61ms</td>
</tr>
<tr class="even">
<td>CUDA memcpy HtoD</td>
<td>134ms</td>
<td>137ms</td>
<td>131ms</td>
<td>30ms</td>
</tr>
</tbody>
</table>
<h1 id="第四次优化overlap">第四次优化（Overlap）</h1>
<h2 id="overlap的原理和限制">Overlap的原理和限制</h2>
<p>在经过我们一系列的优化之后，我们的计算逻辑已经被优化很多了，现在我们本次教程的最后一次优化逻辑是：<strong>使用流水线模式来优化我们的计算。</strong></p>
<p>我们目前的计算逻辑是：</p>
<pre><code class="highlight mermaid">block-beta

columns 8

%% 时间轴标题
t0 t1 t2 t3 t4 space:3

%% PCIe 上行 (H2D)
x0(&quot;cudaMemcpy(x)&quot;) y0(&quot;cudaMemcpy(y)&quot;) space:6

%% SM 计算单元
space:2 add0(&quot;add_parallel&quot;) space:5

%% PCIe 下行 (D2H)
space:3 res0(&quot;cudaMemcpy(r)&quot;)

class x0,y0,x1,y1 green
class add0,add1 pink
class res0,res1 blue
class t0,t1,t2,t3,t4,t5,t6,t7,t8,t9,t10,t11 purple


classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<p>而在我们的GPU中：</p>
<ul>
<li>由于 <code>cudaMemcpy(x)</code> 和 <code>cudataMemcpy(y)</code> 共享
PCIe 总线，所以他们只能串行执行；</li>
<li>但是 <code>cudaMemcpy</code> 和 <code>SM计算</code>
则可以实现真正的并行，就像我们CPU的流水线；</li>
</ul>
<p>最优的执行逻辑应该是如下所示：</p>
<pre><code class="highlight mermaid">block-beta
columns 12

%% 时间轴
t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11

%% PCIe 搬运流 (H2D)
x0(&quot;x[0..]&quot;) y0(&quot;y[0..]&quot;) x1(&quot;x[32..]&quot;) y1(&quot;y[32..]&quot;) x2(&quot;x[64..]&quot;) y2(&quot;y[64..]&quot;) x3(&quot;x[96..]&quot;) y3(&quot;y[96..]&quot;) space:4

%% SM 计算流
space:2 ker0(&quot;Add(0)&quot;) space:1 ker1(&quot;Add(1)&quot;) space:1 ker2(&quot;Add(2)&quot;) space:1 ker3(&quot;Add(3)&quot;) space:4

%% 结果回传流 (D2H)
space:3 d2h_0(&quot;r[0..]&quot;) space:1 d2h_1(&quot;r[32..]&quot;) space:1 d2h_2(&quot;r[64..]&quot;) space:1 d2h_3(&quot;r[96..]&quot;) space:2

class x0,x1,x2,x3,y0,y1,y2,y3 green
class ker0,ker1,ker2,ker3 pink
class d2h_0,d2h_1,d2h_2,d2h_3 blue
class t0,t1,t2,t3,t4,t5,t6,t7,t8,t9,t10,t11 purple

classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<p>这种流水线的优化存在一些限制和特点：</p>
<ol type="1">
<li><code>PCIe</code> 是一个全双工的总线，所以可以看到在 <code>t4</code>
时刻，我们的读数据，计算数据，写输出是并行执行的；</li>
<li><strong>只能使用 Pinned Memory</strong>，<strong>如果使用普通
<code>malloc</code> 的内存，<code>cudaMemcpyAsync</code>
会退化为同步传输。</strong> 因为 DMA 只有在锁页内存上才能保证在 CPU
继续运行的同时，安全地自主搬运。</li>
<li><code>cudaMemcpyAsync</code>
是异步调用函数，它将传输任务排队进指定的 <code>stream</code>
中，在调用中立即返回；而在计算时也需要从流中去读取数据；我们使用
<code>cudaMemcpyAsync(x, stream[i])</code> he
<code>add_parallel&lt;&lt;&lt;..., stream[i]&gt;&gt;&gt;(x)</code>
来实现这个逻辑 -- 所以，<strong>流（Stream）本身就是一个硬件级的
FIFO（先入先出）队列</strong>。</li>
<li>只能优化 <code>IO Bound</code> 的应用，考虑一下场景：
<ul>
<li><code>IO Bound</code>：我们读和写IO需要100ms，计算需要10ms；优化前后所需的时间分别是
210ms 和 100ms；</li>
<li><code>Compute Bound</code>：我们读和写IO需要10ms，计算需要100ms；优化前后所需的时间分别是
120ms 和 100ms；</li>
</ul></li>
<li>在引入并行后的时间通常取决于系统中最慢的逻辑：<span
class="math inline">\(Total \approx \max(T_{h2d}, T_{compute},
T_{d2h})\)</span>；</li>
</ol>
<h2 id="优化后的代码-2">优化后的代码</h2>
<p>使用 <code>stream</code> 优化的代码逻辑流程如下：</p>
<ol type="1">
<li>初始化 <code>hostMemory</code> 和 <code>deviceMemory</code>；</li>
<li>初始化 <code>start</code> 和 <code>stop</code>
时间，这个是我们用来记录 CUDA 的真正执行时间的；</li>
<li>初始化
<code>streams</code>，这里需要注意的是，如果我们要真正的使用CUDA的并行能力，那么我们就不能只创建一个
<code>stream</code>，而需要创建一个 <code>stream[]</code>。在 CUDA
中，如果不指定 Stream，所有的操作都会进入<strong>默认流（Default
Stream）</strong>。默认流是一个同步流，它遵循严格的“串行”逻辑，也就是说，我们生命单个流的话，我们的执行方式其实和不使用流是完全一样的：
<ul>
<li><strong>任务 A (Memcpy)</strong> 没做完，<strong>任务 B
(Kernel)</strong> 就绝对不会开始。</li>
<li><strong>第 0 块数据</strong>没处理完，<strong>第 1
块数据</strong>的指令甚至不会被下发。</li>
</ul></li>
<li>根据流的数据量，将我们的数据拆分为多个块。在我们的例子中，我们使用4个stream，那么每个stream负责的应该是
<code>[0, 25m)</code>/<code>[25m, 50m)</code>/<code>[50m, 75m)</code>/<code>[75m, 100m)</code>；我们使用
<code>cudaMemcpyAsync</code> 将这四个块复制到 <code>stream</code>
中；</li>
<li>根据我们的参数，让每个线程负责自己所在区域的值的计算：
<ul>
<li>每个 <code>block</code> 负责的区域应该为：<span
class="math inline">\([N / TotalBlockCount \times BlockId, min((N /
TotalBlockCount) \times (BlockId + 1), N)\)</span>；</li>
<li>在 <code>block</code> 内部，以 <code>BlockThreadCount</code> 作为
<code>stride</code>，让线程在调度时负责自己区域的 <span
class="math inline">\({threadId, threadId + BlockThreadCount,
...}\)</span> 的集合，以便于 <code>warp</code>
在并行执行时可以一次总线访问拿到足量数据；</li>
<li>那 <span class="math inline">\(TotaolBlockCount\)</span>
应该设置多少呢？这里我们需要先理清 <code>block</code>
的含义：<code>block</code>
为一个逻辑概念，它只是指的是数据被拆分为多少个块，每个块启动多少线程去处理这个块里面的数据。在排队中的
<code>block</code> 并不会像 <code>thread</code> 一样占据实体资源，也就是
<code>block</code> 的数量并不会像 <code>thread</code>
数量一样，设置过大的指会影响任务执行速度。相反，由于SM处理速度远超VRAM访问，所以我们正确的策略是让一个SM负责多个
<code>block</code>。<strong>那么，对于每个 stream，我们可以设置
TotalBlockCount 为 <span class="math inline">\((streamSize +
threadsPerBlock- 1) / threadsPerBlock\)</span></strong>；</li>
<li>现在，我们可以使用我们的这些参数加上 <code>stream</code>
来计算我们的结果了
<code>add_parallel&lt;&lt;&lt;blocksPerGrid, threadsPerBlock, 0, streams[i]&gt;&gt;&gt;(ctx.d_x + offset, ctx.d_y + offset, ctx.d_r + offset, streamSize);</code>。这里每个
<code>stream</code> 对应一个
<code>add_parallel()</code>，函数中我们会发现，我们只做一件事
<code>r[i] = x[i] + y[i];</code>，其中
<code>i = blockIdx.x * blockDim.x + threadIdx.x;</code>。这里我们的
<code>block</code> 一直在流转调度。</li>
</ul></li>
<li>复制数据到 host。</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">add_parallel</span><span class="params">(<span class="type">const</span> <span class="type">float</span>* x, <span class="type">const</span> <span class="type">float</span>* y, <span class="type">float</span>* r, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (i &lt; n) &#123;</span><br><span class="line">        r[i] = x[i] + y[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> N = <span class="number">100000000</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> total_bytes = N * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">add_ctx_t</span> ctx = <span class="built_in">init_add_ctx</span>(N, total_bytes);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start);</span><br><span class="line"></span><br><span class="line marked">    <span class="type">const</span> <span class="type">int</span> nStreams = <span class="number">4</span>;</span><br><span class="line marked">    <span class="type">const</span> <span class="type">int</span> streamSize = N / nStreams;</span><br><span class="line marked">    <span class="type">const</span> <span class="type">size_t</span> streamBytes = streamSize * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line marked">    cudaStream_t streams[nStreams];</span><br><span class="line marked">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nStreams; i++) &#123;</span><br><span class="line marked">        <span class="built_in">cudaStreamCreate</span>(&amp;streams[i]);</span><br><span class="line marked">    &#125;</span><br><span class="line marked">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nStreams; i++) &#123;</span><br><span class="line marked">        <span class="type">int</span> offset = i * streamSize;</span><br><span class="line marked"></span><br><span class="line marked">        <span class="built_in">cudaMemcpyAsync</span>(ctx.d_x + offset, ctx.h_x + offset, streamBytes, cudaMemcpyHostToDevice, streams[i]);</span><br><span class="line marked">        <span class="built_in">cudaMemcpyAsync</span>(ctx.d_y + offset, ctx.h_y + offset, streamBytes, cudaMemcpyHostToDevice, streams[i]);</span><br><span class="line marked"></span><br><span class="line marked">        <span class="type">int</span> threadsPerBlock = <span class="number">256</span>;</span><br><span class="line marked">        <span class="type">int</span> blocksPerGrid = (streamSize + threadsPerBlock - <span class="number">1</span>) / threadsPerBlock;</span><br><span class="line marked">        std::cout &lt;&lt; <span class="string">&quot;blocksPerGrid: &quot;</span> &lt;&lt; blocksPerGrid &lt;&lt; std::endl;</span><br><span class="line marked">        add_parallel&lt;&lt;&lt;blocksPerGrid, threadsPerBlock, <span class="number">0</span>, streams[i]&gt;&gt;&gt;(ctx.d_x + offset, ctx.d_y + offset, ctx.d_r + offset, streamSize);</span><br><span class="line marked"></span><br><span class="line marked">        <span class="built_in">cudaMemcpyAsync</span>(ctx.h_r + offset, ctx.d_r + offset, streamBytes, cudaMemcpyDeviceToHost, streams[i]);</span><br><span class="line marked">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop);</span><br><span class="line">    <span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; ++i) &#123;</span><br><span class="line">        <span class="keyword">if</span> (*(ctx.h_r + i) != <span class="number">3</span>) &#123;</span><br><span class="line">            count++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (count != <span class="number">0</span>) &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Error count : &quot;</span> &lt;&lt; count &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> milliseconds = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaEventElapsedTime</span>(&amp;milliseconds, start, stop);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Pipeline Execution Time (GPU Hardware): &quot;</span> &lt;&lt; milliseconds &lt;&lt; <span class="string">&quot; ms&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nStreams; i++) &#123;</span><br><span class="line">        <span class="built_in">cudaStreamDestroy</span>(streams[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">destroy_add_ctx</span>(ctx);</span><br><span class="line">    <span class="built_in">cudaEventDestroy</span>(start); <span class="built_in">cudaEventDestroy</span>(stop);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="性能指标-1">性能指标</h2>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 13%" />
<col style="width: 22%" />
<col style="width: 23%" />
<col style="width: 12%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="header">
<th>指标</th>
<th><code>add&lt;&lt;&lt;1, 1&gt;&gt;&gt;</code></th>
<th><code>add_stride&lt;&lt;&lt;1, 256&gt;&gt;&gt;</code></th>
<th>add_parallel&lt;&lt;&lt;136, 256&gt;&gt;&gt;</th>
<th>锁页内存</th>
<th>async</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Kernels &amp; Memory</td>
<td><code>89.0%</code>/<code>11.0%</code></td>
<td><code>14.5%</code>/<code>85.5%</code></td>
<td><code>1.0%</code>/<code>99.0%</code></td>
<td><code>6.2%</code>/<code>93.8%</code></td>
<td><code>5.0%</code>/<code>95.0%</code></td>
</tr>
<tr class="even">
<td>Kernal执行时间</td>
<td>4984ms</td>
<td>104ms</td>
<td>6ms</td>
<td>6ms</td>
<td>4ms</td>
</tr>
<tr class="odd">
<td>CUDA memcpy DtoH</td>
<td>409ms</td>
<td>456ms</td>
<td>386ms</td>
<td>61ms</td>
<td>61ms</td>
</tr>
<tr class="even">
<td>CUDA memcpy HtoD</td>
<td>134ms</td>
<td>137ms</td>
<td>131ms</td>
<td>30ms</td>
<td>30ms</td>
</tr>
</tbody>
</table>
<p>虽然从性能指标上，没有看到有明显的提升，但是如果我们打开
<code>nsys</code> 的图，我们会发现现在已经是多个 <code>stream</code>
并行执行了：</p>
<figure>
<img src="\images\cuda\cuda-stream-parallel.png" alt="cuda-parallel" />
<figcaption aria-hidden="true">cuda-parallel</figcaption>
</figure>
<h1 id="cuda的二维计算">CUDA的二维计算</h1>
<p>CUDA 的维度设计（1D, 2D,
3D）本质上是提供了一套<strong>内置的坐标换算器</strong>。虽然 GPU
的内存空间在物理上永远是一维（线性）的地址，但现实世界中的数据往往是多维的。</p>
<h2 id="维度层级从点到体">维度层级：从点到体</h2>
<p>CUDA 使用 <code>dim3</code> 结构体来定义维度。如果我们不显式定义
<code>y</code> 和 <code>z</code>，它们默认就是 1。</p>
<table>
<colgroup>
<col style="width: 11%" />
<col style="width: 16%" />
<col style="width: 38%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>维度</strong></th>
<th><strong>逻辑抽象</strong></th>
<th><strong>典型应用场景</strong></th>
<th><strong>定义方式 (示例)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>1D</strong></td>
<td>向量 / 数组</td>
<td>向量加法、数据转换</td>
<td><code>dim3 block(256, 1, 1);</code></td>
</tr>
<tr class="even">
<td><strong>2D</strong></td>
<td>矩阵 / 图像</td>
<td>照片滤镜、矩阵乘法</td>
<td><code>dim3 block(16, 16, 1);</code></td>
</tr>
<tr class="odd">
<td><strong>3D</strong></td>
<td>体素 / 空间</td>
<td>气象模拟、CT 影像、流体力学</td>
<td><code>dim3 block(8, 8, 8);</code></td>
</tr>
</tbody>
</table>
<h2 id="为什么需要映射">为什么需要映射</h2>
<p>在 <code>kernel</code> 内部，我们会得到两组坐标：</p>
<ul>
<li><code>blockIdx</code> 我们的在哪个块；</li>
<li><code>threadIdx</code> 我们在块里哪个位置；</li>
</ul>
<h2 id="matrix-transpose">Matrix Transpose</h2>
<blockquote>
<p>给定一个二维矩阵 <code>in</code>，我们需要对矩阵进行转置：把坐标
<span class="math inline">\((x, y)\)</span> 变成 <span
class="math inline">\((y, x)\)</span></p>
</blockquote>
<p>首先，我们需要清楚一个定义是：<strong>内存里没有真正的
<code>二维数组</code>，它永远是一维线性排列的</strong>，例如假设我们声明一个
<span class="math inline">\(2 \times 3\)</span> 的矩阵：</p>
<pre><code class="highlight mermaid">block-beta

columns 7

memory(&quot;memory&quot;) m0(&quot;matrix[0][0]&quot;) m1(&quot;matrix[0][1]&quot;) m2(&quot;matrix[1][0]&quot;) m3(&quot;matrix[1][1]&quot;) m4(&quot;matrix[2][0]&quot;) m5(&quot;matrix[2][1]&quot;)

row1(&quot;row1&quot;) r0(&quot;matrix[0][0]&quot;) r1(&quot;matrix[0][1]&quot;) space:4
row2(&quot;row2&quot;) r2(&quot;matrix[1][0]&quot;) r3(&quot;matrix[1][1]&quot;) space:4
row3(&quot;row3&quot;) r4(&quot;matrix[2][0]&quot;) r5(&quot;matrix[2][1]&quot;)

class memory,row1,row2,row3 purple

class m0,m1,r0,r1 green
class m2,m3,r2,r3 blue
class m4,m5,r4,r5 yellow


classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<p>按照我们在 <a href="#第四次优化">第四次优化</a> 中的代码逻辑：</p>
<ol type="1">
<li>将这个矩阵分成多个 <code>block</code>
以便于我们可以并行的执行；</li>
<li>为每个 <code>block</code> 分配合理的 <code>threads</code>
以便于可以最大化的利用GPU的并行能力；</li>
<li><strong>每个核函数只处理它所在的 <code>block</code> 和
<code>thread</code> 应该负责的点，通过 <code>block</code>
的调度来处理整个矩阵。</strong></li>
<li>那么我们调用核函数
<code>transpose_kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;</code>
，此时我们会面临一个问题，<strong>当前线程只知道 blockId 和
threadId，而处理矩阵需要的是当前元素在矩阵中的 <code>row</code> 和
<code>column</code>。</strong>我们可以去计算这个值，公式并不复杂：<code>row = (blockId * blocksPerBlock + threadId) / width</code>，<code>column = (blockId * blocksPerBlock + threadId) % width</code>，但是使得程序不直观，并且引入极大的额外开销（整数除法和取模是非常“昂贵”的操作，它们需要消耗数十个时钟周期）。所以
CUDA 引入了 <code>dim3 grid(x, y)</code> 来对这个逻辑进行抽象。</li>
<li>在引入 <code>dim3</code>
之后，我们可以通过更便捷的方式来访问我们的矩阵：
<ul>
<li><code>x = blockIdx.x * blockDim.x + threadIdx.x;</code></li>
<li><code>y = blockIdx.y * blockDim.y + threadIdx.y;</code></li>
</ul></li>
<li>此时，我们的核函数就会存在如下调用形式
<code>transpose_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code>，其中
<code>grid</code> 和 <code>block</code> 都是 <code>dim3</code>
矩阵，当我们调用
<code>transpose_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code>
时，我们通过 <code>dim3</code>
将原本一维的索引空间<strong>‘重塑’（Reshape）</strong>成了多维的坐标系。此时，<strong>Grid</strong>
不再是一条长线，而是一个由多个 Block
组成的<strong>网格阵列</strong>；每个 <strong>Block</strong>
内部也不再是简单的序号，而是一群拥有 <code>(x, y)</code>
坐标的<strong>线程矩阵</strong>。</li>
</ol>
<h3 id="分割一个矩阵的思路">分割一个矩阵的思路</h3>
<p>我们可以这样理解 CUDA 的多维处理逻辑：</p>
<h4 id="物理本质一维的长阵">物理本质：一维的长阵</h4>
<p>首先必须明确：内存中不存在真正的二维矩阵，它在物理上是一个<strong>超长的线性数组</strong>。如果我们要处理一个宽度为
<code>w</code>、高度为 <code>h</code>
的矩阵，在显存里它就是一段连续的、长度为 <code>w * h</code> 的空间。</p>
<h4 id="逻辑重塑">逻辑重塑</h4>
<p>在之前的一维处理逻辑中，如果我们决定使用 256 个线程作为一个
<strong>Block</strong>（线程块），那么所需的 Block 数量就是
<code>(w * h + 255) / 256</code>。此时核函数的启动方式为：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">transpose</span>&lt;&lt;&lt;(w * h + <span class="number">255</span>) / <span class="number">256</span>, <span class="number">256</span>&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<p>这种方式虽然可行，但在处理二维矩阵数据时，我们需要在核函数内部进行大量代价高昂的<strong>除法（<code>/</code>）和取余（<code>%</code>）</strong>操作来反推行列坐标，这会严重拖累
GPU 性能。</p>
<h4 id="二级矩阵block最小分配单元">二级矩阵（Block）：最小分配单元</h4>
<p>为了优化逻辑，CUDA 引入了 <code>dim3</code> 结构。我们仍然维持每个
Block 包含 256 个线程，但将其重塑为一个 <strong><span
class="math inline">\(16 \times 16\)</span> 的二级矩阵</strong>：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">dim3</span> block(<span class="number">16</span>, <span class="number">16</span>);</span><br></pre></td></tr></table></figure>
<p>此时，这 256 个线程中的每一个都对应这个二级矩阵中的一个点。CUDA
会自动为每个线程生成内置变量 <strong><code>threadIdx.x</code></strong>
和
<strong><code>threadIdx.y</code></strong>，用来表示该线程在小组内部的坐标。这个“线程方阵”是
GPU 调度的最小单元，会被整体绑定到 SM（流式多处理器）上执行。</p>
<h4 id="一级矩阵grid全局任务划分">一级矩阵（Grid）：全局任务划分</h4>
<p>接着，我们将整个大矩阵按照这个 <span class="math inline">\(16 \times
16\)</span>
的尺寸进行切割。我们不再计算总数，而是分别计算横向和纵向需要多少个
Block：</p>
<ul>
<li><strong>水平方向（列数）所需 Block
数</strong>：<code>grid_x = (w + block.x - 1) / block.x;</code></li>
<li><strong>垂直方向（行数）所需 Block
数</strong>：<code>grid_y = (h + block.y - 1) / block.y;</code></li>
</ul>
<p>这就构成了 <strong>一级矩阵（Grid）</strong>：</p>
<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dim3 <span class="attribute">grid</span>(grid_x, grid_y);</span><br></pre></td></tr></table></figure>
<p>此时，我们的核函数调用变为：</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">transpose</span>&lt;&lt;&lt;<span class="built_in">grid</span>, <span class="built_in">block</span>&gt;&gt;&gt;;</span><br></pre></td></tr></table></figure>
<h4 id="坐标合成与索引还原">坐标合成与索引还原</h4>
<p>现在的任务被我们划分成了：<strong>一个由 Block 组成的大矩阵，而每个
Block 内部又是一个由 Thread 组成的子矩阵。</strong></p>
<p>在核函数执行时，定位过程如下：</p>
<ol type="1">
<li><strong>一级定位</strong>：通过
<strong><code>blockIdx.x</code></strong> 和
<strong><code>blockIdx.y</code></strong>
找到当前线程所属的“一级矩阵”（Block）在大网格中的位置。</li>
<li><strong>二级定位</strong>：通过
<strong><code>threadIdx.x</code></strong> 和
<strong><code>threadIdx.y</code></strong>
找到当前线程在“二级矩阵”（小组）内部的精确坐标。</li>
<li><strong>合成全局坐标</strong>：
<ul>
<li><code>element_x = blockIdx.x * blockDim.x + threadIdx.x;</code></li>
<li><code>element_y = blockIdx.y * blockDim.y + threadIdx.y;</code></li>
</ul></li>
</ol>
<p>最后，通过这两个逻辑坐标，我们就能避开复杂的算术运算，直接计算出元素在物理数组中的实际索引，从而完成高效的数据处理。</p>
<h3 id="矩阵分割的实例">矩阵分割的实例</h3>
<p>假设，我们存在一个 <code>7 * 7</code>
的矩阵，我们现在需要使用我们的矩阵分割逻辑来处理这个矩阵，我们使用
<code>2 * 2</code> 的线程矩阵来对它进行第一次分割：</p>
<pre><code class="highlight mermaid">block-beta

columns 9

%% 定义表头（列坐标轴 x）
space:1 c0(&quot;0&quot;) c1(&quot;1&quot;) c2(&quot;2&quot;) c3(&quot;3&quot;) c4(&quot;4&quot;) c5(&quot;5&quot;) c6(&quot;6&quot;) c7(&quot;7&quot;)

%% 第一行 (y=0)
r0(&quot;0&quot;) 
r0c0(&quot;0&quot;) r0c1(&quot;1&quot;) r0c2(&quot;2&quot;) r0c3(&quot;3&quot;) r0c4(&quot;4&quot;) r0c5(&quot;5&quot;) r0c6(&quot;6&quot;) e0(&quot;IndexOut&quot;)

%% 第二行 (y=1)
r1(&quot;1&quot;) 
r1c0(&quot;7&quot;) r1c1(&quot;8&quot;) r1c2(&quot;9&quot;) r1c3(&quot;10&quot;) r1c4(&quot;11&quot;) r1c5(&quot;12&quot;) r1c6(&quot;13&quot;) e1(&quot;IndexOut&quot;)

%% 第三行 (y=2)
r2(&quot;2&quot;) 
r2c0(&quot;14&quot;) r2c1(&quot;15&quot;) r2c2(&quot;16&quot;) r2c3(&quot;17&quot;) r2c4(&quot;18&quot;) r2c5(&quot;19&quot;) r2c6(&quot;20&quot;) e2(&quot;IndexOut&quot;)

%% 第四行 (y=3)
r3(&quot;3&quot;) 
r3c0(&quot;21&quot;) r3c1(&quot;22&quot;) r3c2(&quot;23&quot;) r3c3(&quot;24&quot;) r3c4(&quot;25&quot;) r3c5(&quot;26&quot;) r3c6(&quot;27&quot;) e3(&quot;IndexOut&quot;)

%% 第五行 (y=4)
r4(&quot;4&quot;) 
r4c0(&quot;28&quot;) r4c1(&quot;29&quot;) r4c2(&quot;30&quot;) r4c3(&quot;31&quot;) r4c4(&quot;32&quot;) r4c5(&quot;33&quot;) r4c6(&quot;34&quot;) e4(&quot;IndexOut&quot;)

%% 第六行 (y=5)
r5(&quot;5&quot;) 
r5c0(&quot;35&quot;) r5c1(&quot;36&quot;) r5c2(&quot;37&quot;) r5c3(&quot;38&quot;) r5c4(&quot;39&quot;) r5c5(&quot;40&quot;) r5c6(&quot;41&quot;) e5(&quot;IndexOut&quot;)

%% 第七行 (y=6)
r6(&quot;6&quot;) 
r6c0(&quot;42&quot;) r6c1(&quot;43&quot;) r6c2(&quot;44&quot;) r6c3(&quot;45&quot;) r6c4(&quot;46&quot;) r6c5(&quot;47&quot;) r6c6(&quot;48&quot;) e6(&quot;IndexOut&quot;)

r7(&quot;7&quot;) e7(&quot;IndexOut&quot;) e8(&quot;IndexOut&quot;) e9(&quot;IndexOut&quot;) e10(&quot;IndexOut&quot;) e11(&quot;IndexOut&quot;) e12(&quot;IndexOut&quot;) e13(&quot;IndexOut&quot;) e14(&quot;IndexOut&quot;)

class r0c0,r0c1,r1c0,r1c1 light_green
class r0c2,r0c3,r1c2,r1c3 gray
class r0c4,r0c5,r1c4,r1c5 content
class r0c6,e0,r1c6,e1 blue
class r2c0,r2c1,r3c0,r3c1 yellow
class r2c2,r2c3,r3c2,r3c3 orange
class r2c4,r2c5,r3c4,r3c5 pink
class r2c6,e2,r3c6,e3 green
class r4c0,r4c1,r5c0,r5c1 light_green
class r4c2,r4c3,r5c2,r5c3 gray
class r4c4,r4c5,r5c4,r5c5 content
class r4c6,e4,r5c6,e5 blue
class r6c0,r6c1,e7,e8 yellow
class r6c2,r6c3,e9,e10 orange
class r6c4,r6c5,e11,e12 pink
class r6c6,e13,e14,e6 green

class r0,r1,r2,r3,r4,r5,r6,r7,c0,c1,c2,c3,c4,c5,c6,c7 purple

%% 样式定义
classDef content fill:#fff,stroke:#ccc;
classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef light_green fill:#e8f5e9,stroke:#695;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<p>随后，我们使用 <code>grid</code> 来对我们的线性矩阵进行一次分割：</p>
<pre><code class="highlight mermaid">block-beta
columns 9

space:1 
c0(&quot;0&quot;):2 
c1(&quot;1&quot;):2 
c2(&quot;2&quot;):2 
c3(&quot;3&quot;):2

%% 第一大行 r0
r0(&quot;0&quot;)
block:b01:2
    columns 2
    0(&quot;0&quot;) 1(&quot;1&quot;)
    7(&quot;7&quot;) 8(&quot;8&quot;)
end
block:b02:2
    columns 2
    2(&quot;2&quot;) 3(&quot;3&quot;)
    9(&quot;9&quot;) 10(&quot;10&quot;)
end
block:b03:2
    columns 2
    4(&quot;4&quot;) 5(&quot;5&quot;)
    11(&quot;11&quot;) 12(&quot;12&quot;)
end
block:b04:2
    columns 2
    6(&quot;6&quot;) e1(&quot;IndexOut&quot;)
    13(&quot;13&quot;) e2(&quot;IndexOut&quot;)
end

%% 第二大行 r1
r1(&quot;1&quot;)
block:b11:2
    columns 2
    14(&quot;14&quot;) 15(&quot;15&quot;)
    21(&quot;21&quot;) 22(&quot;22&quot;)
end
block:b12:2
    columns 2
    16(&quot;16&quot;) 17(&quot;17&quot;)
    23(&quot;23&quot;) 24(&quot;24&quot;)
end
block:b13:2
    columns 2
    18(&quot;18&quot;) 19(&quot;19&quot;)
    25(&quot;25&quot;) 26(&quot;26&quot;)
end
block:b14:2
    columns 2
    20(&quot;20&quot;) e3(&quot;IndexOut&quot;)
    27(&quot;27&quot;) e4(&quot;IndexOut&quot;)
end

%% 第三大行 r2
r2(&quot;2&quot;)
block:b21:2
    columns 2
    28(&quot;28&quot;) 29(&quot;29&quot;)
    35(&quot;35&quot;) 36(&quot;36&quot;)
end
block:b22:2
    columns 2
    30(&quot;30&quot;) 31(&quot;31&quot;)
    37(&quot;37&quot;) 38(&quot;38&quot;)
end
block:b23:2
    columns 2
    32(&quot;32&quot;) 33(&quot;33&quot;)
    39(&quot;39&quot;) 40(&quot;40&quot;)
end
block:b24:2
    columns 2
    34(&quot;34&quot;) e5(&quot;IndexOut&quot;)
    41(&quot;41&quot;) e6(&quot;IndexOut&quot;)
end

%% 第四大行 r3 (纵向越界层)
r3(&quot;3&quot;)
block:b31:2
    columns 2
    42(&quot;42&quot;) 43(&quot;43&quot;)
    e7(&quot;IndexOut&quot;) e8(&quot;IndexOut&quot;)
end
block:b32:2
    columns 2
    44(&quot;44&quot;) 45(&quot;45&quot;)
    e9(&quot;IndexOut&quot;) e10(&quot;IndexOut&quot;)
end
block:b33:2
    columns 2
    46(&quot;46&quot;) 47(&quot;47&quot;)
    e11(&quot;IndexOut&quot;) e12(&quot;IndexOut&quot;)
end
block:b34:2
    columns 2
    48(&quot;48&quot;) e13(&quot;IndexOut&quot;)
    e14(&quot;IndexOut&quot;) e15(&quot;IndexOut&quot;)
end

class c0,c1,c2,c3,r0,r1,r2,r3 purple
class b01,b02,b03,b04,b11,b12,b13,b14,b21,b22,b23,b24,b31,b32,b33,b34 blockContainer
class 0,1,7,8 light_green
class 2,3,9,10 gray
class 4,5,11,12 content
class 6,e1,13,e2 blue

class 14,15,21,22 yellow
class 16,17,23,24 orange
class 18,19,25,26 pink
class 20,27,e3,e4 green
class 28,29,35,36 light_green
class 30,31,37,38 gray
class 32,33,39,40 content
class 34,41,e5,e6 blue
class 42,43,e7,e8 yellow
class 44,45,e9,e10 orange
class 46,47,e11,e12 pink
class 48,e13,e14,e15 green

classDef content fill:#fff,stroke:#ccc;
classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef light_green fill:#e8f5e9,stroke:#695;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;

classDef blockContainer fill:#fff3e0,stroke:#ef6c00,stroke-dasharray: 5 5;
classDef axis fill:#f9f9f9,stroke:#333,stroke-width:2px,font-weight:bold;</code></pre>
<p>现在，对于任意一个节点，我们都可以知道：</p>
<ul>
<li><code>x = blockIdx.x * blockDim.x + threadIdx.x</code></li>
<li><code>y = blockId.y * blockDim.y + threadIdx.y</code></li>
<li>例如对于 <code>9</code>，它的实际索引是 <code>(1, 2)</code> =
<code>(0 * 2 + 1, 1 * 2 + 0)</code></li>
<li>而它的的在内存的实际索引也可以通过 <code>x * width + y</code>
计算</li>
</ul>
<h2 id="最后代码">最后代码</h2>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">transpose_kernel</span><span class="params">(<span class="type">float</span> *out, <span class="type">const</span> <span class="type">float</span> *in, <span class="type">int</span> width, <span class="type">int</span> height)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (x &lt; width &amp;&amp; y &lt; height)</span><br><span class="line">    &#123;</span><br><span class="line">        out[x * height + y] = in[y * width + x];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> <span class="type">const</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> nx = <span class="number">1024</span>;</span><br><span class="line">    <span class="type">int</span> ny = <span class="number">512</span>;</span><br><span class="line">    <span class="type">size_t</span> total_bytes = nx * ny * <span class="built_in">sizeof</span>(<span class="type">int</span>);</span><br><span class="line">    <span class="type">float</span> *h_in, *h_out;</span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>(&amp;h_in, total_bytes, cudaHostAllocDefault);</span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>(&amp;h_out, total_bytes, cudaHostAllocDefault);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nx * ny; i++) &#123;</span><br><span class="line">        h_in[i] = i;</span><br><span class="line">        h_out[i] = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">((nx + block.x - <span class="number">1</span>) / block.x, (ny + block.y - <span class="number">1</span>) / block.y)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *d_in;</span><br><span class="line">    <span class="type">float</span> *d_out;</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_in, total_bytes);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_out, total_bytes);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(d_in, h_in, total_bytes, cudaMemcpyHostToDevice);</span><br><span class="line">    transpose_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_out, d_in, nx, ny);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(h_out, d_out, total_bytes, cudaMemcpyDeviceToHost);</span><br><span class="line">    <span class="type">int</span> errorCount = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> r = <span class="number">0</span>; r &lt; ny; r++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> c = <span class="number">0</span>; c &lt; nx; c++) &#123;</span><br><span class="line">            <span class="type">int</span> index_in = r * nx + c;</span><br><span class="line">            <span class="type">int</span> index_out = c * ny + r;</span><br><span class="line">            <span class="keyword">if</span> (h_in[index_in] != h_out[index_out]) &#123;</span><br><span class="line">                errorCount++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (errorCount &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;error count: &quot;</span> &lt;&lt; errorCount &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="使用sharedmemory优化矩阵转置">使用SharedMemory优化矩阵转置</h1>
<p>在我们的矩阵转置实现中，我们很容易发现有个严重拉低我们性能的点，那就是我们在数据读取时是合并访问的，<strong>然而我们数据在输出时，因为我们已经不是合并输出的了。</strong></p>
<p>这里，我们需要先理清楚一个逻辑：参考 <a
href="#矩阵分割的实例">矩阵分割的实例</a> 中的描述，如果CUDA在调度时以
<code>block</code> 作为块输入，那么我们的第一个 <code>block</code>
应该是 <code>{0, 1, 7, 8}</code> --
这是否意味着我们的输入也没有合并输入呢？答案是：<strong>CUDA
既抽象了矩阵逻辑，又严格按照线程束（Warp）的线性顺序执行。</strong></p>
<p>在我们这个为了演示的例子中，我们的这个 <code>block</code> 中：</p>
<ul>
<li><code>{0, 1}</code> 是合并访问的， <code>{7, 8}</code>
也是合并访问的；</li>
<li><code>{0, 1}</code> 和 <code>{7, 8}</code> 则是不合并访问的。</li>
</ul>
<p>但是这个逻辑很好解决，我们只需要优化我们的核函数的参数到
<code>&lt;&lt;&lt;32, y, z&gt;&gt;&gt;</code> 即可。现在我们需要使用
<code>SharedMemory</code> 来优化我们的写入。</p>
<h2
id="为什么会出现不能合并访问的问题">为什么会出现不能合并访问的问题</h2>
<p>假设原矩阵是 <span class="math inline">\(A\)</span>，转置后的矩阵是
<span class="math inline">\(B\)</span>。根据定义，如果 <span
class="math inline">\(A\)</span> 中的一个点坐标是 <span
class="math inline">\((Global\_X, Global\_Y)\)</span>，那么它在 <span
class="math inline">\(B\)</span> 中的位置必须是 <span
class="math inline">\((Global\_Y, Global\_X)\)</span>。</p>
<p>在读取阶段，一个线程处理的原始全局坐标是：</p>
<ul>
<li><span class="math inline">\(Global\_X_{in} = blockIdx.x \times
blockDim.x + threadIdx.x\)</span></li>
<li><span class="math inline">\(Global\_Y_{in} = blockIdx.y \times
blockDim.y + threadIdx.y\)</span></li>
</ul>
<p>我们要把这个点搬到新矩阵 <span class="math inline">\(B\)</span>
的对称位置：</p>
<ul>
<li><span class="math inline">\(Global\_X_{out} =
Global\_Y_{in}\)</span></li>
<li><span class="math inline">\(Global\_Y_{out} =
Global\_X_{in}\)</span></li>
</ul>
<p>如果我们直接代入，会得到：</p>
<ul>
<li><span class="math inline">\(Global\_X_{out} = blockIdx.y \times
blockDim.y + \mathbf{threadIdx.y}\)</span></li>
<li><span class="math inline">\(Global\_Y_{out} = blockIdx.x \times
blockDim.x + \mathbf{threadIdx.x}\)</span></li>
</ul>
<p><strong>此时，问题出现，我们没有办法合并访问了：</strong></p>
<ul>
<li>通常，为了能够合并访问，我们的每个 warp 会处理 block
中的同一行，而矩阵中的同一行的特点是：<code>y</code>
不变，<code>x</code> 递增；</li>
<li><strong>而我们的 <span
class="math inline">\(Global\_Y_{out}\)</span> 依赖于
<code>x</code>，这表示我们出现了行偏移，也就说我们的全局地址将不再连续。</strong></li>
</ul>
<p>解决方案是：<strong>我们变化的只能是 <span
class="math inline">\(Global\_X_{out}\)</span>，只有这样才能保证我们的全局地址连续以便于我们可以合并访问</strong>。这个逻辑很好理解，因为全局地址是
<span class="math inline">\(y * width + x\)</span>，任意的
<code>y</code> 变化都会使得我们的索引直接偏移出一次 Memory Burst
的可读范围。<strong>无论逻辑上我们在算什么，我们必须强行分配任务，让
<code>threadIdx.x</code>
永远与地址计算公式中权重最小的那个项绑定。</strong>这个结论及时扩展到三维一样成立，因为三维的空间中，全局索引等于
<span class="math inline">\(z * width * height + y * width +
x\)</span>。</p>
<h2 id="如何处理合并访问异常">如何处理合并访问异常</h2>
<p>正如我们前面提到的，在任何计算中，<code>threadIdx.x</code>
永远与地址计算公式中权重最小的那个项绑定，也就是，我们要通过某种方式，将我们的公式转换为：</p>
<ul>
<li><span class="math inline">\(Global\_X_{out} = blockIdx.y \times
blockDim.y + \mathbf{threadIdx.x}\)</span></li>
<li><span class="math inline">\(Global\_Y_{out} = blockIdx.x \times
blockDim.x + \mathbf{threadIdx.y}\)</span></li>
</ul>
<p>而这个逻辑实现并不复杂，我们知道，我们是矩阵嵌套，而外层的矩阵对应的是
<code>block</code>，它的先后执行顺序对我们的结果和性能不会有影响。我们只需要预先将内层矩阵进行一次矩阵转置，此时我们便可以将我们的索引绑定到
<code>threadIdx.x</code>。</p>
<h2 id="实际代码">实际代码</h2>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">transpose_tiled_kernel</span><span class="params">(<span class="type">float</span> *out, <span class="type">const</span> <span class="type">float</span> *in, <span class="type">int</span> width, <span class="type">int</span> height)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> tile[<span class="number">32</span>][<span class="number">32</span>];</span><br><span class="line">    <span class="type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    <span class="keyword">if</span> (x &lt; width &amp;&amp; y &lt; height) &#123;</span><br><span class="line">        tile[threadIdx.y][threadIdx.x] = in[y * width + x];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    <span class="comment">// Assuming that we have a 4 * 4 matrix, the matrix is:</span></span><br><span class="line">    <span class="comment">// 0 1 2 3</span></span><br><span class="line">    <span class="comment">// 4 5 6 7</span></span><br><span class="line">    <span class="comment">// 8 9 a b</span></span><br><span class="line">    <span class="comment">// c d e f</span></span><br><span class="line">    <span class="comment">// Assuming that we&#x27;re running block 0, so the tile will be :</span></span><br><span class="line">    <span class="comment">// 0 4</span></span><br><span class="line">    <span class="comment">// 1 5</span></span><br><span class="line">    <span class="comment">// Now the submatrix is updated in shared memory, what we have to do now is to update the value in current block to the new block.</span></span><br><span class="line">    <span class="type">int</span> new_x = blockIdx.y * blockDim.y+ threadIdx.x;</span><br><span class="line">    <span class="type">int</span> new_y = blockIdx.x * blockDim.x + threadIdx.y;</span><br><span class="line">    out[new_y * height + new_x] = tile[threadIdx.x][threadIdx.y];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="sharedmemory下的矩阵转换实例">SharedMemory下的矩阵转换实例</h2>
<blockquote>
<p>假设一次 <code>Memory Burst</code> 读取到的是两个
<code>float</code>。</p>
</blockquote>
<p>我们的初始矩阵如下，我们读取时是可以合并访问的，因为
<code>block</code> 中的每行的全局索引中是连续的：</p>
<pre><code class="highlight mermaid">block-beta

columns 5

space:1 c0[&quot;0&quot;] c1[&quot;1&quot;] c2[&quot;2&quot;] c3[&quot;3&quot;]
r0[&quot;0&quot;]
block:s1:2
    0(&quot;0&quot;) 1(&quot;1&quot;)
end
block:s2:2
    2(&quot;2&quot;) 3(&quot;3&quot;)
end
r1[&quot;1&quot;]
block:s3:2
    4(&quot;4&quot;) 5(&quot;5&quot;)
end
block:s4:2
    6(&quot;6&quot;) 7(&quot;7&quot;)
end
r2[&quot;2&quot;]
block:s5:2
    8(&quot;8&quot;) 9(&quot;9&quot;)
end
block:s6:2
    a(&quot;a&quot;) b(&quot;b&quot;)
end
r3[&quot;3&quot;]
block:s7:2
    c(&quot;c&quot;) d(&quot;d&quot;)
end
block:s8:2
    e(&quot;e&quot;) f(&quot;f&quot;)
end

class c0,c1,c2,c3,r0,r1,r2,r3 purple

class 0,1,4,5 light_green
class 2,3,6,7 gray
class 8,9,c,d yellow
class a,b,e,f orange

class s1,s2,s3,s4,s5,s6,s7,s8 animate

%% 样式定义
classDef content fill:#fff,stroke:#ccc;
classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef light_green fill:#e8f5e9,stroke:#695;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<p>如果我们直接在
<code>block</code>的线程中使用矩阵转置，我们每次的写入都相当于是一次随机的内存访问，并不能实现合并访问：</p>
<pre><code class="highlight mermaid">block-beta

columns 5

block:columns:5
    columns 5
    space:6 c0[&quot;0&quot;] c1[&quot;1&quot;] c2[&quot;2&quot;] c3[&quot;3&quot;]
end
block:s0:1
    columns 1
    r0[&quot;0&quot;]
    r1[&quot;1&quot;]
end
block:s1:1
    columns 1
    0(&quot;0&quot;)
    1(&quot;1&quot;)
end
block:s2:1
    columns 1
    4(&quot;4&quot;)
    5(&quot;5&quot;)
end
block:s3:1
    columns 1
    8(&quot;8&quot;)
    9(&quot;9&quot;)
end
block:s4:1
    columns 1
    c(&quot;c&quot;)
    d(&quot;d&quot;)
end
block:s9:1
    columns 1
    r2[&quot;2&quot;]
    r3[&quot;3&quot;]
end
block:s5:1
    columns 1
    2(&quot;2&quot;)
    3(&quot;3&quot;)
end
block:s6:1
    columns 1
    6(&quot;6&quot;)
    7(&quot;7&quot;)
end
block:s7:1
    columns 1
    a(&quot;a&quot;)
    b(&quot;b&quot;)
end
block:s8:1
    columns 1
    e(&quot;e&quot;)
    f(&quot;f&quot;)
end

class c0,c1,c2,c3,r0,r1,r2,r3 purple

class 0,1,4,5 light_green
class 2,3,6,7 gray
class 8,9,c,d yellow
class a,b,e,f orange
class columns,s0,s9 transparent
class s1,s2,s3,s4,s5,s6,s7,s8 error


classDef transparent fill:none,stroke:none,color:inherit;
classDef content fill:#fff,stroke:#ccc;
classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef light_green fill:#e8f5e9,stroke:#695;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<p>我们使用<code>Shared Memory</code>
将内部矩阵转置，那么我们得到，<strong>Shared Memory
并没有需要合并写入的需求</strong>：</p>
<pre><code class="highlight mermaid">block-beta

columns 5

space:1 c0[&quot;0&quot;] c1[&quot;1&quot;] c2[&quot;2&quot;] c3[&quot;3&quot;]
r0[&quot;0&quot;]
block:s1:2
    0(&quot;0&quot;) 1(&quot;4&quot;)
end
block:s2:2
    2(&quot;2&quot;) 3(&quot;6&quot;)
end
r1[&quot;1&quot;]
block:s3:2
    4(&quot;1&quot;) 5(&quot;5&quot;)
end
block:s4:2
    6(&quot;3&quot;) 7(&quot;7&quot;)
end
r2[&quot;2&quot;]
block:s5:2
    8(&quot;8&quot;) 9(&quot;c&quot;)
end
block:s6:2
    a(&quot;a&quot;) b(&quot;e&quot;)
end
r3[&quot;3&quot;]
block:s7:2
    c(&quot;9&quot;) d(&quot;d&quot;)
end
block:s8:2
    e(&quot;b&quot;) f(&quot;f&quot;)
end

class c0,c1,c2,c3,r0,r1,r2,r3 purple

class 0,1,4,5 light_green
class 2,3,6,7 gray
class 8,9,c,d yellow
class a,b,e,f orange

class s1,s2,s3,s4,s5,s6,s7,s8 animate

%% 样式定义
classDef content fill:#fff,stroke:#ccc;
classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef light_green fill:#e8f5e9,stroke:#695;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<p>随后，我们再转置外层的矩阵得到：</p>
<pre><code class="highlight mermaid">block-beta

columns 5

space:1 c0[&quot;0&quot;] c1[&quot;1&quot;] c2[&quot;2&quot;] c3[&quot;3&quot;]
r0[&quot;0&quot;]
block:s1:2
    0(&quot;0&quot;) 1(&quot;4&quot;)
end
block:s5:2
    8(&quot;8&quot;) 9(&quot;c&quot;)
end
r1[&quot;1&quot;]
block:s3:2
    4(&quot;1&quot;) 5(&quot;5&quot;)
end

block:s7:2
    c(&quot;9&quot;) d(&quot;d&quot;)
end
r2[&quot;2&quot;]
block:s2:2
    2(&quot;2&quot;) 3(&quot;6&quot;)
end

block:s6:2
    a(&quot;a&quot;) b(&quot;e&quot;)
end
r3[&quot;3&quot;]

block:s4:2
    6(&quot;3&quot;) 7(&quot;7&quot;)
end
block:s8:2
    e(&quot;b&quot;) f(&quot;f&quot;)
end

class c0,c1,c2,c3,r0,r1,r2,r3 purple

class 0,1,4,5 light_green
class 2,3,6,7 gray
class 8,9,c,d yellow
class a,b,e,f orange

class s1,s2,s3,s4,s5,s6,s7,s8 animate

%% 样式定义
classDef content fill:#fff,stroke:#ccc;
classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef light_green fill:#e8f5e9,stroke:#695;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<h2 id="优化性能对比">优化性能对比</h2>
<p>可以看到：</p>
<ul>
<li>我们的合并访问版本比非合并访问版本，核函数执行速度提升了接近30%；</li>
<li>无论我们如何优化内核，这两个数值几乎是不动的，而这个指标非常大，说明我们的程序瓶颈在
<strong>PCIe 硬件</strong>。内核优化虽然能把 21ms 缩减到 15ms，但对于
328ms 的搬运来说，用户体感不明显。：
<ul>
<li><code>[CUDA memcpy Host-to-Device]</code>：约 <strong>166
ms</strong></li>
<li><code>[CUDA memcpy Device-to-Host]</code>：约 <strong>162
ms</strong></li>
</ul></li>
</ul>
<table>
<thead>
<tr class="header">
<th>指标</th>
<th>shared memory</th>
<th>无shared memory</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>cuda_gpu_kern_sum</td>
<td>21ms</td>
<td>30ms</td>
</tr>
<tr class="even">
<td>CUDA memcpy DtoH</td>
<td>166ms</td>
<td>166ms</td>
</tr>
<tr class="odd">
<td>CUDA memcpy HtoD</td>
<td>162ms</td>
<td>162ms</td>
</tr>
</tbody>
</table>
<h1 id="sgemm">SGEMM</h1>
<blockquote>
<p><code>SGEMM</code> 表示 <strong>S</strong>ingle-precision
<strong>GE</strong>neral <strong>M</strong>atrix
<strong>M</strong>ultiply （<strong>单精度通用矩阵乘法</strong>）。</p>
<p>它是 <strong>BLAS</strong>（Basic Linear Algebra
Subprograms，基础线性代数子程序库）标准中的核心函数。简单来说，它实现的就是最基本的数学运算：</p>
<p><span class="math display">\[C = \alpha (A \times B) + \beta
C\]</span></p>
<p>（在大多数简单的 CUDA 练习中，通常令 <span
class="math inline">\(\alpha = 1, \beta = 0\)</span>，即简化的 <span
class="math inline">\(C = A \times
B\)</span>），我们这里来实现这个简化的版本。</p>
</blockquote>
<h2 id="sgemm的工具函数">SGEMM的工具函数</h2>
<p>为了方便，我们直接在这里将 <code>SGEMM</code>
抽象出来多个函数，为我们执行资源管理：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;util.cuh&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">host_sgemm_t</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">float</span> *A;</span><br><span class="line">    <span class="type">float</span> *B;</span><br><span class="line">    <span class="type">float</span> *C;</span><br><span class="line">    <span class="type">int</span> M;</span><br><span class="line">    <span class="type">int</span> N;</span><br><span class="line">    <span class="type">int</span> K;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">device_sgemm_t</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">float</span> *A;</span><br><span class="line">    <span class="type">float</span> *B;</span><br><span class="line">    <span class="type">float</span> *C;</span><br><span class="line">    <span class="type">int</span> M;</span><br><span class="line">    <span class="type">int</span> N;</span><br><span class="line">    <span class="type">int</span> K;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">bool</span> <span class="title">is_nearly_equal</span><span class="params">(<span class="type">float</span> a, <span class="type">float</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">float</span> diff = <span class="built_in">fabsf</span>(a - b);</span><br><span class="line">    a = <span class="built_in">fabsf</span>(a);</span><br><span class="line">    b = <span class="built_in">fabsf</span>(b);</span><br><span class="line">    <span class="type">float</span> max_val = (a &gt; b) ? a : b;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (max_val &lt; <span class="number">1e-6f</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">return</span> (diff / max_val) &lt; <span class="number">1e-4f</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">host_sgemm_t</span> *<span class="title">allocate_host</span><span class="params">(<span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">free_host</span><span class="params">(<span class="type">host_sgemm_t</span> *h_sgemm)</span></span>;</span><br><span class="line"><span class="function"><span class="type">device_sgemm_t</span> *<span class="title">allocate_device</span><span class="params">(<span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">free_device</span><span class="params">(<span class="type">device_sgemm_t</span> *d_sgemm_handler)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">memcpy_host_to_device</span><span class="params">(<span class="type">device_sgemm_t</span> *d_sgemm, <span class="type">host_sgemm_t</span> *h_sgemm)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">memcpy_device_to_host</span><span class="params">(<span class="type">host_sgemm_t</span> *h_sgemm, <span class="type">device_sgemm_t</span> *d_sgemm)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">init_matrix</span><span class="params">(<span class="type">host_sgemm_t</span> *h_sgemm)</span></span>;</span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">get_theoretical_result</span><span class="params">(<span class="type">int</span> i, <span class="type">int</span> j, <span class="type">int</span> K)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">verify_result</span><span class="params">(<span class="type">host_sgemm_t</span> *h_sgemm)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">launch_sgemm_kernel</span><span class="params">(<span class="type">const</span> <span class="type">device_sgemm_t</span> *d_sgemm)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">run</span><span class="params">(<span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这里，我们唯一需要注意的是，我们按照如下方式初始化我们的
<code>A</code> 和 <code>B</code>：</p>
<ul>
<li><span class="math inline">\(A[i][j] = i + j;\)</span></li>
<li><span class="math inline">\(B[i][j] = i - j;\)</span></li>
</ul>
<p>这是为了避免，编译器在编译的过程中对我们的数据逻辑进行优化，更好的模拟真正的线上数据。</p>
<h2 id="sgemm-的结果验证">SGEMM 的结果验证</h2>
<p>我们需要计算：<span class="math display">\[C[i][j] = \sum_{k=0}^{K-1}
(i + k)(k - j)\]</span></p>
<p>展开括号内的项：<span class="math display">\[(i + k)(k - j) = k^2 +
(i - j)k - ij\]</span></p>
<p>利用求和符号的线性性质，将其拆分为三个部分：<span
class="math display">\[C[i][j] = \sum_{k=0}^{K-1} k^2 + (i - j)
\sum_{k=0}^{K-1} k - \sum_{k=0}^{K-1} ij\]</span></p>
<p>根据标准的数列求和公式：</p>
<ol type="1">
<li>平方和：<span class="math inline">\(\sum_{k=0}^{K-1} k^2 =
\frac{(K-1)K(2K-1)}{6}\)</span></li>
<li>等差数列求和：<span class="math inline">\(\sum_{k=0}^{K-1} k =
\frac{(K-1)K}{2}\)</span></li>
<li>常数项求和：<span class="math inline">\(\sum_{k=0}^{K-1} ij = K
\cdot i \cdot j\)</span></li>
</ol>
<p>将上述公式代入并整理，得到 <span
class="math inline">\(C[i][j]\)</span> 的解析式：<span
class="math display">\[C[i][j] = \frac{K(K-1)(2K-1)}{6} + (i - j)
\frac{K(K-1)}{2} - Kij\]</span></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">float</span> <span class="title">get_theoretical_result</span><span class="params">(<span class="type">int</span> i, <span class="type">int</span> j, <span class="type">int</span> K)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">double</span> k_double = (<span class="type">double</span>)K;</span><br><span class="line">    <span class="type">double</span> term1 = (k_double * (k_double - <span class="number">1.0f</span>) * (<span class="number">2.0f</span> * k_double - <span class="number">1.0f</span>)) / <span class="number">6.0f</span>;</span><br><span class="line">    <span class="type">double</span> term2 = (<span class="type">double</span>)(i - j) * (k_double * (k_double - <span class="number">1.0f</span>)) / <span class="number">2.0f</span>;</span><br><span class="line">    <span class="type">double</span> term3 = k_double * i * j;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (<span class="type">float</span>)(term1 + term2 - term3);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="naive-sgemm">Naive SGEMM</h2>
<p>一个 <span class="math inline">\(M \times K\)</span> 的矩阵乘以一个
<span class="math inline">\(K \times N\)</span> 的矩阵会得到一个 <span
class="math inline">\(M \times N\)</span>
的矩阵，我们将最终的矩阵拆分为对应数量的 <span class="math inline">\(32
\times 8\)</span> 的 <code>block</code>，随后每个 block
负责计算最终的矩阵中的点：<span class="math inline">\((row,
col)\)</span>，我们当前的实现也存在相当多的优化点：</p>
<ul>
<li>对于结果矩阵的第 <span class="math inline">\(R\)</span> 行，它的元素
<code>[R[0], R[M])</code>，每一次计算都需要用到 <span
class="math inline">\(M \times K\)</span> 矩阵的第 <span
class="math inline">\(R\)</span>
行，我们目前的计算中，每次都是从显存中读取，我们是否可以通过合理的
<code>block</code> 安排，让计算第 <span class="math inline">\(R\)</span>
行的 <code>block</code> 绑定到同一个
<code>SM</code>。这样我们在计算的过程中，就可以将 <span
class="math inline">\(A[R]\)</span> 存放到 Shared Memory
中来提升性能。</li>
</ul>
<p>在 <span class="math inline">\(M \times N\)</span>
的计算任务中，假设我们使用大小为 <span class="math inline">\((32,
1)\)</span> 的线程块（Block），即每个 Block 负责结果矩阵 <span
class="math inline">\(C\)</span>
中的一行（或其一部分）。其访存行为可精确描述如下：</p>
<ol type="1">
<li>矩阵 A 的访问：广播（Broadcast）与访存冗余，对于 index_a = row * K +
k：</li>
</ol>
<ul>
<li>在一个 Block 内，所有线程的 row 相同，且在 k
循环的每一步中，所有线程请求的都是 同一个内存地址。</li>
<li>这虽然符合合并访问的广义定义（地址在同一个内存事务内），但实际上触发了硬件的
广播机制（Broadcast）。</li>
<li>尽管 IO 效率看似很高，但由于每个线程都各自发起了一次请求，导致 A
的同一个元素被重复读取了 32 次。带宽浪费源于极低的数据复用率。</li>
<li>应利用 Shared Memory 作为“一级缓存”，由 Block 内线程协作一次性加载 A
的片断，实现“一处加载，处处复用”。</li>
</ul>
<ol start="2" type="1">
<li>矩阵 B 的访问：完美的合并加载（Coalesced Load），对于 index_b = k *
N + col：</li>
</ol>
<ul>
<li>在 k 循环的每一步，k * N 是固定偏移，而 col 是随 threadIdx.x
连续递增的。</li>
<li>当 Warp 执行此指令时，32 个线程访问的是连续的 32 个 float 地址（128
字节）。这恰好对齐了显存的一个 L2 Cache Line，实现了合并访问（Memory
Coalescing）。</li>
<li>这是目前 Naive 版中 IO 效率最高的部分。</li>
</ul>
<ol start="3" type="1">
<li>矩阵 C 的写入：合并存储（Coalesced Store），对于 index_c = row * N +
col：</li>
</ol>
<ul>
<li>计算结束写回结果时，由于 row 固定而 col 连续，触发了合并存储。</li>
<li>写回操作只会占用极少的总线周期。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">naive_sgemm_kernel</span><span class="params">(<span class="type">const</span> <span class="type">device_sgemm_t</span> d_sgemm)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> row = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    <span class="type">int</span> col = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> value = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">if</span> (row &lt; d_sgemm.M &amp;&amp; col &lt; d_sgemm.N)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; d_sgemm.K; ++k)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> index_a = row * d_sgemm.K + k;</span><br><span class="line">            <span class="type">int</span> index_b = k * d_sgemm.N + col;</span><br><span class="line">            value += d_sgemm.A[index_a] * d_sgemm.B[index_b];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    d_sgemm.C[row * d_sgemm.N + col] = value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="shared-memory-sgemm">Shared Memory SGEMM</h2>
<blockquote>
<p>使用 <code>Shared Memory</code> 来优化 SGEMM 的思路，和使用
<code>block</code> 来优化计算的思路是一样的：</p>
<ul>
<li>在 <code>block</code> 中，我们将一个大的矩阵划分为一定数量的
<code>block</code>，每个 <code>block</code> 负责一部分数据的处理；</li>
<li>在 <code>Shared Memory</code> 中，我们将一个 <code>block</code>
中的矩阵划分为一定数量的更小的矩阵 -- 我们称之为 <code>Tile</code>。每个
<code>Tile</code>
可以缓存一部分的数据，然后随着程序的动态执行，我们可以动态的更新
<code>Tile</code> 中的数据。而在 <code>Tile</code>
中的数据满足需求之前，我们就可以直接从 <code>Tile</code>
中读取数据，从而避免高昂的显存访问开销。</li>
</ul>
<p>整体的思路就是：</p>
<ol type="1">
<li>优化前的逻辑是，直接使用一个 for 循环直接计算所有的点；</li>
<li>优化后我们将for循环拆分为 (K + TILE - 1) / TILE
个循环，而每个循环内我们存在一个子循环，计算 TILE 个数字，这是因为
<code>M * K</code> * <code>K * N</code> 的矩阵乘法需要K次计算；</li>
<li>我们的这里，需要做的就是，<strong>分析在子循环中需要用到 A 和 B
中的哪些数字，并在外层循环的最开始将这些数字全部加载到 Shared
Memory；</strong></li>
<li>在 <span class="math inline">\((M \times K) \times (K \times
N)\)</span> 的矩阵乘法中，我们这里复杂的地方在于，我们要将之前的
<code>for</code> 循环的思路转变 -- 现在不是 <code>for</code>
循环，而是一个 <code>TILE</code>
中包含哪些元素，我们便可以计算哪些元素。而这里我们需要：在计算过程中
<code>A</code> 和 <code>B</code> 所需要的元素是不一样的，我们需要将
<code>A</code> 和 <code>B</code> 所需的元素都缓存起来。</li>
<li>而此时 <code>A</code> 和 <code>B</code>
需要哪些元素呢？我们再思考一下 <span class="math inline">\(C(x,
y)\)</span> 的定义，它是 <span class="math inline">\(A的第x行 *
B的第y列\)</span> 的点积，而它的整个的流程就像是，在 A
中存在一个点从左往右移动，在 B
中存在一个点从上往下移动。而如果我们将这个点扩展到 TILE
也是一样的道理：<strong>在 A 中存在一个 TILE 从左往右移动，在 B
中存在一个 TILE 从上往下移动，那么当循环迭代完成时我们就算出来
<code>C</code> 中的这个 TILE 的结果。这里的每个点，就是我们的
<code>block</code> 中不同的线程，也就是说，我们的 <code>TILE</code>
必须和 <code>block</code> 的线程矩阵完全一致。</strong></li>
<li>整个 TILE 的引入，是典型的空间换时间的逻辑。</li>
</ol>
</blockquote>
<p>我们再来回顾一下我们的 <code>Native SGEMM</code> 中的索引
<code>index_a</code> 和 <code>index_b</code>，假设我们现在在计算的点是
<span class="math inline">\((x, y)\)</span>，而我们的 <code>block</code>
是一个 <span class="math inline">\(4 \times 4\)</span> 的矩阵；</p>
<p>当 <code>k == 0</code> 时，我们的索引如下：</p>
<pre><code class="highlight mermaid">block-beta

columns 6

block:rows:6
    columns 6
    space:8 c0[&quot;0&quot;] c1[&quot;1&quot;] c2[&quot;2&quot;] c3[&quot;3&quot;]
end
r0[&quot;0&quot;]
block:columns0:1
    columns 1
    index_a_0[&quot;index_a&quot;]
    index_b_0[&quot;index_b&quot;]
end
block:t00:1
    columns 1
    a_00(&quot;row * K&quot;)
    b_00(&quot;col&quot;)
end
block:t01:1
    columns 1
    a_01(&quot;row * K&quot;)
    b_01(&quot;col + 1&quot;)
end
block:t02:1
    columns 1
    a_02(&quot;row * K&quot;)
    b_02(&quot;col + 2&quot;)
end
block:t03:1
    columns 1
    a_03(&quot;row * K&quot;)
    b_03(&quot;col + 3&quot;)
end
r1[&quot;1&quot;]
block:columns1:1
    columns 1
    index_a_1[&quot;index_a&quot;]
    index_b_1[&quot;index_b&quot;]
end
block:t10:1
    columns 1
    a_10(&quot;(row + 1) * K&quot;)
    b_10(&quot;col&quot;)
end
block:t11:1
    columns 1
    a_11(&quot;(row + 1) * K&quot;)
    b_11(&quot;col + 1&quot;)
end
block:t12:1
    columns 1
    a_12(&quot;(row + 1) * K&quot;)
    b_12(&quot;col + 2&quot;)
end
block:t13:1
    columns 1
    a_13(&quot;(row + 1) * K&quot;)
    b_13(&quot;col + 2&quot;)
end
r2[&quot;2&quot;]
block:columns2:1
    columns 1
    index_a_2[&quot;index_a&quot;]
    index_b_2[&quot;index_b&quot;]
end
block:t20:1
    columns 1
    a_20(&quot;(row + 2) * K&quot;)
    b_20(&quot;col&quot;)
end
block:t21:1
    columns 1
    a_21(&quot;(row + 2) * K&quot;)
    b_21(&quot;col + 1&quot;)
end
block:t22:1
    columns 1
    a_22(&quot;(row + 2) * K&quot;)
    b_22(&quot;col + 2&quot;)
end
block:t23:1
    columns 1
    a_23(&quot;(row + 2) * K&quot;)
    b_23(&quot;col + 2&quot;)
end
r3[&quot;3&quot;]

block:columns3:1
    columns 1
    index_a_3[&quot;index_a&quot;]
    index_b_3[&quot;index_b&quot;]
end
block:t30:1
    columns 1
    a_30(&quot;(row + 3) * K&quot;)
    b_30(&quot;col&quot;)
end
block:t31:1
    columns 1
    a_31(&quot;(row + 3) * K&quot;)
    b_31(&quot;col + 1&quot;)
end
block:t32:1
    columns 1
    a_32(&quot;(row + 3) * K&quot;)
    b_32(&quot;col + 2&quot;)
end
block:t33:1
    columns 1
    a_33(&quot;(row + 3) * K&quot;)
    b_33(&quot;col + 3&quot;)
end

class c0,c1,c2,c3,r0,r1,r2,r3 purple
class index_a_0,index_a_1,index_a_2,index_a_3 green
class index_b_0,index_b_1,index_b_2,index_b_3 blue
class rows transparent
class t00,t01,t02,t03,t10,t11,t12,t13,t20,t21,t22,t23,t30,t31,t32,t33 animate
class a_00,a_01,a_02,a_03,a_10,a_11,a_12,a_13,a_20,a_21,a_22,a_23,a_30,a_31,a_32,a_33,b_00,b_01,b_02,b_03,b_10,b_11,b_12,b_13,b_20,b_21,b_22,b_23,b_30,b_31,b_32,b_33 gray

classDef transparent fill:none,stroke:none,color:inherit;
classDef content fill:#fff,stroke:#ccc;
classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef light_green fill:#e8f5e9,stroke:#695;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<p>当 <code>k == 1</code> 时，我们的索引如下：</p>
<pre><code class="highlight mermaid">block-beta

columns 6

block:rows:6
    columns 6
    space:8 c0[&quot;0&quot;] c1[&quot;1&quot;] c2[&quot;2&quot;] c3[&quot;3&quot;]
end
r0[&quot;0&quot;]
block:columns0:1
    columns 1
    index_a_0[&quot;index_a&quot;]
    index_b_0[&quot;index_b&quot;]
end
block:t00:1
    columns 1
    a_00(&quot;row * k + 1&quot;)
    b_00(&quot;N + col&quot;)
end
block:t01:1
    columns 1
    a_01(&quot;row * k + 1&quot;)
    b_01(&quot;N + col + 1&quot;)
end
block:t02:1
    columns 1
    a_02(&quot;row * k + 1&quot;)
    b_02(&quot;N + col + 2&quot;)
end
block:t03:1
    columns 1
    a_03(&quot;row * k + 1&quot;)
    b_03(&quot;N + col + 3&quot;)
end
r1[&quot;1&quot;]
block:columns1:1
    columns 1
    index_a_1[&quot;index_a&quot;]
    index_b_1[&quot;index_b&quot;]
end
block:t10:1
    columns 1
    a_10(&quot;(row + 1) * k + 1&quot;)
    b_10(&quot;N + col&quot;)
end
block:t11:1
    columns 1
    a_11(&quot;(row + 1) * k + 1&quot;)
    b_11(&quot;N + col + 1&quot;)
end
block:t12:1
    columns 1
    a_12(&quot;(row + 1) * k + 1&quot;)
    b_12(&quot;N + col + 2&quot;)
end
block:t13:1
    columns 1
    a_13(&quot;(row + 1) * k + 1&quot;)
    b_13(&quot;N + col + 2&quot;)
end
r2[&quot;2&quot;]
block:columns2:1
    columns 1
    index_a_2[&quot;index_a&quot;]
    index_b_2[&quot;index_b&quot;]
end
block:t20:1
    columns 1
    a_20(&quot;(row + 2) * k + 1&quot;)
    b_20(&quot;N + col&quot;)
end
block:t21:1
    columns 1
    a_21(&quot;(row + 2) * k + 1&quot;)
    b_21(&quot;N + col + 1&quot;)
end
block:t22:1
    columns 1
    a_22(&quot;(row + 2) * k + 1&quot;)
    b_22(&quot;N + col + 2&quot;)
end
block:t23:1
    columns 1
    a_23(&quot;(row + 2) * k + 1&quot;)
    b_23(&quot;N + col + 2&quot;)
end
r3[&quot;3&quot;]

block:columns3:1
    columns 1
    index_a_3[&quot;index_a&quot;]
    index_b_3[&quot;index_b&quot;]
end
block:t30:1
    columns 1
    a_30(&quot;(row + 3) * k + 1&quot;)
    b_30(&quot;N + col&quot;)
end
block:t31:1
    columns 1
    a_31(&quot;(row + 3) * k + 1&quot;)
    b_31(&quot;N + col + 1&quot;)
end
block:t32:1
    columns 1
    a_32(&quot;(row + 3) * k + 1&quot;)
    b_32(&quot;N + col + 2&quot;)
end
block:t33:1
    columns 1
    a_33(&quot;(row + 3) * k + 1&quot;)
    b_33(&quot;N + col + 3&quot;)
end

class c0,c1,c2,c3,r0,r1,r2,r3 purple
class index_a_0,index_a_1,index_a_2,index_a_3 green
class index_b_0,index_b_1,index_b_2,index_b_3 blue
class rows transparent
class t00,t01,t02,t03,t10,t11,t12,t13,t20,t21,t22,t23,t30,t31,t32,t33 animate
class a_00,a_01,a_02,a_03,a_10,a_11,a_12,a_13,a_20,a_21,a_22,a_23,a_30,a_31,a_32,a_33,b_00,b_01,b_02,b_03,b_10,b_11,b_12,b_13,b_20,b_21,b_22,b_23,b_30,b_31,b_32,b_33 gray

classDef transparent fill:none,stroke:none,color:inherit;
classDef content fill:#fff,stroke:#ccc;
classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef light_green fill:#e8f5e9,stroke:#695;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<p>如果我们将 <code>TILE</code>
的思想引入，那么这里的逻辑就可以划分为：每个 <code>TILE</code> 包含
<code>[4][4]</code>
的数据，那么我们在一次循环中，就可以一次读取显存，而执行 <code>32</code>
次计算。如果观察我们上面的两个图，那么我们就能得到几个结论：</p>
<h3 id="index_a">index_a</h3>
<p>对于 <code>index_a</code>，我们 <code>4</code>
次循环需要用到的值如下：</p>
<ul>
<li>这个值在同一行中完全一致，随着 <code>for</code> 循环递增；</li>
<li>在不同的列中，随着行的增加大范围跳转；</li>
</ul>
<p>所以，<code>index_a</code>
为了保证合并访问，必须如下图的形式组织：最终我们得到访问该
<code>TILE</code> 的公式：就是行偏移量+列偏移量。</p>
<p><span class="math display">\[GlobalIndex\_A = row \times K + (m
\times TILE\_WIDTH + tx)\]</span></p>
<pre><code class="highlight mermaid">block-beta

columns 5

space:1 c0[&quot;0&quot;] c1[&quot;1&quot;] c2[&quot;2&quot;] c3[&quot;3&quot;]
r0[&quot;0&quot;] a00(&quot;row * K&quot;) a01(&quot;row * K + 1&quot;) a02(&quot;row * K + 2&quot;) a03(&quot;row * K + 3&quot;)
r1[&quot;1&quot;] a10(&quot;(row + 1) * K&quot;) a11(&quot;(row + 1) * K + 1&quot;) a12(&quot;(row + 1) * K + 2&quot;) a13(&quot;(row + 1) * K + 3&quot;)
r2[&quot;2&quot;] a20(&quot;(row + 2) * K&quot;) a21(&quot;(row + 2) * K + 1&quot;) a22(&quot;(row + 2) * K + 2&quot;) a23(&quot;(row + 2) * K + 3&quot;)
r3[&quot;3&quot;] a30(&quot;(row + 3) * K&quot;) a31(&quot;(row + 3) * K + 1&quot;) a32(&quot;(row + 3) * K + 2&quot;) a33(&quot;(row + 3) * K + 3&quot;)

class c0,c1,c2,c3,r0,r1,r2,r3 purple

class a00,a01,a02,a03,a10,a11,a12,a13,a20,a21,a22,a23,a30,a31,a32,a33 gray

classDef transparent fill:none,stroke:none,color:inherit;
classDef content fill:#fff,stroke:#ccc;
classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef light_green fill:#e8f5e9,stroke:#695;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<h3 id="index_b">index_b</h3>
<p>对于 <code>index_b</code>，我们 <code>4</code>
次循环需要用到的值如下：</p>
<ul>
<li>这个值在同一列中一致，随着 <code>threadIdx.x</code> 递增；</li>
<li>在不同的线程中，随着线程的迭代而进行大范围跳转。</li>
</ul>
<p><code>index_b</code> 的公式如下：也是行偏移量+列偏移量</p>
<p><span class="math display">\[GlobalIndex\_B = (m \times TILE\_WIDTH +
ty) \times N + col\]</span></p>
<pre><code class="highlight mermaid">block-beta

columns 5

space:1 c0[&quot;0&quot;] c1[&quot;1&quot;] c2[&quot;2&quot;] c3[&quot;3&quot;]
r0[&quot;0&quot;] b00(&quot;col&quot;) b01(&quot;col + 1&quot;) b02(&quot;col + 2&quot;) b03(&quot;col + 3&quot;)
r1[&quot;1&quot;] b10(&quot;N + col&quot;) b11(&quot;N + col + 1&quot;) b12(&quot;N + col + 2&quot;) b13(&quot;N + col + 3&quot;)
r2[&quot;2&quot;] b20(&quot;2N + col&quot;) b21(&quot;2N + col + 1&quot;) b22(&quot;2N + col + 2&quot;) b23(&quot;2N + col + 3&quot;)
r3[&quot;3&quot;] b30(&quot;3N + col&quot;) b31(&quot;3N + col + 1&quot;) b32(&quot;3N + col + 2&quot;) b33(&quot;3N + col + 3&quot;)

class c0,c1,c2,c3,r0,r1,r2,r3 purple

class b00,b01,b02,b03,b10,b11,b12,b13,b20,b21,b22,b23,b30,b31,b32,b33 gray

classDef transparent fill:none,stroke:none,color:inherit;
classDef content fill:#fff,stroke:#ccc;
classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef light_green fill:#e8f5e9,stroke:#695;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<h2 id="第一次性能优化对比">第一次性能优化对比</h2>
<p>如果我们再使用 <code>nsys</code>
对两个程序进行性能对比，我们会发现现在已经看不出太大的差别了，因为他们的指标不够细化，只能在核函数的执行上看到一点差异，我们现在需要更细致的指标
-- <code>ncu</code>，我们通过一下两个命令输出我们的对比。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ncu ./build/naive_sgemm</span><br><span class="line">ncu ./build/shared_sgemm</span><br></pre></td></tr></table></figure>
<p>当我们执行完毕时，我们会发现：</p>
<ul>
<li>对于 <code>shared</code> 版本有一个 <code>warning</code>
级别的提示：This kernel's theoretical occupancy (66.7%) is limited by
the number of required registers. This kernel's theoretical occupancy
(66.7%) is limited by the required amount of shared memory. This
kernel's theoretical occupancy (66.7%) is limited by the number of warps
within each block. See the CUDA Best Practices Guide
(https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy)
for more details on optimizing occupancy.</li>
<li>而对于 <code>naive</code> 版本则有一个 <code>info</code>
级别的提示：This kernel's theoretical occupancy is not impacted by any
block limit.</li>
</ul>
<p>这表明，我们的 <code>shared</code>
版本，因为配置不合理导致硬件使用受限 --
<strong>然而，在这种情况下，我们的计算耗时和显存吞吐量仍然得到了显著的优化。</strong></p>
<h3 id="gpu运行效率对比">GPU运行效率对比</h3>
<table>
<colgroup>
<col style="width: 27%" />
<col style="width: 11%" />
<col style="width: 12%" />
<col style="width: 49%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>指标 (Metric)</strong></th>
<th><strong>Naive 版本</strong></th>
<th><strong>Shared 版本</strong></th>
<th><strong>性能解读</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Duration (耗时)</strong></td>
<td>556.38 ms</td>
<td><strong>411.00 ms</strong></td>
<td>纯计算耗时缩短了约 <strong>26%</strong>。</td>
</tr>
<tr class="even">
<td><strong>DRAM Throughput (显存压力)</strong></td>
<td><strong>93.53%</strong></td>
<td><strong>32.88%</strong></td>
<td><strong>可以看到，这是我们明显的优化，我们大部分时候都是从 Shared
Memory读取数据</strong></td>
</tr>
<tr class="odd">
<td><strong>Memory Throughput (总带宽)</strong></td>
<td>93.53%</td>
<td>76.45%</td>
<td>总数据压力下降，系统变得更“轻松”。</td>
</tr>
<tr class="even">
<td><strong>Compute Throughput (算力利用)</strong></td>
<td>81.66%</td>
<td>76.45%</td>
<td>虽然数值降了，但现在的 76% 是在做“有效功”，不再是空等数据。</td>
</tr>
<tr class="odd">
<td><strong>L2 Cache Throughput</strong></td>
<td>26.32%</td>
<td><strong>9.30%</strong></td>
<td>数据直接在 Shared Memory 解决了，连 L2 缓存都不怎么需要跑了。</td>
</tr>
</tbody>
</table>
<h3 id="任务调度对比">任务调度对比</h3>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 12%" />
<col style="width: 13%" />
<col style="width: 53%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>指标 (Metric)</strong></th>
<th><strong>Naive 版本</strong></th>
<th><strong>Shared 版本</strong></th>
<th><strong>性能解读</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Block Size</strong></td>
<td>256</td>
<td><strong>1024</strong></td>
<td>Shared 版用了更粗的粒度 (<span class="math inline">\(32 \times
32\)</span>)，这有助于复用 Shared Memory 数据。</td>
</tr>
<tr class="even">
<td><strong>Grid Size</strong></td>
<td>524,288</td>
<td>131,072</td>
<td>因为 Block 变大了，所以需要的 Block 数量减少了。</td>
</tr>
<tr class="odd">
<td><strong>Static Shared Memory</strong></td>
<td><strong>0 byte</strong></td>
<td><strong>8.19 Kbyte</strong></td>
<td>每个 <code>block</code> 使用了 32 * 32 * 4 * 2 的 SSM</td>
</tr>
<tr class="even">
<td>Registers Per Thread</td>
<td>40</td>
<td>37</td>
<td>每个线程用的寄存器数量差不多。</td>
</tr>
</tbody>
</table>
<h3 id="硬件占用率">硬件占用率</h3>
<table>
<colgroup>
<col style="width: 22%" />
<col style="width: 12%" />
<col style="width: 13%" />
<col style="width: 52%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>指标 (Metric)</strong></th>
<th><strong>Naive 版本</strong></th>
<th><strong>Shared 版本</strong></th>
<th><strong>性能解读</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Theoretical Occupancy</strong></td>
<td>100%</td>
<td><strong>66.67%</strong></td>
<td>理论占用率，我们的 <code>block</code> 使用了 32 * 32 = 1024
线程，而我的显卡 4060TI 最多可以容纳 1536
个线程，也就是说不论我们使用了多少 Shared Memory，这里最高已经不能超过
66.67% 了。</td>
</tr>
<tr class="even">
<td><strong>Achieved Occupancy</strong></td>
<td>99.93%</td>
<td><strong>66.65%</strong></td>
<td>实际占用率，通常来说，我们需要通过合理的调整我们的配置参数，让我们的理论占用率达到100%，否则会发生硬件占用受限。而实际占用率则是越接近理论占用率说明我们的硬件利用率越高。</td>
</tr>
<tr class="odd">
<td><strong>Block Limit Shared Mem</strong></td>
<td>16</td>
<td><strong>1</strong></td>
<td>每个 Block 占用的 Shared Memory 限制了同时存在的块数。</td>
</tr>
</tbody>
</table>
<h2 id="线程粗化2d-thread-tiling">线程粗化（2D Thread Tiling）</h2>
<p>在我们之前的实现逻辑中，我们总是使用一个线程计算一个点，而如果我们可以实现一个线程计算多个点的话：</p>
<ul>
<li>优点是：我们可以减少我们计算所需要的线程数量，也就是说，当矩阵的大小扩张时，我们可以通过一个线程处理更多的数据量来代替增加更多的线程；
<ul>
<li><strong>寄存器级的数据复用</strong>：执行一次寄存器的读取，就可以计算多个值，降低了对
SharedMemory 的访问频率；</li>
<li><strong>指令吞吐量（Instruction Throughput）</strong>： GPU
喜欢“流水线”。一个线程连续做 4 个乘加指令（FMA），比 4 个线程各做一个
FMA 更容易填满算术逻辑单元（ALU）的流水线，掩盖指令发射的延迟；</li>
<li><strong>减少 Global Memory 冗余</strong>： 虽然 Shared Memory
已经缓解了这个问题，但线程粗化进一步减少了 Block 的总量，从而减少了在
Block 边缘由于 Padding 或边界检查带来的开销。</li>
</ul></li>
<li>缺点是：我们每个线程需要使用更多的寄存器，而一个SM中的寄存器数量是有限的，这会降低我们的SM中能容纳的线程数量。这会使得我们面临<strong>Register
Spilling（寄存器溢出）</strong> 风险，以及降低我们的
<code>Occupation</code>。</li>
</ul>
<p>事实是，这种优化通常是在线程的总数量和单个线程处理的数据之间取得一个合适的平衡点：<strong>通过数据复用节省的时间，是否能超过因为线程数减少（掩盖延迟能力变弱）损失的时间？</strong>
通常情况下，稍微牺牲一点 Occupancy（比如降到
50%）来换取寄存器级的数据复用，性能反而会大幅提升。</p>
<p>下面给出了对于一个 <code>4 * 4</code>
的矩阵的优化前后对比：<strong>我们以每个线程多使用一定的寄存器作为代价，来降低了我们线程的数量。</strong></p>
<pre><code class="highlight mermaid">block-beta

columns 5

space:1 c0[&quot;0&quot;] c1[&quot;1&quot;] c2[&quot;2&quot;] c3[&quot;3&quot;]
r0[&quot;0&quot;]
block:s1:2
    0(&quot;0&quot;) 1(&quot;1&quot;)
end
block:s2:2
    2(&quot;2&quot;) 3(&quot;3&quot;)
end
r1[&quot;1&quot;]
block:s3:2
    4(&quot;4&quot;) 5(&quot;5&quot;)
end
block:s4:2
    6(&quot;6&quot;) 7(&quot;7&quot;)
end
r2[&quot;2&quot;]
block:s5:2
    8(&quot;8&quot;) 9(&quot;9&quot;)
end
block:s6:2
    a(&quot;a&quot;) b(&quot;b&quot;)
end
r3[&quot;3&quot;]
block:s7:2
    c(&quot;c&quot;) d(&quot;d&quot;)
end
block:s8:2
    e(&quot;e&quot;) f(&quot;f&quot;)
end

class c0,c1,c2,c3,r0,r1,r2,r3 purple

class 0,1,4,5 light_green
class 2,3,6,7 gray
class 8,9,c,d yellow
class a,b,e,f orange

class s1,s2,s3,s4,s5,s6,s7,s8 animate

%% 样式定义
classDef content fill:#fff,stroke:#ccc;
classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef light_green fill:#e8f5e9,stroke:#695;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<pre><code class="highlight mermaid">block-beta

columns 10

space:2 c0[&quot;0&quot;]:2 c1[&quot;1&quot;]:2 c2[&quot;2&quot;]:2 c3[&quot;3&quot;]:2
r0[&quot;0&quot;]:2
block:s1:8
    0(&quot;0&quot;)
end
r1[&quot;1&quot;]:2
block:s3:8
    1(&quot;1&quot;)
end
r2[&quot;2&quot;]:2
block:s5:8
    2(&quot;2&quot;)
end
r3[&quot;3&quot;]:2
block:s7:8
    3(&quot;3&quot;)
end

class c0,c1,c2,c3,r0,r1,r2,r3 purple

class 0 light_green
class 1 gray
class 2 yellow
class 3 orange

class s1,s2,s3,s4,s5,s6,s7,s8 animate

%% 样式定义
classDef content fill:#fff,stroke:#ccc;
classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef light_green fill:#e8f5e9,stroke:#695;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<figure>
<img src="\images\cuda\线程粗化.png" alt="线程粗化" />
<figcaption aria-hidden="true">线程粗化</figcaption>
</figure>
<blockquote>
<p>上面这个图引用自 <a
target="_blank" rel="noopener" href="https://cuda.keter.host/optimize_matmul/">CUDA Tutorial</a></p>
<p>上图中，A 和 B 都是 7x7
的矩阵。当每一个线程只计算一个结果的时候，我们需要从 A 中读取 7
个数据，从 B 中读取 7 个数据，从 C 中读取 1 个数据，然后写一次
C。这样的话，每个线程需要读取 15
个数据，写一次数据。如果我们每一个线程计算 4 个结果，那么我们需要从 A
中读取 14 个数据，从 B 中读取 14 个数据，从 C 中读取 4 个数据，然后写 4
次 C。这样的话，每个线程需要读取 32 个数据，写 4
次数据。计算每个线程的平方结果比计算结果的列更有效，因为这样我们可以共享更多的输入。</p>
</blockquote>
<h3 id="线程粗化的实现逻辑">线程粗化的实现逻辑</h3>
<p>首先我们看看几个重要的预定义参数：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> BM = <span class="number">128</span>;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> BN = <span class="number">128</span>;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> BK = <span class="number">8</span>;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> TM = <span class="number">8</span>;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> TN = <span class="number">8</span>;</span><br></pre></td></tr></table></figure>
<ol type="1">
<li>这里 TM 和 TN 表示线程负责 TM * TN 个结果点；</li>
<li>BM 是我们block的每次循环中，从矩阵 A 搬运到SSM的行数；</li>
<li>BN 是我们block的每次循环中，从矩阵 B 搬运到 SSM的列数；</li>
<li><strong>BK是对 BM 和 BN 的共同约束，因为矩阵乘法存在一个硬性要求，U
* V 要求 U 的列数等于 V 的行数，而 BK
就是这个共同约束，没有这个共同约束的话，U 和 V
就不能算是一个可以相乘的矩阵；</strong></li>
<li>按照我们之前的描述，A 从左向右移动，每个循环移动 BK 列，B
从上向下移动，每次移动 BK 行，当 A 和 B
移动到矩阵的结束为止，就意味着我们计算得到了 BM * BN 个点的数据。</li>
</ol>
<p>在有了这些信息之后，我们可以推论得到一下结果，在下面的代码中，总共有三个高亮部分：</p>
<ol type="1">
<li><strong>第二行</strong>：在一个 <span class="math inline">\((M
\times K) \times (K \times N)\)</span> 的矩阵乘法中，总共需要 <span
class="math inline">\(K\)</span>
次计算（一次先乘再加的运算算作一次运算），而我们的滑动窗口每次可以计算
<span class="math inline">\(BK\)</span> 次，向上取证即可；</li>
<li><strong>第六行</strong>：在进行线程粗化（2D Thread
Tiling）后，每个线程可以计算的值变多了，相当于我们的矩阵宽膨胀了
TM，而高度膨胀了 TN；那么我们所需的线程数也需要相应的减少；</li>
<li><strong>第九行和第十行：</strong>现在，所需的 <code>block</code>
数量由步长决定，我们可以这么去理解：
<ul>
<li>每个线程可以处理 <code>TM</code> 行；</li>
<li>每个 <code>block</code> 包含了 <code>BM / TM</code>
个处理行的线程；</li>
<li>那么每个 <code>block</code> 可以处理的行数是
<code>BM / TM * TM</code>，我们按照 <code>M</code> 和 <code>BM</code>
向上取整即可。</li>
</ul></li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 算完当前 block 负责的矩阵块的循环数量</span></span><br><span class="line marked"><span class="keyword">for</span> (<span class="type">int</span> m = <span class="number">0</span>; m &lt; (K + BK - <span class="number">1</span>) / BK; m++) &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">launch_sgemm_kernel</span><span class="params">(<span class="type">const</span> <span class="type">device_sgemm_t</span> *d_sgemm)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 每个线程算 8x8，为了算完 128x128 的 BMxBN，需要 16x16 的线程</span></span><br><span class="line marked">    <span class="function">dim3 <span class="title">blockDim</span><span class="params">(BN / TN, BM / TM)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Grid 必须以 BM 和 BN 为步长划分，而不是以线程数划分</span></span><br><span class="line marked">    <span class="function">dim3 <span class="title">gridDim</span><span class="params">((d_sgemm-&gt;N + BN - <span class="number">1</span>) / BN,</span></span></span><br><span class="line marked"><span class="params"><span class="function">                 (d_sgemm-&gt;M + BM - <span class="number">1</span>) / BM)</span></span>;</span><br><span class="line"></span><br><span class="line">    tiled_sgemm_kernel&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(d_sgemm-&gt;A, d_sgemm-&gt;B, d_sgemm-&gt;C, d_sgemm-&gt;M, d_sgemm-&gt;N, d_sgemm-&gt;K);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过这种分配模式，我们实现了：</p>
<ul>
<li><strong>1 Block</strong> <span
class="math inline">\(\rightarrow\)</span> 处理 <span
class="math inline">\(128 \times 128\)</span> 的结果块。</li>
<li><strong>1 Thread</strong> <span
class="math inline">\(\rightarrow\)</span> 处理 <span
class="math inline">\(8 \times 8\)</span>
的结果微块（Micro-tile）。</li>
<li><strong>1 Warp (32 threads)</strong> <span
class="math inline">\(\rightarrow\)</span> 处理 <span
class="math inline">\(32 \times 8 \times 8\)</span>
的数据流（取决于具体的线程排列）。</li>
</ul>
<h3 id="实现代码">实现代码</h3>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">tiled_sgemm_kernel</span><span class="params">(<span class="type">const</span> <span class="type">float</span> *A, <span class="type">const</span> <span class="type">float</span> *B, <span class="type">float</span> *C, <span class="type">int</span> M, <span class="type">int</span> N, <span class="type">int</span> K)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 声明共享内存：尺寸为 BMxBK 和 BKxBN</span></span><br><span class="line">    __shared__ <span class="type">float</span> sA[BM][BK];</span><br><span class="line">    __shared__ <span class="type">float</span> sB[BK][BN];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 寄存器：存储该线程负责的 TMxTN 个结果点</span></span><br><span class="line">    <span class="type">float</span> accum[TM][TN] = &#123;<span class="number">0.0f</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 线程在 Block 内的 ID</span></span><br><span class="line">    <span class="type">const</span> <span class="type">unsigned</span> <span class="type">int</span> ty = threadIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">unsigned</span> <span class="type">int</span> tx = threadIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 每次循环，我们的 sA 向右移动 BM，sB 向下移动 BN，每个线程计算自身对应的矩阵块并记录到 accum 中</span></span><br><span class="line">    <span class="comment">// 当循环结束时，我们应该计算得到了一个 TM * TN 的矩阵块，矩阵块中包含了 C 中的部分元素。</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> m = <span class="number">0</span>; m &lt; (K + BK - <span class="number">1</span>) / BK; m++) &#123;</span><br><span class="line">        <span class="comment">// 1. BM * BK 是行偏移</span></span><br><span class="line">        <span class="comment">// 2. blockDim.x * blockDim.y 是每个 block 中的线程数量，也就是 (BN / TN) * (BM / TM) 个线程</span></span><br><span class="line">        <span class="comment">//          也就是说每次循环搬运的数量是 (BM * BN) / (TM * TN)</span></span><br><span class="line">        <span class="comment">//          而我们总共需要搬运的数量是 BM * BK</span></span><br><span class="line">        <span class="comment">//          也就是说我们用 BM * BK / ((BM * BN) / (TM * TN)) 即可，只是需要进行一次向上取整。</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; (BM * BK + (blockDim.x * blockDim.y) - <span class="number">1</span>) / (blockDim.x * blockDim.y); i++) &#123;</span><br><span class="line">            <span class="comment">// 这里的逻辑比较容易让人误解，这个位置的逻辑不是要进行线程粗化的计算，</span></span><br><span class="line">            <span class="comment">// 而是需要将所有线程粗化后所需的数据全部加载到SSM，所以这里是对显存的合并访问：</span></span><br><span class="line">            <span class="comment">// 在一次循环中，我们在 x 轴上合并访问的元素为 (BN / TN) 个元素；</span></span><br><span class="line">            <span class="comment">// 而这样的行，我们总共有 (BM / TM) 行；</span></span><br><span class="line">            <span class="comment">// 这两个参数也是我们的 block 中的元素的矩阵的行和列。</span></span><br><span class="line">            <span class="type">const</span> <span class="type">unsigned</span> thread_id = ty * blockDim.x + tx;</span><br><span class="line">            <span class="comment">// 这个位置的逻辑非常的绕，简单来说就是，我们</span></span><br><span class="line">            <span class="comment">// 我们将矩阵从逻辑上看做一个连续的数组，尽管不同行的数组</span></span><br><span class="line">            <span class="comment">// 中会有很多不属于该block的元素。</span></span><br><span class="line">            <span class="comment">// 那么，我们要将这个连续的数组顺序的读取到 sA 中，但是问题在于，我们并不能和CPU编程一样：</span></span><br><span class="line">            <span class="comment">// for (int i = 0; i &lt; BM * BK; ++i) &#123;&#125;</span></span><br><span class="line">            <span class="comment">// 我们计算的 load_id 就是目标元素在这个数组中的索引，也就是 &#123;0, 1, 2, ... (BN / TN) * (BM / TM)&#125;</span></span><br><span class="line">            <span class="comment">// 这依赖于以下几个逻辑：</span></span><br><span class="line">            <span class="comment">// 1. 在 block 中，thread_id 总是顺序递增的；</span></span><br><span class="line">            <span class="comment">// 2. 每个 block 中的 thread 总数，总是等于 blockDim.x * blockDim.y</span></span><br><span class="line">            <span class="comment">// 所以，我们根据当前 block 中的 threadId 是一个集合：[0, blockDim.x * blockDim.y - 1)</span></span><br><span class="line">            <span class="comment">// 我们通过每次执行完之后加上 i * (blockDim.x * blockDim.y) 来实现行增加</span></span><br><span class="line">            <span class="comment">// 而我们可以通过这个行和列计算出来他们的全局行和全局列</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">// 1. i * (blockDim.x * blockDim.y) 的本质是增加一个行偏移</span></span><br><span class="line">            <span class="comment">// 2. 这里还有一点需要注意的是，即使我们线程的数量超过 block 中一行的数量也不会发生异常</span></span><br><span class="line">            <span class="comment">//      因为，我们这里通过 (load_id &lt; BM * BK) 确保不会数组越界</span></span><br><span class="line">            <span class="comment">//      同时，通过 / Bk 和 % BK 保证我们取到的行和列也是正确的</span></span><br><span class="line">            <span class="comment">//      只不过，这种错误的参数设置，会破坏我们的合并访问。</span></span><br><span class="line">            <span class="type">const</span> <span class="type">unsigned</span> load_id = thread_id + i * (blockDim.x * blockDim.y);</span><br><span class="line">            <span class="keyword">if</span> (load_id &lt; BM * BK) &#123;</span><br><span class="line">                <span class="type">const</span> <span class="type">unsigned</span> r = load_id / BK;</span><br><span class="line">                <span class="type">const</span> <span class="type">unsigned</span> c = load_id % BK;</span><br><span class="line">                <span class="type">const</span> <span class="type">unsigned</span> global_row = blockIdx.y * BM + r;</span><br><span class="line">                <span class="type">const</span> <span class="type">unsigned</span> global_col = m * BK + c;</span><br><span class="line">                <span class="keyword">if</span> (global_row &lt; M &amp;&amp; global_col &lt; K) &#123;</span><br><span class="line">                    sA[r][c] = A[global_row * K + global_col];</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    sA[r][c] = <span class="number">0.0f</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 这里和 sA 的逻辑完全一样</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; (BK * BN + (blockDim.x * blockDim.y) - <span class="number">1</span>) / (blockDim.x * blockDim.y); i++) &#123;</span><br><span class="line">            <span class="type">const</span> <span class="type">unsigned</span> thread_id = ty * blockDim.x + tx;</span><br><span class="line">            <span class="type">const</span> <span class="type">unsigned</span> load_id = thread_id + i * (blockDim.x * blockDim.y);</span><br><span class="line">            <span class="keyword">if</span> (load_id &lt; BK * BN) &#123;</span><br><span class="line">                <span class="type">const</span> <span class="type">unsigned</span> r = load_id / BN;</span><br><span class="line">                <span class="type">const</span> <span class="type">unsigned</span> c = load_id % BN;</span><br><span class="line">                <span class="type">const</span> <span class="type">unsigned</span> global_row = m * BK + r;</span><br><span class="line">                <span class="type">const</span> <span class="type">unsigned</span> global_col = blockIdx.x * BN + c;</span><br><span class="line">                sB[r][c] = (global_row &lt; K &amp;&amp; global_col &lt; N) ? B[global_row * N + global_col] : <span class="number">0.0f</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 我们前面的算法中，使用的是内积，内积的实现通常是</span></span><br><span class="line">        <span class="comment">// 取出 A 的第 M 行和 B 的第 N 列，计算他们的积。</span></span><br><span class="line">        <span class="comment">// 内积更直观，缺点是每次计算一个点，我们就需要读取一整行和一整列</span></span><br><span class="line">        <span class="comment">// 而这个位置，使用外积效率更高，因为我们外积读取一行和一列，便可以计算出一个矩阵</span></span><br><span class="line">        <span class="comment">// 也就是，对于 `M * K` 和 `K * N` 的矩阵，我们只需要在 K 次循环中</span></span><br><span class="line">        <span class="comment">// 进行 M 次访问（读取A的行）和 N 次访问（读取 B 的列），</span></span><br><span class="line">        <span class="comment">// 在计算完成之后 A 的这一行和 B 的这一列使命就完成了</span></span><br><span class="line">        <span class="comment">// 总共需要 BK * (TM + TN) 次 SSM 访问和 BK * TM * TN * 2 次寄存器访问</span></span><br><span class="line marked">        <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; BK; k++) &#123;</span><br><span class="line marked">            <span class="type">float</span> a_reg[TM];</span><br><span class="line marked">            <span class="type">float</span> b_reg[TN];</span><br><span class="line marked"></span><br><span class="line marked">            <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; TM; i++) &#123;</span><br><span class="line marked">                a_reg[i] = sA[ty * TM + i][k];</span><br><span class="line marked">            &#125;</span><br><span class="line marked">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; TN; j++) &#123;</span><br><span class="line marked">                b_reg[j] = sB[k][tx * TN + j];</span><br><span class="line marked">            &#125;</span><br><span class="line marked"></span><br><span class="line marked">            <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; TM; i++) &#123;</span><br><span class="line marked">                <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; TN; j++) &#123;</span><br><span class="line marked">                    accum[i][j] += a_reg[i] * b_reg[j];</span><br><span class="line marked">                &#125;</span><br><span class="line marked">            &#125;</span><br><span class="line marked">        &#125;</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 此时，我们已经将计算的结果存放到 accum 了</span></span><br><span class="line">    <span class="comment">// 注意，这里不论外层循环是TM还是TN，我们都是合并访问的</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; TM; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; TN; j++) &#123;</span><br><span class="line">            <span class="type">const</span> <span class="type">unsigned</span> global_row = blockIdx.y * BM + ty * TM + i;</span><br><span class="line">            <span class="type">const</span> <span class="type">unsigned</span> global_col = blockIdx.x * BN + tx * TN + j;</span><br><span class="line">            <span class="keyword">if</span> (global_row &lt; M &amp;&amp; global_col &lt; N) &#123;</span><br><span class="line">                C[global_row * N + global_col] = accum[i][j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">launch_sgemm_kernel</span><span class="params">(<span class="type">const</span> <span class="type">device_sgemm_t</span> *d_sgemm)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 每个线程算 8x8，为了算完 128x128 的 BMxBN，需要 16x16 的线程</span></span><br><span class="line">    <span class="function">dim3 <span class="title">blockDim</span><span class="params">(BN / TN, BM / TM)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Grid 必须以 BM 和 BN 为步长划分，而不是以线程数划分</span></span><br><span class="line">    <span class="function">dim3 <span class="title">gridDim</span><span class="params">((d_sgemm-&gt;N + BN - <span class="number">1</span>) / BN,</span></span></span><br><span class="line"><span class="params"><span class="function">                 (d_sgemm-&gt;M + BM - <span class="number">1</span>) / BM)</span></span>;</span><br><span class="line"></span><br><span class="line">    tiled_sgemm_kernel&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(d_sgemm-&gt;A, d_sgemm-&gt;B, d_sgemm-&gt;C, d_sgemm-&gt;M, d_sgemm-&gt;N, d_sgemm-&gt;K);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="第二次性能优化对比">第二次性能优化对比</h2>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ncu ./build/shared_sgemm_thread</span><br></pre></td></tr></table></figure>
<h3 id="性能分析-1">性能分析</h3>
<p>当我们执行完毕时，我们会发现有如下警告信息：</p>
<ol type="1">
<li>This kernel grid is too small to fill the available resources on
this device, resulting in only 0.9 full waves across all SMs. Look at
Launch Statistics for more details.</li>
<li>If you execute __syncthreads() to synchronize the threads of a
block, it is recommended to have more than the achieved 1 blocks per
multiprocessor. This way, blocks that aren't waiting for __syncthreads()
can keep the hardware busy.</li>
<li>This kernel's theoretical occupancy (33.3%) is limited by the number
of required registers. See the CUDA Best Practices Guide
(https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy)
for more details on optimizing occupancy.</li>
</ol>
<p>我们逐个解析这些逻辑：</p>
<h4 id="sm使用率">SM使用率</h4>
<blockquote>
<p>This kernel grid is too small to fill the available resources on this
device, resulting in only 0.9 full waves across all SMs. Look at Launch
Statistics for more details.</p>
</blockquote>
<p>在 CUDA 的性能分析中，<strong>Full
Wave</strong>是一个衡量<strong>并行度是否填满了硬件</strong>的核心指标。显卡（GPU）由许多个
<strong>SM</strong>（流式多处理器）组成。一个 SM 在同一时间能处理的
<strong>Thread Block</strong>
数量是有限的（受限于寄存器、共享内存等）。</p>
<ul>
<li><strong>1 个 Wave</strong>：指的是 GPU 的所有 SM 恰好都塞满了
Block，正在同步运行的状态。</li>
<li><strong>Waves 的计算方式</strong>：<span class="math inline">\(Waves
= \frac{\text{Total Blocks in Grid}}{\text{Max Active Blocks per SM}
\times \text{Total SMs on GPU}}\)</span></li>
</ul>
<p>而我们这里的 <strong>0.9 full waves</strong>
表示，我们的任务连让整个GPU满载都达不到，此外，并不是 full waves
大于一就表示系统配置正常：</p>
<ul>
<li>1.0 waves 表示所有的SMs都在工作；</li>
<li>0.9 waves 表示只有 90% 的 SMs 在工作，剩下的 10% SMs 在空闲中；</li>
<li>1.1 waves 则是更差的状态，在完成部分工作后，很有可能进入到只有 10%
的 SMs 在工作的状态。</li>
</ul>
<p>所以，我们合理的配置是，waves 是 SMs 的整数倍。</p>
<h4 id="syncthreads">__syncthreads</h4>
<blockquote>
<p>If you execute __syncthreads() to synchronize the threads of a block,
it is recommended to have more than the achieved 1 blocks per
multiprocessor. This way, blocks that aren't waiting for __syncthreads()
can keep the hardware busy.</p>
</blockquote>
<p><strong>我们目前的系统，只有 0.9 的 full waves，也就是说，我们的每个
SM 最多会执行一个 block。</strong>这意味着当我们在 <code>block</code>
中执行 <code>__syncthreads</code> 时，我们的整个 SM
会直接停止工作，最好的状态是，当有 block 执行 <code>syncthreads</code>
时，有其他的 block 可以被执行；</p>
<h4 id="theoretical-occupancy">theoretical occupancy</h4>
<blockquote>
<p>This kernel's theoretical occupancy (33.3%) is limited by the number
of required registers. See the CUDA Best Practices Guide
(https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy)
for more details on optimizing occupancy.</p>
</blockquote>
<p>我们的GPU上，每个SM的寄存器数量是有限的，而在我们的代码中：</p>
<ul>
<li><code>float res[TM][TN];</code> 使用了 64 个寄存器</li>
<li><code>float a_reg[TM];</code> 使用了 8 个寄存器；</li>
<li><code>float b_reg[TN];</code> 使用了 8 个寄存器</li>
</ul>
<p>除此之外，其他的临时变量也会使用寄存器，这使得我们的的 SM 能容纳的
<code>block</code>
数量减少。需要注意的是，这里是理论占用率，因为由于我们的 full waves 只有
0.9，我们本身就会存在严重的低Occupation异常。</p>
<h3 id="优化思路-1">优化思路</h3>
<p>我们的目标是，提升 full waves，这样可以同时解决两个问题：</p>
<ol type="1">
<li>在 <code>SM使用率</code>
中避免SMs限制，问题在于，我们很难配置一个参数，在不影响其他指标（例如尽可能的利用
Memory Burst）
的情况下，正好为SMs的整数倍，我们只能尽量去接近这个目标；</li>
<li><code>__syncthreads</code> 如果每个 SM 中有多个 blocks，那么在 block
休眠时，SM 不会进入限制状态；</li>
</ol>
<p>降低寄存器的使用数量，这可以提高我们的 occupancy。</p>
<p>为了达到这些目的，我们可以：</p>
<ol type="1">
<li>减小 <span class="math inline">\(BM\)</span> 和 <span
class="math inline">\(BN\)</span> 提升 Full Waves
<ul>
<li><strong>硬件对齐目标</strong>：在理想情况下，你希望 <span
class="math inline">\(Total Blocks \div Total SMs\)</span>
的结果不仅是一个大于 1 的数，而且最好是<strong>整数</strong>（例如 2.0,
3.0）。</li>
<li><strong>潜在风险（Memory Coalescing）</strong>：当我们减小 <span
class="math inline">\(BN\)</span> 时（比如从 128 减到 64 或
32），需要注意 <span class="math inline">\(B\)</span>
矩阵加载时的<strong>合并访问</strong>。
<ul>
<li>在加载 <span class="math inline">\(sB\)</span> 时，如果 <span
class="math inline">\(BN\)</span>
太小，可能导致一排线程加载的内存地址跨度不够“整齐”，无法充分利用显存的单次
Burst（通常是 128 字节）。</li>
<li><strong>建议</strong>：即便减小 <span
class="math inline">\(BN\)</span>，也尽量保持其为 32 或 64
的倍数，以维持良好的访存对齐。</li>
</ul></li>
</ul></li>
<li>降低 <span class="math inline">\(TM, TN\)</span> 以平衡 Occupancy
<ul>
<li><strong>Occupancy 提升的代价</strong>： 减少 <span
class="math inline">\(TM, TN\)</span>（如从 <span
class="math inline">\(8 \times 8\)</span> 降到 <span
class="math inline">\(4 \times 4\)</span>）确实能显著释放寄存器，让
<span class="math inline">\(33.3\%\)</span> 的 Occupancy 翻倍。</li>
<li><strong>计算强度的下降</strong>：我们现在的代码核心优势在于<strong>外积（Outer
Product）</strong>带来的寄存器复用。
<ul>
<li>在 <span class="math inline">\(8 \times 8\)</span> 模式下，读取 1 个
<span class="math inline">\(a\_reg\)</span> 和 1 个 <span
class="math inline">\(b\_reg\)</span> 可以支撑 1 次乘法，而这个 <span
class="math inline">\(a\_reg\)</span> 在内层循环被复用了 8 次。</li>
<li>如果降到 <span class="math inline">\(4 \times
4\)</span>，复用率直接减半。这意味着你需要花费更多的指令去从 Shared
Memory 读取数据。</li>
<li><strong>平衡点</strong>：通常在矩阵乘法中，Occupancy 达到
<strong>50% - 60%</strong> 左右就足够隐藏大部分延迟了。盲目追求 100%
Occupancy 而过度牺牲 <span class="math inline">\(TM/TN\)</span>
往往会导致单线程执行效率大幅下滑，总时间反而变长。</li>
</ul></li>
</ul></li>
</ol>
<h3 id="指标对比">指标对比</h3>
<p>可以看到，我们在优化配置后，虽然 occupancy
指标显著提高，但是处理时间并没有显著的提高。这是因为，为了使得 SMs
中能执行更多的
block，我们通过降低内部寄存器的使用，这意味着：<strong>虽然在等待时会有其他的
block
被调度，但是因为内部寄存器缓存数量的减少导致我们需要更多的访问显存，这也是为什么
DRAM Throughput 和 L2 Cache Throughput
都显著提高的原因。</strong>所以说，性能的优化并不能单纯的关注某些指标，而需要关注整体的指标做权衡。</p>
<h4 id="gpu运行效率对比-1">GPU运行效率对比</h4>
<table>
<colgroup>
<col style="width: 27%" />
<col style="width: 20%" />
<col style="width: 22%" />
<col style="width: 14%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>指标 (Metric)</strong></th>
<th><strong>Naive 版本</strong></th>
<th><strong>Shared 版本</strong></th>
<th>未优化配置</th>
<th>优化配置后</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Duration</td>
<td>556.38 ms</td>
<td><strong>411.00 ms</strong></td>
<td>130ms</td>
<td>122ms</td>
</tr>
<tr class="even">
<td>DRAM Throughput</td>
<td><strong>93.53%</strong></td>
<td><strong>32.88%</strong></td>
<td>26.65%</td>
<td>53.97%</td>
</tr>
<tr class="odd">
<td>Memory Throughput</td>
<td>93.53%</td>
<td>76.45%</td>
<td>50.12%</td>
<td>53.97%</td>
</tr>
<tr class="even">
<td>Compute Throughput</td>
<td>81.66%</td>
<td>76.45%</td>
<td>36.75%</td>
<td>51.61%</td>
</tr>
<tr class="odd">
<td>L2 Cache Throughput</td>
<td>26.32%</td>
<td><strong>9.30%</strong></td>
<td>9.46%</td>
<td>14.73%</td>
</tr>
</tbody>
</table>
<h4 id="任务调度对比-1">任务调度对比</h4>
<table>
<colgroup>
<col style="width: 32%" />
<col style="width: 19%" />
<col style="width: 20%" />
<col style="width: 13%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>指标 (Metric)</strong></th>
<th><strong>Naive 版本</strong></th>
<th><strong>Shared 版本</strong></th>
<th>未优化配置</th>
<th>优化配置后</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Block Size</strong></td>
<td>256</td>
<td><strong>1024</strong></td>
<td>256</td>
<td>256</td>
</tr>
<tr class="even">
<td><strong>Grid Size</strong></td>
<td>524,288</td>
<td>131,072</td>
<td>64</td>
<td>32768</td>
</tr>
<tr class="odd">
<td><strong>Static Shared Memory</strong></td>
<td><strong>0 byte</strong></td>
<td><strong>8.19 Kbyte</strong></td>
<td>8.19 Kbyte</td>
<td>4.10 Kbyte</td>
</tr>
<tr class="even">
<td>Registers Per Thread</td>
<td>40</td>
<td>37</td>
<td>94</td>
<td>56</td>
</tr>
</tbody>
</table>
<h4 id="硬件占用率-1">硬件占用率</h4>
<table>
<colgroup>
<col style="width: 34%" />
<col style="width: 18%" />
<col style="width: 20%" />
<col style="width: 13%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>指标 (Metric)</strong></th>
<th><strong>Naive 版本</strong></th>
<th><strong>Shared 版本</strong></th>
<th>未优化版本</th>
<th>优化后配置</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Theoretical Occupancy</strong></td>
<td>100%</td>
<td><strong>66.67%</strong></td>
<td>33.33</td>
<td>66.67</td>
</tr>
<tr class="even">
<td><strong>Achieved Occupancy</strong></td>
<td>99.93%</td>
<td><strong>66.65%</strong></td>
<td>31.22</td>
<td>66.67</td>
</tr>
<tr class="odd">
<td><strong>Block Limit Shared Mem</strong></td>
<td>16</td>
<td><strong>1</strong></td>
<td>7</td>
<td>12</td>
</tr>
</tbody>
</table>
<h1 id="qa">QA</h1>
<h2 id="术语">术语</h2>
<table>
<colgroup>
<col style="width: 3%" />
<col style="width: 39%" />
<col style="width: 57%" />
</colgroup>
<thead>
<tr class="header">
<th>缩写</th>
<th>全称</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td>Graphics Card</td>
<td>- 显卡是计算机的并行计算子系统。它的核心任务是将 CPU
发出的指令和数据，转换成大规模并行的计算任务并执行。在 AI
领域，它是承载深度学习模型训练与推理的<strong>物理平台</strong>。<br/>-
最早显卡是用于图形计算，但是随着AI的发展，对并行计算能力要求越来越高，慢慢的开始逐渐从一个游戏玩家青睐的工具转变为AI训练的基础算力资源。<br/>-
显卡通过 PCIe 与计算机的其他组件，如CPU，内存进行数据传输。</td>
</tr>
<tr class="even">
<td>GPU</td>
<td>Graphics Processing Unit</td>
<td>显卡的“大脑”，负责执行实际的数学运算</td>
</tr>
<tr class="odd">
<td>VRAM</td>
<td>Video RAM</td>
<td>-
显卡的“仓库”，高速存储当前计算所需的所有数据。这个对于整个显卡来说是共享的。<br/>-
存放 <code>cudaMalloc</code> 申请的空间。数据必须先从系统内存（RAM）通过
PCIe 搬运到 VRAM，GPU 才能对其进行计算。</td>
</tr>
<tr class="even">
<td>PCB</td>
<td>Printed Circuit Board</td>
<td>显卡的“骨架”，提供电气连接。</td>
</tr>
<tr class="odd">
<td>PCIe</td>
<td>peripheral component interconnect express</td>
<td>显卡的“大门”，负责与主机（CPU/内存）进行数据交换。</td>
</tr>
<tr class="even">
<td>VRM</td>
<td>Voltage Regulator Module</td>
<td>显卡的“心脏”，将电源电量转化为芯片所需的精密电流。</td>
</tr>
<tr class="odd">
<td>GPC</td>
<td>Graphics Processing Cluster</td>
<td>GPU 内部最大的物理分区。每个 GPC
就像一个独立的“自治区”，拥有完整的计算和光栅化资源。</td>
</tr>
<tr class="even">
<td>TPC</td>
<td>Texture Processing Cluster</td>
<td>GPC 内部的进一步划分，主要负责管理和分发计算指令。</td>
</tr>
<tr class="odd">
<td>SM</td>
<td>Streaming Multiprocessor</td>
<td><strong>GPU 最关键的计算单元</strong>。所有的 CUDA Block
最终都会被分配到某个 SM 上运行。</td>
</tr>
<tr class="even">
<td></td>
<td>CUDA Core(ALU)</td>
<td>负责执行最基础的浮点数加减乘除（FP32/INT32）。</td>
</tr>
<tr class="odd">
<td></td>
<td>Tensor Cores</td>
<td>专门为深度学习设计的硬件加速器，处理 <span class="math inline">\(4
\times 4\)</span> 矩阵乘法。</td>
</tr>
<tr class="even">
<td></td>
<td>Register File</td>
<td>SM 内部速度最快但空间最小的存储，分配给每个线程。</td>
</tr>
<tr class="odd">
<td></td>
<td>Shared Memory</td>
<td>允许同一个 Block 内的线程互相通信。</td>
</tr>
<tr class="even">
<td></td>
<td>Warp Scheduler</td>
<td>负责每 32 个线程（一个 Warp）一组，指派到计算单元上运行。</td>
</tr>
</tbody>
</table>
<h2 id="ncu的指标">ncu的指标</h2>
<h3 id="gpu-speed-of-light-sol-throughput">GPU Speed Of Light (SOL)
Throughput</h3>
<p>这是“性能概览”表，反映了 GPU 硬件资源的利用极限。</p>
<table>
<colgroup>
<col style="width: 28%" />
<col style="width: 8%" />
<col style="width: 63%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>指标名称 (Metric Name)</strong></th>
<th><strong>单位</strong></th>
<th><strong>意义说明</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Memory Throughput</strong></td>
<td>%</td>
<td><strong>总存储吞吐率</strong>。表示显卡访存带宽的使用比例。超过 80%
即意味着访存极其繁忙。</td>
</tr>
<tr class="even">
<td><strong>DRAM Throughput</strong></td>
<td>%</td>
<td><strong>显存（大仓库）吞吐率</strong>。</td>
</tr>
<tr class="odd">
<td><strong>Compute (SM) Throughput</strong></td>
<td>%</td>
<td><strong>计算核心利用率</strong>。</td>
</tr>
<tr class="even">
<td><strong>L1/TEX Cache Throughput</strong></td>
<td>%</td>
<td>L1 缓存/纹理缓存吞吐率。反映了离核心最近的缓存层级的活跃度。</td>
</tr>
<tr class="odd">
<td><strong>L2 Cache Throughput</strong></td>
<td>%</td>
<td>L2 缓存吞吐率。数值较低说明数据没怎么在二级缓存命中，全都跑去 DRAM
搬了。</td>
</tr>
<tr class="even">
<td><strong>Duration</strong></td>
<td>ms</td>
<td><strong>总耗时</strong>。Kernel 执行完花掉的物理时间。</td>
</tr>
<tr class="odd">
<td><strong>Elapsed Cycles</strong></td>
<td>cycle</td>
<td>整个过程消耗的 GPU 时钟周期总数。</td>
</tr>
</tbody>
</table>
<h3 id="launch-statistics">Launch Statistics</h3>
<p>启动配置表</p>
<table>
<colgroup>
<col style="width: 27%" />
<col style="width: 8%" />
<col style="width: 63%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>指标名称 (Metric Name)</strong></th>
<th><strong>单位</strong></th>
<th><strong>意义说明</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Grid Size</strong></td>
<td>-</td>
<td><strong>网格规模</strong>。</td>
</tr>
<tr class="even">
<td><strong>Block Size</strong></td>
<td>-</td>
<td><strong>块规模</strong>。</td>
</tr>
<tr class="odd">
<td><strong>Threads</strong></td>
<td>thread</td>
<td><strong>总线程数</strong>。</td>
</tr>
<tr class="even">
<td>Registers Per Thread</td>
<td>reg</td>
<td><strong>每个线程消耗的寄存器</strong>。</td>
</tr>
<tr class="odd">
<td>Static Shared Memory</td>
<td>byte</td>
<td><strong>静态共享内存</strong>。</td>
</tr>
<tr class="even">
<td>Waves Per SM</td>
<td>-</td>
<td>每个计算单元（SM）上排队轮转的“波次”数，数值越高说明任务越重。</td>
</tr>
</tbody>
</table>
<h3 id="occupation">Occupation</h3>
<p>这是“占用率”表，反映了 GPU 核心的工位利用率。</p>
<table>
<colgroup>
<col style="width: 37%" />
<col style="width: 9%" />
<col style="width: 53%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>指标名称 (Metric Name)</strong></th>
<th><strong>单位</strong></th>
<th><strong>意义说明</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Theoretical Occupancy</strong></td>
<td>%</td>
<td><strong>理论占用率</strong></td>
</tr>
<tr class="even">
<td><strong>Achieved Occupancy</strong></td>
<td>%</td>
<td><strong>实际占用率</strong>。</td>
</tr>
<tr class="odd">
<td>Theoretical Active Warps per SM</td>
<td>warp</td>
<td>每个 SM 理论上能同时跑多少个线程束（Warp）。</td>
</tr>
<tr class="even">
<td>Achieved Active Warps Per SM</td>
<td>warp</td>
<td>每个 SM 实际上跑的实际 warp。</td>
</tr>
<tr class="odd">
<td>Block Limit SM</td>
<td>block</td>
<td>每个 SM 最多能放的 Block。</td>
</tr>
<tr class="even">
<td>Block Limit Shared Mem</td>
<td>block</td>
<td>共享内存对 Block 数量的限制。</td>
</tr>
</tbody>
</table>
<h2 id="warp">warp</h2>
<h3 id="什么是-warp">什么是 Warp</h3>
<p>在硬件层面，GPU
并不是一个线程一个线程地去执行代码的。为了管理成千上万个线程，它规定：<strong>每
32 个线程组成一个小组，这个小组被称为一个 Warp。</strong></p>
<ul>
<li><strong>同步执行</strong>：在任何一个时刻，这 32
个线程都在执行<strong>同一行指令</strong>，只是在处理<strong>不同的数据</strong>（这就是
SIMT：单指令多线程）</li>
<li><strong>最小调度单位</strong>：GPU
的硬件调度器（SM）每次分发任务，最少也是分发一整个 Warp 的指令。</li>
</ul>
<p>这里还是需要注意：<strong>GPU的并行和单核CPU的并行是完全不一样的，单核CPU是通过上下切换实现的伪并行，而GPU的并行是物理意义上的并行（每个线程都拥有自己的寄存器）。</strong></p>
<h3 id="为什么warp是32个线程">为什么warp是32个线程</h3>
<blockquote>
<p><strong>32
的意义在于：它是高性能吞吐与复杂逻辑控制之间的“最大公约数”。</strong></p>
</blockquote>
<p><strong>首先我们需要知道的是：不是所有的warp都是32个线程一组</strong>，虽然
NVIDIA 定义了 "Warp" 这个概念并固定为 32，但其他架构有不同的选择：</p>
<ul>
<li><strong>NVIDIA (Warp)</strong>：从 2006 年至今，所有架构始终坚持
<strong>32</strong>。</li>
<li><strong>AMD (Wavefront)</strong>：在较老的 GCN 架构中是
<strong>64</strong>；在最新的 RDNA 架构（如 RX 6000/7000
系列）中，为了提高灵活性，可以配置为 <strong>32</strong>。</li>
<li><strong>Intel (SIMD Lane)</strong>：Intel 的 GPU（如 Xe
架构）通常使用更小的宽度，如 <strong>8、16 或 32</strong>。</li>
</ul>
<p>而通常来说，由于NVIDIA目前在GPU的统治地位，我们通常可以认为所有的warp都是32个线程一组。而
NVIDIA 选择32个线程作为一个warp，本质上是在
<strong>“硬件成本（效率）”</strong> 与
<strong>“编程灵活性（易用性）”</strong> 之间做权衡，主要的意图包括：</p>
<ul>
<li><strong>掩盖访存延迟（Latency
Hiding）</strong>：这是最核心的理由，因为GPU的计算速度远超内存访问速度：
<ul>
<li>当一个 Warp (32人) 因为等待数据而停下来时，SM
硬件调度器可以瞬间切换到 <strong>另一个 Warp</strong> 去执行。</li>
<li>而32个线程作为一组，它大到足以通过“批处理”掩盖大部分硬件开销，又小到让
SM 内部的寄存器文件（Register
File）不至于因为要存储太多线程的状态而变得太大太慢。</li>
</ul></li>
<li><strong>减少控制单元的面积</strong>：GPU
的核心（SM）非常拥挤。如果每个线程都有自己的“指令获取器”和“程序计数器（PC）”，那芯片的大部分面积都会被控制电路占满，没地方放计算单元（ALU）了。使用warp意味着硬件开销减少了
32 倍。这让 GPU 能把省下的空间塞进更多的计算单元，实现恐怖的算力。</li>
<li><strong>适配现代内存合并访问</strong>：现代显存（DDR6/HBM）的数据总线宽度通常是
<strong>256 bit（即 32 字节）</strong>，而 <code>4</code>
个内存突发传输（<a href="#gpu的数据总线">Memory Burst</a>）总共是
<code>128</code>字节，符合 DRAM 芯片的最佳吞吐性能。如果选 4
个线程，带宽利用率太低；如果选 512
个线程，管理开销和冲突又太严重。（这里需要注意的是，我们读取的数据 128
bytes，其实是超过数据总线带宽的，我们可以参考 <a
href="#gpu的数据总线">GPU的数据总线</a> 这一小节来查看原因。）</li>
</ul>
<h3 id="warp和block">warp和block</h3>
<p>不论我们<code>dim3 block</code>
是几维的，在硬件眼里，它们都会被拉平成一条直线，然后按 <strong>32
个一排</strong> 砍断：拉平的顺序是：<strong>X 轴最快（内层），Y
轴次之（中层），Z 轴最慢（外层）。</strong></p>
<p>我们可以用一个公式来定位任何一个线程在物理队列中的位置（TID）：</p>
<p><span class="math display">\[TID = threadIdx.z \times (blockDim.x
\times blockDim.y) + threadIdx.y \times blockDim.x +
threadIdx.x\]</span></p>
<p>假设我们声明了 <span class="math inline">\(block(16, 16,
16)\)</span>，那么我们总共会生成 <span class="math inline">\(16 * 16 *
16 / 32 = 128\)</span> 个warp，而我们 warp 填满这些线程的逻辑是：</p>
<ol type="1">
<li>把 <code>block(16, 16, 16)</code> 拉平为一个
<code>{(0, 0, 0), (0, 0, 1), ..., (0, 0, 15), (0, 1, 0), ..., (0, 15, 15), (1, 0, 0), ...}</code>
排列的 thread 数组；</li>
<li>在数组中，每次选择相邻的 32 个线程作为一个 warp；</li>
</ol>
<p>也就是说，我们的128个warp应该是如下的顺序：</p>
<ol type="1">
<li><code>(0, 0, 0) ~ (0, 0, 15)</code> &amp;&amp;
<code>(0, 1, 0) ~ (0, 1, 15)</code></li>
<li><code>(0, 2, 0) ~ (0, 2, 15)</code> &amp;&amp;
<code>(0, 3, 0) ~ (0, 3, 15)</code></li>
<li><code>...</code></li>
<li><code>(1, 0, 0) ~ (1, 0, 15)</code> &amp;&amp;
<code>(1, 1, 0) ~ (1, 1, 15)</code></li>
<li><code>...</code></li>
<li><code>(15, 14, 0) ~ (15, 14, 15)</code> &amp;&amp;
<code>(15, 15, 0) ~ (15, 15, 15)</code></li>
</ol>
<h3 id="warp和合并访问">warp和合并访问</h3>
<p>那现在核心的问题来了，当我们使用warp来调度三维矩阵时，我们是否实现了我们的合并访问逻辑？我们使用下面的例子来逐一分析：</p>
<ul>
<li><code>(16, 16, 16)</code> 我们的每个 warp 包含的线程应该是
<code>(z, y, 0) ~ (z, y, 15)</code> 和
<code>(z, y + 1, 0) ~ (z, y + 1， 15)</code>，而这两个部分大概率是位于不连续的内存，而这两个部分的内部
<code>15</code>
个元素，又位于一个连续的内存区域。也就是说，part内是合并访问的，两个part之间不是合并访问的。这里就陷入了我们的一个内存访问陷阱，我们假设
<code>(0, 0, 0)</code> 元素是按照 128 bytes 对齐的：
<ul>
<li>当我们访问 <code>(0,0,0) ~ (0,0,15)</code> 的数据时，从内存读取
<code>(0,0,0) ~ (0,0,31)</code> 数据，但是其中的 64bytes 被浪费了；</li>
<li>当我们访问 <code>(0,1,0) ~ (0,1,15)</code> 的数据时，从内存读取
<code>(0,0,0) ~ (0,0,31)</code> 数据，但是其中的 64bytes 被浪费了；</li>
<li>也就是，每次访问，都会浪费掉一半的数据，我们的带宽利用率极低；</li>
</ul></li>
<li><code>(32, 16, 16)</code>，每个 warp 包含的线程是
<code>(z, y, 0) ~ (z, y, 31)</code>，我们充分的利用了我们的 Memory
Burst，这是合理的选择；</li>
<li><code>(64, 8, 8)</code> 每个 warp 包含的线程是
<code>(z, y, 0) ~ (z, y, 31)</code> 或者
<code>(z, y, 32) ~ (z, y, 63)</code>，同样是合理的选择。</li>
</ul>
<h3 id="warp-的核心原则">warp 的核心原则</h3>
<p>warp的最核心原则是：</p>
<ol type="1">
<li><code>合并访问</code>，为了实现合并访问，我们需要保证
<code>数据内存对齐</code> 和 <code>warp对齐</code>：其中
<code>数据内存对齐</code> 是为了保证我们每次读取的数据都在同一个 Memory
Burst 中，避免需要多次内存读取；而 <code>warp对齐</code> 是保证 Memory
Burst 读取到的数据能被充分利用，避免浪费；</li>
<li><code>避免 Warp Divergence</code>：我们知道，GPU的线程执行是物理意义上的并行，并且同一个
warp 下所有的线程只能执行同样的指令。这意味着，当同一个 warp
下的所有线程，在 <code>if</code>
条件进入到不同的分支时，每次只能对进入到相同分支的线程并行执行。最差情况下，32个线程进入到32个不同的分支，这意味着我们的GPU已经完全丧失了并行计算能力。</li>
</ol>
<h2 id="gpu的数据总线">GPU的数据总线</h2>
<blockquote>
<p>我们在前面提到：<strong>现代显存（DDR6/HBM）的数据总线宽度通常是 256
bit（即 32 字节），而 <code>4</code> 个内存突发传输（<a
href="#gpu的数据总线">Memory Burst</a>）总共是
<code>128</code>字节，符合 DRAM 芯片的最佳吞吐性能。如果选 4
个线程，带宽利用率太低；如果选 512
个线程，管理开销和冲突又太严重。</strong></p>
<p>问题是，为什么我们不设计每个 warp 为8个线程，这样访问 float
时不是正好 <code>32</code> 个 bytes 正好占满总线贷款（Bus
Width）吗？</p>
</blockquote>
<p>这里存在一个关键的硬件细节：<strong>总线宽度（Bus Width）</strong> 和
<strong>突发传输长度（Burst Length, BL）</strong> 是两个不同的概念。</p>
<h3 id="memory-burst">Memory Burst</h3>
<p>在我们的总线发送数据时，我们需要先访问内存得到数据，而标准的内存访问流程如下：</p>
<ol type="1">
<li><strong>行激活 (RAS - Row Address Strobe)</strong>：
内存是由行和列组成的阵列。这一步会激活特定的<strong>行（Page）</strong>，并将这一整行的数据搬运到内存芯片内部的“行缓存（Row
Buffer）”中。这个过程比较慢，因为涉及电荷的转移。</li>
<li><strong>列寻址 (CAS - Column Address Strobe)</strong>：
一旦行被激活，控制器会发送列地址，告诉芯片：“我要这一行里的哪几个连续的字节”。</li>
<li><strong>发送数据 (Data Burst)</strong>：
这一步才是真正的“突发传输”。数据开始在数据总线上像流水一样传回
GPU。</li>
</ol>
<p>而我们的 <code>&lt;1&gt;</code> 和<code>&lt;2&gt;</code>
是拥有较为高昂的开销，所以在读取数据时我们不会仅仅读取目标行的数据，而同一行内连续读取多个列（Column），这个就是我们所谓的
<strong>Memory
Burst</strong>：为了提高效率，当我们给内存控制器一个地址时，它不会只返回那一个地址的数据，而是会连续快速地发送一批数据（通常是
8 次或 16 次传输）。</p>
<p><span class="math display">\[\text{一次突发访问的大小} =
\text{总线宽度} \times \text{突发长度 (BL)}\]</span></p>
<p><strong>也就说，最高效的数据传输，并不是每次占满总线带宽，而是最好能够覆盖我们一次
Memory Burst
读取到的内存大小。总线宽度决定了瞬时带宽，而突发传输（Burst）决定了单次事务的原子性（Atomicity）。</strong></p>
<p>此外，我们还需要注意的是：</p>
<ul>
<li>如果我们的 Warp 只请求 32 字节，但内存硬件的“最小起送量（Burst
Size）”是 128 字节，那么剩下的 96
字节流量依然会被发出，但会被硬件丢弃。这被称为 <strong>Bandwidth
Waste（带宽浪费）</strong>。</li>
<li><strong>对齐</strong>：即便我们一个 Warp 读了 128
字节，但如果起始地址不是 128 的倍数（比如从地址 64 开始），这 128
字节的数据会跨越两个物理的 Burst 块，导致内存控制器必须发起 <strong>2
次</strong> Burst 才能凑齐这一个 Warp 的需求。</li>
</ul>
<h2 id="gpu的演进">GPU的演进</h2>
<p>在实现 GPU 矩阵加法时，我们面临的核心问题是：<strong>GPU
如何在异构架构下高效获取数据？</strong> 很多人认为 GPU
只是简单的计算器，但实际上，为了实现复杂的内存访问，GPU
内部同样拥有一套完整的硬件机制。</p>
<h3 id="gpu-的硬件支撑组件">GPU 的硬件支撑组件</h3>
<p>虽然 CPU 和 GPU 的设计哲学不同，但 GPU
并非没有内存管理硬件。为了支持虚拟内存和多任务，现代 GPU 同样具备：</p>
<ul>
<li><strong>GMMU (GPU Memory Management Unit)：</strong> 类似于 CPU 的
MMU，负责将虚拟地址转换为物理地址。</li>
<li><strong>TLB (Translation Lookaside Buffer)：</strong> GPU 内部同样有
TLB 快表，用于加速地址转换过程。</li>
<li><strong>寄存器与多级缓存：</strong> GPU
拥有极大规模的寄存器堆（Register File）以及 L1/L2
缓存，用于抵消显存访问的高延迟。</li>
</ul>
<h3 id="cpu-与-gpu-的领地差异">CPU 与 GPU 的领地差异</h3>
<p>在传统 PCIE 架构中，两者界限分明：</p>
<ul>
<li><strong>CPU 领地：</strong> 内存控制器管理 <strong>系统内存
(DDR)</strong>。</li>
<li><strong>GPU 领地：</strong> 显存控制器管理 <strong>显存 (GDDR 或
HBM)</strong>。 两者通过 <strong>PCIe 总线</strong>
连接。由于物理隔离，GPU 无法通过原生偏移量直接读取 CPU 内存，必须跨越
PCIe 这座“独木桥”。</li>
</ul>
<hr />
<h3 id="内存访问模式的演进">内存访问模式的演进</h3>
<h4 id="传统方式手动搬运-explicit-copy">传统方式：手动搬运 (Explicit
Copy)</h4>
<p>这是 CUDA 编程的起点，使用 <code>cudaMemcpy</code>。</p>
<ul>
<li><strong>逻辑：</strong> CPU 先在系统内存准备好数据，通过 PCIe
将数据显式“推”送到显存。</li>
<li><strong>痛点：</strong> 产生巨大的延迟（PCIe
瓶颈）。当计算任务很小时，数据传输的时间往往远超计算时间，导致 GPU
算力闲置。</li>
</ul>
<h4 id="伪直接访问统一内存-unified-memory">“伪”直接访问：统一内存
(Unified Memory)</h4>
<p>使用 <code>cudaMallocManaged()</code> 分配地址。</p>
<ul>
<li><strong>原理：</strong> 创建一个 CPU 和 GPU
都能看到的<strong>统一虚拟地址空间</strong>。</li>
<li><strong>底层机制：</strong> 它是<strong>按需分页 (On-demand Page
Faulting)</strong>。当 GPU
访问不在显存的数据时，会触发硬件缺页异常，驱动程序自动将数据页从内存迁移到显存。它简化了编程，但频繁的缺页中断会显著降低性能。</li>
</ul>
<h4 id="锁页内存zero-copy-memory-pinned-memory">锁页内存：Zero-Copy
Memory (Pinned Memory)</h4>
<p>这是 AI Infra 性能优化的关键点。</p>
<ul>
<li><strong>核心原理：</strong> 申请一段 <strong>Pinned
Memory（锁页内存）</strong>。其真正作用是<strong>防止操作系统内核将该物理页交换（Swap）到磁盘或移动其物理位置</strong>。</li>
<li><strong>原因：</strong> 硬件 DMA
引擎必须在确定的物理地址上工作。如果内存位置被内核移动，DMA
将会读写错误的地址导致系统崩溃。</li>
<li><strong>效果：</strong> GPU 线程可以通过 PCIe
直接读取这块内存（不经过显存拷贝）。虽然省了拷贝步骤，但受限于 PCIe
带宽，访问速度远慢于显存。</li>
</ul>
<h4 id="终极加速gpudirect-族技术">终极加速：GPUDirect 族技术</h4>
<p>在高性能集群（如 A100/H100 集群）中，我们追求完全绕过 CPU：</p>
<ul>
<li><strong>GPUDirect Storage (GDS)：</strong> 使数据直接从 <strong>NVMe
SSD</strong> 搬运到 GPU 显存，跳过 CPU 内存中转。</li>
<li><strong>GPUDirect RDMA：</strong> 允许 GPU
通过网卡直接访问另一台机器的 GPU 显存。在分布式训练（如
DeepSpeed）中，这极大地降低了节点间通信的延迟和 CPU 负载。</li>
</ul>
<pre><code class="highlight mermaid">---
title: 传统的GPU读取数据
---
graph LR

SSD(&quot;SSD&quot;) --&gt;|PCIe| CPU(&quot;CPU内存&quot;) --&gt;|PCIe| GPU(&quot;GPU显存&quot;)
    
    classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
    classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
    classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
    classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
    classDef green fill:#695,color:#fff,font-weight:bold;
    classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
    classDef gray fill:#ccc,stroke:#333,font-weight:bold;
    classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
    classDef coral fill:#f8f,stroke:#333,stroke-width:4px;</code></pre>
<pre><code class="highlight mermaid">---
title: GPUDirect Storage
---
graph LR

SSD(&quot;SSD&quot;) --&gt;|PCIe| GPU(&quot;GPU显存&quot;)
    
    classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
    classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
    classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
    classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
    classDef green fill:#695,color:#fff,font-weight:bold;
    classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
    classDef gray fill:#ccc,stroke:#333,font-weight:bold;
    classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
    classDef coral fill:#f8f,stroke:#333,stroke-width:4px;</code></pre>
<h2 id="锁页内存dma">锁页内存/DMA</h2>
<p>在我们的 <a href="#第三次优化">第三次优化</a>
中，我们通过锁页内存和DMA优化了 <code>内存 &lt;-&gt; 显存</code>
的双向复制，那他们到底是什么呢？</p>
<h3 id="dma">DMA</h3>
<p>DMA 的全程是<strong>Direct Memory
Access</strong>，是一个专门负责内存访问的硬件，通常来说，由于操作系统的虚拟内存机制，应用在访问内存时需要CPU的参与：MMU/TLB
配合操作系统的页表（PageTable）来将虚拟内存转换为物理内存。而 DMA
则是主板上的一个独立控制器，允许外部设备（如显卡、网卡）<strong>直接</strong>读写系统内存，而不需要
CPU 的干预。</p>
<p>在引入 DMA 之后，只需要向 CPU 申请一片特殊的内存，随后 CPU
会将所有对这片特殊内存的读/写 <code>授权</code> 给 DMA --
这个过程被称之为<strong>DMA 传输的配置与启动</strong>。</p>
<h4 id="为什么必须申请特殊内存">为什么必须申请“特殊内存”</h4>
<p>CPU
视角下的内存是<strong>虚拟的、离散的、可移动的</strong>，它和物理内存不一样，一个连续的虚拟内存分片在物理内存上可能会分布在物理内存的任意位置。<strong>但
DMA 硬件通常直接操作的是物理总线</strong>，这也为 DMA
的操作带来了一些限制：</p>
<ul>
<li><strong>物理连续性</strong>：DMA
搬运数据时，最理想的情况是物理地址连续。普通内存（Pageable）在物理上可能碎成了很多片，DMA
搬起来效率极低。</li>
<li><strong>地址稳定性</strong>：如果 CPU 在 DMA
搬运时，因为内存不足把这块页表换出（Page Out）到了磁盘，DMA
就会去读一个错误的物理地址。</li>
</ul>
<p><code>cudaMallocHost</code> 申请的这片“特殊内存”，本质上就是试图向
CPU 申请一块连续的，不会被置换到磁盘的物理内存。</p>
<h4 id="dma是如何工作的">DMA是如何工作的</h4>
<p>DMA 的工作，需要操作系统和CPU通力合作：</p>
<ul>
<li><p><strong>设置与授权 (Setup)</strong>： CPU 告诉 DMA 控制器：</p>
<ul>
<li><p>CPU 通知 DMA 内存的起始地址；</p></li>
<li><p>CPU 通知 DMA 显存的目标地址；</p></li>
<li><p>CPU 通知 DMA 总共需要传输的数据大小；</p></li>
</ul></li>
<li><p><code>Execution</code> 一旦 CPU 发出 <code>GO</code> 的指令，DMA
就会接管总线（Bus Mastering）。CPU 从此不再参与对 DMA
管理的内存的访问，DMA 会直接和内存控制器对话，把数据顺着 PCIe 通道推向
GPU。</p></li>
<li><p><code>Interrupt</code> 当数据传输完成时，DMA
会触发硬件中断，此时，CUDA 的 <code>cudaMemcpy</code>
才会返回，或者触发异步的回调。</p></li>
</ul>
<h4 id="dma-的限制">DMA 的限制</h4>
<p>CPU 为了访问速度，引入了 L1/L2 等多级 cache，而这种 cache
会引入所谓的<strong>内存一致性</strong>问题，通常的解决方案有两个：</p>
<ul>
<li>CPU 不要对数据进行修改；</li>
<li>通过内存屏障来保证内存和缓存的一致性。</li>
</ul>
<p>而当我们开启内存屏障时，此时CPU缓存相当于失效，会拖慢CPU的执行速度，所以在我们的这个场景下，通常会选择直接选择CPU不参与任何数据修改。</p>
<pre><code class="highlight mermaid">flowchart LR

CPU(&quot;CPU&quot;):::purple
DMA(&quot;DMA&quot;):::purple

subgraph memory
    m1(&quot;memroy page 1&quot;):::gray
    m2(&quot;memroy page 2&quot;):::green
    m3(&quot;memroy page 3&quot;):::green
    m4(&quot;memroy page 4&quot;):::green
    m5(&quot;...&quot;):::gray
end

subgraph vram
    v1(&quot;vmemory page 1&quot;):::gray
    v2(&quot;...&quot;):::gray
    v3(&quot;vmemory page 5&quot;):::green
    v4(&quot;vmemory page 6&quot;):::green
    v5(&quot;vmemory page 7&quot;):::green
    v6(&quot;...&quot;):::gray
end

CPU --&gt;|1.映射memory page2,3,4 到 vmemory page 5,6,7| DMA

m2 -.-&gt;|复制数据| DMA -.-&gt;|写入数据| v3
m3 -.-&gt;|复制数据| DMA -.-&gt;|写入数据| v4
m4 -.-&gt;|复制数据| DMA -.-&gt;|写入数据| v5

DMA --&gt;|在复制完毕后触发硬件中断，由CPU配合操作系统处理后续逻辑| CPU

classDef animate stroke:#666,stroke-dasharray: 8 4,stroke-dashoffset: 900,animation: dash 20s linear infinite;
classDef yellow fill:#FFEB3B,stroke:#333,color:#000,font-weight:bold;
classDef blue fill:#489,stroke:#333,color:#fff,font-weight:bold;
classDef pink fill:#FFCCCC,stroke:#333,color:#333,font-weight:bold;
classDef green fill:#695,color:#fff,font-weight:bold;
classDef purple fill:#968,stroke:#333,color:#fff,font-weight:bold;
classDef gray fill:#ccc,stroke:#333,font-weight:bold;
classDef error fill:#bbf,stroke:#f65,stroke-width:2px,color:#fff,stroke-dasharray: 5 5;
classDef coral fill:#f8f,stroke:#333,stroke-width:4px;
classDef orange fill:#fff3e0,stroke:#ef6c00,color:#ef6c00,font-weight:bold;</code></pre>
<h3 id="锁页内存">锁页内存</h3>
<p>在操作系统（Windows/Linux）中，为了提高内存利用率，使用了<strong>虚拟内存管理</strong>。</p>
<ul>
<li><strong>分页机制
(Paging)</strong>：操作系统为了腾出物理内存给更急需的任务，会把一些不常用的内存数据偷偷挪到硬盘上（这个过程叫
Page Out）。</li>
<li><strong>物理地址漂移</strong>：同一个虚拟地址对应的物理地址可能会随时间改变。</li>
<li><strong>物理地址不连续</strong>：通常，一个连续的虚拟地址很有可能对应不连续的物理地址；</li>
</ul>
<p>而 DMA 的使用通常具有很多限制：</p>
<ul>
<li>DMA的硬件通常没有MMU/TLB组件，也不能访问操作系统内的页表，这意味着DMA只能直接操作物理内存；</li>
<li>DMA不能读取CPU的L1/L2等缓存，这意味着当CPU修改内存时会影响数据一致性；</li>
</ul>
<p>而锁页内存告诉操作系统，这一片内存不能移动（例如，当操作系统整理内存碎片时，可能会在不改变虚拟内存的情况下移动数据在物理内存的地址），也不能被置换到硬盘。这样可以保证DMA访问的可靠性和性能。</p>
<p><strong>锁页内存通常是在操作系统维护的PageTable中的PageTableEntry结构体下，标记为‘锁定’，从而保证其在硬件页表中的
Present位始终有效。</strong></p>
<h3 id="最后的结果">最后的结果</h3>
<pre><code class="highlight mermaid">flowchart TD
    subgraph Host_Side [&quot;Host (CPU 侧)&quot;]
        direction TB
        A[标准内存 malloc]:::gray -- 1.额外拷贝 --&gt; B[驱动隐藏缓冲区]:::pink
        C[锁页内存 cudaMallocHost]:::green -- 1.直接映射 --&gt; D[DMA 控制器]:::purple
    end

    subgraph Hardware_Bus [&quot;硬件总线 (PCIe)&quot;]
        D -- 2.授权 Bus Mastering --&gt; PCIe&#123;PCIe 4.0 总线&#125;
        B -- 2.搬运数据 --&gt; PCIe
    end

    subgraph Device_Side [&quot;Device (GPU 侧)&quot;]
        PCIe -- 3.直达显存 --&gt; VRAM[(VRAM 显存)]
        VRAM -- 4.连续访存 Coalesced --&gt; SMs[34 个 SMs 核心]
        
        subgraph SM_Detail [&quot;SM 内部细节&quot;]
            direction LR
            Warp1[Warp 0]:::blue
            Warp2[Warp 1]:::blue
            Warp3[Warp 2]:::blue
            Warp1 &lt;--&gt; Regs[(寄存器组)]
            Warp2 &lt;--&gt; Regs
            Warp3 &lt;--&gt; Regs
        end
    end

    SMs -.-&gt; SM_Detail

    %% 状态定义
    classDef green fill:#695,color:#fff,font-weight:bold;
    classDef purple fill:#968,color:#fff,font-weight:bold;
    classDef gray fill:#ccc,color:#666;
    classDef pink fill:#f96,color:#fff;
    classDef blue fill:#489,color:#ff</code></pre>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/GPU/" rel="tag"># GPU</a>
              <a href="/tags/CUDA/" rel="tag"># CUDA</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2026/01/16/build-my-own-raft-from-scratch/" rel="prev" title="build my own raft from scratch">
                  <i class="fa fa-angle-left"></i> build my own raft from scratch
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">2183814023</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"0x822a5b87/blog-comments","issue_term":"pathname","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js" defer></script>

</body>
</html>
